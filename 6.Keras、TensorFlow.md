# Keras

## 用 Sequential API 创建模型

Sequential模型，这是Keras最简单的模型，是由单层神经元顺序连起来的，被称为Sequential API。

详见：《Scikit-Learn、Keras与TensorFlow机器学习实用指南（第二版）》第10章 使用Keras搭建人工神经网络

可以看到，使用Sequential API是很方便的。但是，尽管`Sequential`十分常见，但用它搭建复杂拓扑形态或多输入多输出的神经网络还是不多。所以，Keras还提供了Functional API。

## 使用Functional API搭建复杂模型

Wide & Deep是一个非序列化的神经网络模型。这个架构是Heng-Tze Cheng在2016年在[论文](https://arxiv.org/abs/1606.07792)中提出来的。这个模型可以将全部或部分输入与输出层连起来，见图10-14。这样，就可以**既学到深层模式（使用深度路径）和简单规则（使用短路径）**。作为对比，常规MLP会强制所有数据流经所有层，因此数据中的简单模式在多次变换后会被扭曲。

### 有以下要使用多输入的场景：

- 任务要求。例如，你想定位和分类图片中的主要物体。这既是一个回归任务（找到目标中心的坐标、宽度和高度）和分类任务。
- 相似的，对于相同的数据，你可能**有多个独立的任务**。当然可以每个任务训练一个神经网络，但在多数情况下，同时对所有任务训练一个神经网络，每个任务一个输出，后者的效果更好。这是因为神经网络可以在不同任务间学习有用的数据特征。例如，在人脸的多任务分类时，你可以用一个输出做人物表情的分类（微笑惊讶等等），用另一个输出判断是否戴着眼镜。
- 另一种情况是作为一种正则的方法（即，一种降低过拟合和提高泛化能力的训练约束）。例如，你想在神经网络中加入一些辅助输出（见图10-16），好让神经网络的一部分依靠自身就能学到一些东西。![](D:\AI\Friedrich\pictrue\Functional.png)

```python
input_A = keras.layers.Input(shape=[5], name="wide_input")
input_B = keras.layers.Input(shape=[6], name="deep_input")
# 创建一个有30个神经元的紧密层，激活函数是ReLU。创建好之后，将其作为函数，直接将输入传给它。
# 这就是Functional API的得名原因。
hidden1 = keras.layers.Dense(30, activation="relu")(input_B)
hidden2 = keras.layers.Dense(30, activation="relu")(hidden1)
concat = keras.layers.concatenate([input_A, hidden2])
output = keras.layers.Dense(1, name="output")(concat)

model = keras.Model(inputs=[input_A, input_B], outputs=[output])
```

 添加额外的输出很容易：只需要将输出和相关的层连起来、将输出写入输出列表就行。例如，下面的代码搭建的就是图10-16的架构： 

![](D:\AI\Friedrich\pictrue\aux_output.png)

```python
# 添加额外的输出，output层前面都一样
output = keras.layers.Dense(1, name="main_output")(concat)
aux_output = keras.layers.Dense(1, name="aux_output")(hidden2)
model = keras.Model(inputs=[input_A, input_B], outputs=[output, aux_output])

# 每个输出都要有自己的损失函数。因此在编译模型时，需要传入损失列表（如果只传入一个损失，Keras会认为所有输出是同一个损失函数）。
# Keras默认计算所有损失，将其求和得到最终损失用于训练。主输出比辅助输出更值得关心，所以要提高它的权重
model.compile(loss=["mse", "mse"], loss_weights=[0.9, 0.1], optimizer="sgd")

# 此时若要训练模型，必须给每个输出贴上标签。在这个例子中，主输出和辅输出预测的是同一件事，因此标签相同。
# 传入数据必须是(y_train, y_train)（y_valid和y_test也是如此）
history = model.fit([X_train_A, X_train_B], [y_train, y_train], epochs=20,
                    validation_data=([X_valid_A, X_valid_B], [y_valid, y_valid]))

# 当评估模型时，Keras会返回总损失和各个损失值
total_loss, main_loss, aux_loss = model.evaluate([X_test_A, X_test_B], [y_test, y_test])
# 相似的，方法predict()会返回每个输出的预测值
y_pred = model.predict((X_new_A, X_new_B))
```

## 使用Subclassing API搭建动态模型

Sequential API和Functional API都是声明式的：只有声明创建每个层以及层的连接方式，才能给模型加载数据以进行训练和推断。这种方式有其优点：模型可以方便的进行保存、克隆和分享；模型架构得以展示，便于分析；框架可以推断数据形状和类型，便于及时发现错误（加载数据之前就能发现错误）。调试也很容易，因为模型是层的静态图。但是缺点也很明显：模型是静态的。一些模型包含循环、可变数据形状、条件分支，和其它的动态特点。对于这些情况，或者你只是喜欢命令式编程，不妨使用Subclassing API。

对`Model`类划分子类，在构造器中创建需要的层，调用`call()`进行计算。例如，创建一个下面的`WideAndDeepModel`类的实例，就可以创建与前面Functional API例子的同样模型，同样可以进行编译、评估、预测：

```python
class WideAndDeepModel(keras.Model):
    def __init__(self, units=30, activation="relu", **kwargs):
        super().__init__(**kwargs) # handles standard args (e.g., name)
        self.hidden1 = keras.layers.Dense(units, activation=activation)
        self.hidden2 = keras.layers.Dense(units, activation=activation)
        self.main_output = keras.layers.Dense(1)
        self.aux_output = keras.layers.Dense(1)

    def call(self, inputs):
        input_A, input_B = inputs
        hidden1 = self.hidden1(input_B)
        hidden2 = self.hidden2(hidden1)
        concat = keras.layers.concatenate([input_A, hidden2])
        main_output = self.main_output(concat)
        aux_output = self.aux_output(hidden2)
        return main_output, aux_output

model = WideAndDeepModel()
```

这个例子和Functional API很像，除了不用创建输入；只需要在`call()`使用参数`input`，另外的不同是将层的创建和和使用分割了。最大的差别是，在`call()`方法中，你可以做任意想做的事：for循环、if语句、低级的TensorFlow操作，可以尽情发挥想象（见第12章）！Subclassing API可以让研究者试验各种新创意。

然而代价也是有的：模型架构隐藏在`call()`方法中，所以Keras不能对其检查；不能保存或克隆；当调用`summary()`时，得到的只是层的列表，没有层的连接信息。另外，Keras不能提前检查数据类型和形状，所以很容易犯错。所以除非真的需要灵活性，还是使用Sequential API或Functional API吧。



## 保存和恢复模型

 使用Sequential API或Functional API时，保存训练好的Keras模型和训练一样简单： 

```python
model = keras.layers.Sequential([...]) # or keras.Model([...])
model.compile([...])
model.fit([...])
model.save("my_keras_model.h5")
```

Keras使用HDF5格式保存模型架构（包括每层的超参数）和每层的所有参数值（连接权重和偏置项）。还保存了优化器（包括超参数和状态）。

通常用脚本训练和保存模型，一个或更多的脚本（或web服务）来加载模型和做预测。加载模型很简单：

```python
model = keras.models.load_model("my_keras_model.h5")
```

> 警告：这种加载模型的方法只对Sequential API或Functional API有用，不适用于Subclassing API。对于后者，可以用`save_weights()`和`load_weights()`保存参数，其它的就得手动保存恢复了。



## 使用调回

`fit()`方法接受参数`callbacks`，可以让用户指明一个Keras列表，让Keras在训练开始和结束、每个周期开始和结束、甚至是每个批次的前后调用。例如，`ModelCheckpoint`可以在每个时间间隔保存检查点，默认是每个周期结束之后：

```python
[...] # 搭建编译模型
checkpoint_cb = keras.callbacks.ModelCheckpoint("my_keras_model.h5")
history = model.fit(X_train, y_train, epochs=10, callbacks=[checkpoint_cb])
```

另外，如果训练时使用了验证集，可以在创建检查点时设定`save_best_only=True`，只有当模型在验证集上取得最优值时才保存模型。这么做可以不必担心训练时间过长和训练集过拟合：只需加载训练好的模型，就能保证是在验证集上表现最好的模型。下面的代码演示了早停（见第4章）：

```bash
checkpoint_cb = keras.callbacks.ModelCheckpoint("my_keras_model.h5",
                                                save_best_only=True)
history = model.fit(X_train, y_train, epochs=10,
                    validation_data=(X_valid, y_valid),
                    callbacks=[checkpoint_cb])
model = keras.models.load_model("my_keras_model.h5") # roll back to best model
```

另一种实现早停的方法是使用`EarlyStopping`调回。当检测到经过几个周期（周期数由参数`patience`确定），验证集表现没有提升时，就会中断训练，还能自动滚回到最优模型。可以将保存检查点（避免宕机）和早停（避免浪费时间和资源）结合起来：

```bash
early_stopping_cb = keras.callbacks.EarlyStopping(patience=10,
                                                  restore_best_weights=True)
history = model.fit(X_train, y_train, epochs=100,
                    validation_data=(X_valid, y_valid),
                    callbacks=[checkpoint_cb, early_stopping_cb])
```

周期数可以设的很大，因为准确率没有提升时，训练就会自动停止。此时，就没有必要恢复最优模型，因为`EarlyStopping`调回一直在跟踪最优权重，训练结束时能自动恢复。

> 提示：包[`keras.callbacks`]( Callbacks - Keras Documentation )中还有其它可用的调回。

如果还想有其它操控，还可以编写自定义的调回。下面的例子展示了一个可以展示验证集损失和训练集损失比例的自定义（检测过拟合）调回：

```python
class PrintValTrainRatioCallback(keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs):
        print("\nval/train: {:.2f}".format(logs["val_loss"] / logs["loss"]))
```

类似的，还可以实现`on_train_begin()`、`on_train_end()`、`on_epoch_begin()`、`on_epoch_end()`、`on_batch_begin()`、和`on_batch_end()`。如果需要的话，在评估和预测时也可以使用调回（例如为了调试）。对于评估，可以实现`on_test_begin()`、`on_test_end()`、`on_test_batch_begin()`或`on_test_batch_end()`（通过`evaluate()`调用）；对于预测，可以实现`on_predict_begin()`、`on_predict_end()`、`on_predict_batch_begin()`或`on_predict_batch_end()`（通过`predict()`调用）。



## 使用TensorBoard进行可视化

TensorBoard是一个强大的交互可视化工具，使用它可以查看训练过程中的学习曲线、比较每次运行的学习曲线、可视化计算图、分析训练数据、查看模型生成的图片、可视化投射到3D的多维数据，等等。TensorBoard是TensorFlow自带的。

要使用TensorBoard，必须修改程序，将要可视化的数据输出为二进制的日志文件`event files`。每份二进制数据称为摘要`summary`，TensorBoard服务器会监测日志文件目录，自动加载更新并可视化：这样就能看到实时数据（稍有延迟），比如训练时的学习曲线。通常，将TensorBoard服务器指向根日志目录，程序的日志写入到它的子目录，这样一个TensorBoard服务就能可视化并比较多次运行的数据，而不会将其搞混。

我们先定义TensorBoard的根日志目录，还有一些根据当前日期生成子目录的小函数。你可能还想在目录名中加上其它信息，比如超参数的值，方便知道查询的内容：

```python
import os
root_logdir = os.path.join(os.curdir, "my_logs")

def get_run_logdir():
    import time
    run_id = time.strftime("run_%Y_%m_%d-%H_%M_%S")
    return os.path.join(root_logdir, run_id)

run_logdir = get_run_logdir() # e.g., './my_logs/run_2019_06_07-15_15_22'
```

Keras提供了一个`TensorBoard()`调回：

```python
[...] # 搭建编译模型
tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)
history = model.fit(X_train, y_train, epochs=30,
                    validation_data=(X_valid, y_valid),
                    callbacks=[tensorboard_cb])
```

简直不能再简单了。如果运行这段代码，`TensorBoard()`调回会负责创建日志目录（包括父级目录），在训练过程中会创建事件文件并写入概要。

每次运行都会创建一个目录，每个目录都有一个包含训练日志和验证日志的子目录。两者都包括事件文件，训练日志还包括分析追踪信息：它可以让TensorBoard展示所有设备上的模型的各个部分的训练时长，有助于定位性能瓶颈。

然后就可以启动TensorBoard服务了。一种方式是通过运行命令行。如果是在虚拟环境中安装的TensorFlow，需要激活虚拟环境。接着，在根目录（也可以是其它路径，但一定要指向日志目录）运行下面的命令：

```shell
$ tensorboard --logdir=./my_logs --port=6006
TensorBoard 2.0.0 at http://mycomputer.local:6006/ (Press CTRL+C to quit)
```

如果终端没有找到`tensorboard`命令，必须更新环境变量PATH（或者，可以使用`python3 -m tensorboard.main`）。服务启动后，打开浏览器访问 [*http://localhost:6006*](简书)。

或者，通过运行下面的命令，可以在Jupyter里面直接使用TensorBoard。第一行代码加载了TensorBoard扩展，第二行在端口6006启动了一个TensorBoard服务，并连接：

```shell
%load_ext tensorboard
%tensorboard --logdir=./my_logs --port=6006
```

无论是使用哪种方式，都得使用TensorBoard的浏览器界面。点击栏`SCALARS`可以查看学习曲线（见图10-17）。左下角选择想要可视化的路径（比如第一次和第二次运行的训练日志），再点击`epoch_loss`。可以看到，在两次训练过程中，训练损失都是下降的，但第二次下降的更快。事实上，第二次的学习率是0.05（`optimizer=keras.optimizers.SGD(lr=0.05)`）而不是0.001。

还可以对全图、权重（投射到3D）或其它信息做可视化。`TensorBoard()`调回还有选项可以记录其它数据的日志，比如嵌入（见第13章）。另外，TensorBoard在`tf.summary`包中还提供了低级API。下面的代码使用方法`create_file_writer()`创建了`SummaryWriter`，TensorBoard使用`SummaryWriter`作为记录标量、柱状图、图片、音频和文本的上下文，所有这些都是可以可视化的！

```python
test_logdir = get_run_logdir()
writer = tf.summary.create_file_writer(test_logdir)
with writer.as_default():
    for step in range(1, 1000 + 1):
        tf.summary.scalar("my_scalar", np.sin(step / 10), step=step)
        data = (np.random.randn(100) + 2) * step / 100 # some random data
        tf.summary.histogram("my_hist", data, buckets=50, step=step)
        images = np.random.rand(2, 32, 32, 3) # random 32×32 RGB images
        tf.summary.image("my_images", images * step / 1000, step=step)
        texts = ["The step is " + str(step), "Its square is " + str(step**2)]
        tf.summary.text("my_text", texts, step=step)
        sine_wave = tf.math.sin(tf.range(12000) / 48000 * 2 * np.pi * step)
        audio = tf.reshape(tf.cast(sine_wave, tf.float32), [1, -1, 1])
tf.summary.audio("my_audio", audio, sample_rate=48000, step=step)
```



# TensorFlow2.0

