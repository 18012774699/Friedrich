# Keras

## 用 Sequential API 创建模型

Sequential模型，这是Keras最简单的模型，是由单层神经元顺序连起来的，被称为Sequential API。

详见：《Scikit-Learn、Keras与TensorFlow机器学习实用指南（第二版）》第10章 使用Keras搭建人工神经网络

可以看到，使用Sequential API是很方便的。但是，尽管`Sequential`十分常见，但用它搭建复杂拓扑形态或多输入多输出的神经网络还是不多。所以，Keras还提供了Functional API。

## 使用Functional API搭建复杂模型

Wide & Deep是一个非序列化的神经网络模型。这个架构是Heng-Tze Cheng在2016年在[论文](Wide & Deep Learning for Recommender Systems)中提出来的。这个模型可以将全部或部分输入与输出层连起来，见图10-14。这样，就可以**既学到深层模式（使用深度路径）和简单规则（使用短路径）**。作为对比，常规MLP会强制所有数据流经所有层，因此数据中的简单模式在多次变换后会被扭曲。

### 有以下要使用多输入的场景：

- 任务要求。例如，你想定位和分类图片中的主要物体。这既是一个回归任务（找到目标中心的坐标、宽度和高度）和分类任务。
- 相似的，对于相同的数据，你可能**有多个独立的任务**。当然可以每个任务训练一个神经网络，但在多数情况下，同时对所有任务训练一个神经网络，每个任务一个输出，后者的效果更好。这是因为神经网络可以在不同任务间学习有用的数据特征。例如，在人脸的多任务分类时，你可以用一个输出做人物表情的分类（微笑惊讶等等），用另一个输出判断是否戴着眼镜。
- 另一种情况是作为一种正则的方法（即，一种降低过拟合和提高泛化能力的训练约束）。例如，你想在神经网络中加入一些辅助输出（见图10-16），好让神经网络的一部分依靠自身就能学到一些东西。

```python
input_A = keras.layers.Input(shape=[5], name="wide_input")
input_B = keras.layers.Input(shape=[6], name="deep_input")
# 创建一个有30个神经元的紧密层，激活函数是ReLU。创建好之后，将其作为函数，直接将输入传给它。
# 这就是Functional API的得名原因。
hidden1 = keras.layers.Dense(30, activation="relu")(input_B)
hidden2 = keras.layers.Dense(30, activation="relu")(hidden1)
concat = keras.layers.concatenate([input_A, hidden2])
output = keras.layers.Dense(1, name="output")(concat)

model = keras.Model(inputs=[input_A, input_B], outputs=[output])
```

 添加额外的输出很容易：只需要将输出和相关的层连起来、将输出写入输出列表就行。例如，下面的代码搭建的就是图10-16的架构： 

```python
# 添加额外的输出，output层前面都一样
output = keras.layers.Dense(1, name="main_output")(concat)
aux_output = keras.layers.Dense(1, name="aux_output")(hidden2)
model = keras.Model(inputs=[input_A, input_B], outputs=[output, aux_output])

# 每个输出都要有自己的损失函数。因此在编译模型时，需要传入损失列表（如果只传入一个损失，Keras会认为所有输出是同一个损失函数）。
# Keras默认计算所有损失，将其求和得到最终损失用于训练。主输出比辅助输出更值得关心，所以要提高它的权重
model.compile(loss=["mse", "mse"], loss_weights=[0.9, 0.1], optimizer="sgd")

# 此时若要训练模型，必须给每个输出贴上标签。在这个例子中，主输出和辅输出预测的是同一件事，因此标签相同。
# 传入数据必须是(y_train, y_train)（y_valid和y_test也是如此）
history = model.fit([X_train_A, X_train_B], [y_train, y_train], epochs=20,
                    validation_data=([X_valid_A, X_valid_B], [y_valid, y_valid]))

# 当评估模型时，Keras会返回总损失和各个损失值
total_loss, main_loss, aux_loss = model.evaluate([X_test_A, X_test_B], [y_test, y_test])
# 相似的，方法predict()会返回每个输出的预测值
y_pred = model.predict((X_new_A, X_new_B))
```

## 使用Subclassing API搭建动态模型

Sequential API和Functional API都是声明式的：只有声明创建每个层以及层的连接方式，才能给模型加载数据以进行训练和推断。这种方式有其优点：模型可以方便的进行保存、克隆和分享；模型架构得以展示，便于分析；框架可以推断数据形状和类型，便于及时发现错误（加载数据之前就能发现错误）。调试也很容易，因为模型是层的静态图。但是缺点也很明显：模型是静态的。一些模型包含循环、可变数据形状、条件分支，和其它的动态特点。对于这些情况，或者你只是喜欢命令式编程，不妨使用Subclassing API。

对`Model`类划分子类，在构造器中创建需要的层，调用`call()`进行计算。例如，创建一个下面的`WideAndDeepModel`类的实例，就可以创建与前面Functional API例子的同样模型，同样可以进行编译、评估、预测：

```python
class WideAndDeepModel(keras.Model):
    def __init__(self, units=30, activation="relu", **kwargs):
        super().__init__(**kwargs) # handles standard args (e.g., name)
        self.hidden1 = keras.layers.Dense(units, activation=activation)
        self.hidden2 = keras.layers.Dense(units, activation=activation)
        self.main_output = keras.layers.Dense(1)
        self.aux_output = keras.layers.Dense(1)

    def call(self, inputs):
        input_A, input_B = inputs
        hidden1 = self.hidden1(input_B)
        hidden2 = self.hidden2(hidden1)
        concat = keras.layers.concatenate([input_A, hidden2])
        main_output = self.main_output(concat)
        aux_output = self.aux_output(hidden2)
        return main_output, aux_output

model = WideAndDeepModel()
```

这个例子和Functional API很像，除了不用创建输入；只需要在`call()`使用参数`input`，另外的不同是将层的创建和和使用分割了。最大的差别是，在`call()`方法中，你可以做任意想做的事：for循环、if语句、低级的TensorFlow操作，可以尽情发挥想象（见第12章）！Subclassing API可以让研究者试验各种新创意。

然而代价也是有的：模型架构隐藏在`call()`方法中，所以Keras不能对其检查；不能保存或克隆；当调用`summary()`时，得到的只是层的列表，没有层的连接信息。另外，Keras不能提前检查数据类型和形状，所以很容易犯错。所以除非真的需要灵活性，还是使用Sequential API或Functional API吧。



## 保存和恢复模型

 使用Sequential API或Functional API时，保存训练好的Keras模型和训练一样简单： 

```python
model = keras.layers.Sequential([...]) # or keras.Model([...])
model.compile([...])
model.fit([...])
model.save("my_keras_model.h5")
```

Keras使用HDF5格式保存模型架构（包括每层的超参数）和每层的所有参数值（连接权重和偏置项）。还保存了优化器（包括超参数和状态）。

通常用脚本训练和保存模型，一个或更多的脚本（或web服务）来加载模型和做预测。加载模型很简单：

```python
model = keras.models.load_model("my_keras_model.h5")
```

> 警告：这种加载模型的方法只对Sequential API或Functional API有用，不适用于Subclassing API。对于后者，可以用`save_weights()`和`load_weights()`保存参数，其它的就得手动保存恢复了。



## 使用调回

`fit()`方法接受参数`callbacks`，可以让用户指明一个Keras列表，让Keras在训练开始和结束、每个周期开始和结束、甚至是每个批次的前后调用。例如，`ModelCheckpoint`可以在每个时间间隔保存检查点，默认是每个周期结束之后：

```python
[...] # 搭建编译模型
checkpoint_cb = keras.callbacks.ModelCheckpoint("my_keras_model.h5")
history = model.fit(X_train, y_train, epochs=10, callbacks=[checkpoint_cb])
```

另外，如果训练时使用了验证集，可以在创建检查点时设定`save_best_only=True`，只有当模型在验证集上取得最优值时才保存模型。这么做可以不必担心训练时间过长和训练集过拟合：只需加载训练好的模型，就能保证是在验证集上表现最好的模型。下面的代码演示了早停（见第4章）：

```bash
checkpoint_cb = keras.callbacks.ModelCheckpoint("my_keras_model.h5",
                                                save_best_only=True)
history = model.fit(X_train, y_train, epochs=10,
                    validation_data=(X_valid, y_valid),
                    callbacks=[checkpoint_cb])
model = keras.models.load_model("my_keras_model.h5") # roll back to best model
```

另一种实现早停的方法是使用`EarlyStopping`调回。当检测到经过几个周期（周期数由参数`patience`确定），验证集表现没有提升时，就会中断训练，还能自动滚回到最优模型。可以将保存检查点（避免宕机）和早停（避免浪费时间和资源）结合起来：

```bash
early_stopping_cb = keras.callbacks.EarlyStopping(patience=10,
                                                  restore_best_weights=True)
history = model.fit(X_train, y_train, epochs=100,
                    validation_data=(X_valid, y_valid),
                    callbacks=[checkpoint_cb, early_stopping_cb])
```

周期数可以设的很大，因为准确率没有提升时，训练就会自动停止。此时，就没有必要恢复最优模型，因为`EarlyStopping`调回一直在跟踪最优权重，训练结束时能自动恢复。

> 提示：包[`keras.callbacks`]( Callbacks - Keras Documentation )中还有其它可用的调回。

如果还想有其它操控，还可以编写自定义的调回。下面的例子展示了一个可以展示验证集损失和训练集损失比例的自定义（检测过拟合）调回：

```python
class PrintValTrainRatioCallback(keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs):
        print("\nval/train: {:.2f}".format(logs["val_loss"] / logs["loss"]))
```

类似的，还可以实现`on_train_begin()`、`on_train_end()`、`on_epoch_begin()`、`on_epoch_end()`、`on_batch_begin()`、和`on_batch_end()`。如果需要的话，在评估和预测时也可以使用调回（例如为了调试）。对于评估，可以实现`on_test_begin()`、`on_test_end()`、`on_test_batch_begin()`或`on_test_batch_end()`（通过`evaluate()`调用）；对于预测，可以实现`on_predict_begin()`、`on_predict_end()`、`on_predict_batch_begin()`或`on_predict_batch_end()`（通过`predict()`调用）。



## 使用TensorBoard进行可视化

TensorBoard是一个强大的交互可视化工具，使用它可以查看训练过程中的学习曲线、比较每次运行的学习曲线、可视化计算图、分析训练数据、查看模型生成的图片、可视化投射到3D的多维数据，等等。TensorBoard是TensorFlow自带的。

要使用TensorBoard，必须修改程序，将要可视化的数据输出为二进制的日志文件`event files`。每份二进制数据称为摘要`summary`，TensorBoard服务器会监测日志文件目录，自动加载更新并可视化：这样就能看到实时数据（稍有延迟），比如训练时的学习曲线。通常，将TensorBoard服务器指向根日志目录，程序的日志写入到它的子目录，这样一个TensorBoard服务就能可视化并比较多次运行的数据，而不会将其搞混。

我们先定义TensorBoard的根日志目录，还有一些根据当前日期生成子目录的小函数。你可能还想在目录名中加上其它信息，比如超参数的值，方便知道查询的内容：

```python
import os
root_logdir = os.path.join(os.curdir, "my_logs")

def get_run_logdir():
    import time
    run_id = time.strftime("run_%Y_%m_%d-%H_%M_%S")
    return os.path.join(root_logdir, run_id)

run_logdir = get_run_logdir() # e.g., './my_logs/run_2019_06_07-15_15_22'
```

Keras提供了一个`TensorBoard()`调回：

```python
[...] # 搭建编译模型
tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)
history = model.fit(X_train, y_train, epochs=30,
                    validation_data=(X_valid, y_valid),
                    callbacks=[tensorboard_cb])
```

简直不能再简单了。如果运行这段代码，`TensorBoard()`调回会负责创建日志目录（包括父级目录），在训练过程中会创建事件文件并写入概要。

每次运行都会创建一个目录，每个目录都有一个包含训练日志和验证日志的子目录。两者都包括事件文件，训练日志还包括分析追踪信息：它可以让TensorBoard展示所有设备上的模型的各个部分的训练时长，有助于定位性能瓶颈。

然后就可以启动TensorBoard服务了。一种方式是通过运行命令行。如果是在虚拟环境中安装的TensorFlow，需要激活虚拟环境。接着，在根目录（也可以是其它路径，但一定要指向日志目录）运行下面的命令：

```shell
$ tensorboard --logdir=./my_logs --port=6006
TensorBoard 2.0.0 at http://mycomputer.local:6006/ (Press CTRL+C to quit)
```

如果终端没有找到`tensorboard`命令，必须更新环境变量PATH（或者，可以使用`python3 -m tensorboard.main`）。服务启动后，打开浏览器访问 [*http://localhost:6006*](简书)。

或者，通过运行下面的命令，可以在Jupyter里面直接使用TensorBoard。第一行代码加载了TensorBoard扩展，第二行在端口6006启动了一个TensorBoard服务，并连接：

```shell
%load_ext tensorboard
%tensorboard --logdir=./my_logs --port=6006
```

无论是使用哪种方式，都得使用TensorBoard的浏览器界面。点击栏`SCALARS`可以查看学习曲线（见图10-17）。左下角选择想要可视化的路径（比如第一次和第二次运行的训练日志），再点击`epoch_loss`。可以看到，在两次训练过程中，训练损失都是下降的，但第二次下降的更快。事实上，第二次的学习率是0.05（`optimizer=keras.optimizers.SGD(lr=0.05)`）而不是0.001。

还可以对全图、权重（投射到3D）或其它信息做可视化。`TensorBoard()`调回还有选项可以记录其它数据的日志，比如嵌入（见第13章）。另外，TensorBoard在`tf.summary`包中还提供了低级API。下面的代码使用方法`create_file_writer()`创建了`SummaryWriter`，TensorBoard使用`SummaryWriter`作为记录标量、柱状图、图片、音频和文本的上下文，所有这些都是可以可视化的！

```python
test_logdir = get_run_logdir()
writer = tf.summary.create_file_writer(test_logdir)
with writer.as_default():
    for step in range(1, 1000 + 1):
        tf.summary.scalar("my_scalar", np.sin(step / 10), step=step)
        data = (np.random.randn(100) + 2) * step / 100 # some random data
        tf.summary.histogram("my_hist", data, buckets=50, step=step)
        images = np.random.rand(2, 32, 32, 3) # random 32×32 RGB images
        tf.summary.image("my_images", images * step / 1000, step=step)
        texts = ["The step is " + str(step), "Its square is " + str(step**2)]
        tf.summary.text("my_text", texts, step=step)
        sine_wave = tf.math.sin(tf.range(12000) / 48000 * 2 * np.pi * step)
        audio = tf.reshape(tf.cast(sine_wave, tf.float32), [1, -1, 1])
tf.summary.audio("my_audio", audio, sample_rate=48000, step=step)
```



# TensorFlow2.0

目前为止，我们只是使用了TensorFlow的高级API —— tf.keras，它的功能很强大：搭建了各种神经网络架构，包括回归、分类网络、Wide & Deep 网络、自归一化网络，使用了各种方法，包括批归一化、dropout和学习率调度。事实上，你在实际案例中95%碰到的情况只需要tf.keras就足够了（和tf.data，见第13章）。现在来深入学习TensorFlow的低级Python API。当你需要实现自定义损失函数、自定义标准、层、模型、初始化器、正则器、权重约束时，就需要低级API了。甚至有时需要全面控制训练过程，例如使用特殊变换或对约束梯度时。这一章就会讨论这些问题，还会学习如何使用TensorFlow的自动图生成特征提升自定义模型和训练算法。

## TensorFlow速览

TensorFlow是一个强大的数值计算库，特别适合做和微调大规模机器学习（但也可以用来做其它的重型计算）。TensorFlow是谷歌大脑团队开发的，支持了谷歌的许多大规模服务，包括谷歌云对话、谷歌图片和谷歌搜索。TensorFlow是2015年11月开源的，（按文章引用、公司采用、GitHub星数）是目前最流行的深度学习库。无数的项目是用TensorFlow来做各种机器学习任务，包括**图片分类、自然语言处理、推荐系统和时间序列预测**。TensorFlow提供的功能如下：

- TensorFlow的核心与NumPy很像，但TensorFlow支持GPU；
- TensorFlow支持（多设备和服务器）分布式计算；
- TensorFlow使用了即时JIT编译器对计算速度和内存使用优化。编译器的工作是从Python函数提取出计算图，然后对计算图优化（比如剪切无用的节点），最后高效运行（比如自动并行运行独立任务）；
- 计算图可以导出为迁移形式，因此可以在一个环境中训练一个TensorFlow模型（比如使用Python或Linux），然后在另一个环境中运行（比如在安卓设备上用Java运行）；
- TensorFlow实现了自动微分，并提供了一些高效的优化器，比如RMSProp和NAdam，因此可以容易的最小化各种损失函数。

基于上面这些特点，TensorFlow还提供了许多其他功能：最重要的是tf.keras，还有数据加载和预处理操作（tf.data，tf.io等等），图片处理操作（tf.image），信号处理操作（tf.signal），等等（图12-1总结了TensorFlow的Python API）

![img](https://upload-images.jianshu.io/upload_images/7178691-4d1453d7d971cba7.png?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp)

> 提示：这一章会介绍TensorFlow API的多个包和函数，但来不及介绍全部，所以读者最好自己花点时间好好看看API。TensorFlow的API十分丰富，且文档详实。

TensorFlow的低级操作都是用高效的C++实现的。许多操作有多个实现，称为`核`：每个核对应一个具体的设备型号，比如CPU、GPU，甚至TPU（张量处理单元）。GPU通过将任务分成小块，在多个GPU线程中并行运行，可以极大提高提高计算的速度。TPU更快：TPU是自定义的ASIC芯片，专门用来做深度学习运算的（第19章会讨论适合使用GPU和TPU）。

TensorFlow的架构见图12-2。大多数时候你的代码使用高级API就够了（特别是tf.keras和tf.data），但如果需要更大的灵活性，就需要使用低级Python API，来直接处理张量。TensorFlow也支持其它语言的API。任何情况下，甚至是跨设备和机器的情况下，TensorFlow的执行引擎都会负责高效运行。

![img](https:////upload-images.jianshu.io/upload_images/7178691-c77b37df3aea8d69.png?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp)

图12-2 TensorFlow的架构

TensorFlow不仅可以运行在Windows、Linux和macOS上，也可以运行在移动设备上（使用TensorFlow Lite），包括iOS和安卓（见第19章）。如果不想使用Python API，还可以使用C++、Java、Go和Swift的API。甚至还有JavaScript的实现TensorFlow.js，它可以直接在浏览器中运行。

TensorFlow不只有这些库。TensorFlow处于一套可扩展的生态系统库的核心位置。首先，TensorBoard可以用来可视化。其次，TensorFlow Extended（TFX），是谷歌推出的用来生产化的库，包括：数据确认、预处理、模型分析和服务（使用TF Serving，见第19章）。谷歌的TensorFlow Hub上可以方便下载和复用预训练好的神经网络。你还可以从TensorFlow的model garden（[tensorflow/models](简书)）获取许多神经网络架构，其中一些是预训练好的。[TensorFlow Resources](简书) 和 [*https://github.com/jtoy/awesome-tensorflow*](简书)上有更多的资源。你可以在GitHub上找到数百个TensorFlow项目，无论干什么都可以方便地找到现成的代码。

> 提示：越来越多的ML论文都附带了实现过程，一些甚至带有预训练模型。可以在[*https://paperswithcode.com/*](简书)找到。

最后，TensorFlow有一支热忱满满的开发者团队，也有庞大的社区。要是想问技术问题，可以去[*http://stackoverflow.com/*](简书)
 ，问题上打上tensorflow和python标签。还可以在[GitHub](简书)上提bug和新功能。一般的讨论可以去谷歌群组（[https://groups.google.com/a/tensorflow.org/forum/](简书)）。



## 像NumPy一样使用TensorFlow

使用`tf.constant()`创建张量。例如，下面的张量表示的是两行三列的浮点数矩阵：

```bash
>>> tf.constant([[1., 2., 3.], [4., 5., 6.]]) # matrix
<tf.Tensor: id=0, shape=(2, 3), dtype=float32, numpy=
array([[1., 2., 3.],
       [4., 5., 6.]], dtype=float32)>
>>> tf.constant(42) # 标量
<tf.Tensor: id=1, shape=(), dtype=int32, numpy=42>
```

就像`ndarray`一样，`tf.Tensor`也有形状和数据类型（`dtype`）：

```bash
>>> t = tf.constant([[1., 2., 3.], [4., 5., 6.]])
>>> t.shape
TensorShape([2, 3])
>>> t.dtype
tf.float32
```

索引和NumPy中很像：

```bash
>>> t[:, 1:]
<tf.Tensor: id=5, shape=(2, 2), dtype=float32, numpy=
array([[2., 3.],
       [5., 6.]], dtype=float32)>
>>> t[..., 1, tf.newaxis]
<tf.Tensor: id=15, shape=(2, 1), dtype=float32, numpy=
array([[2.],
       [5.]], dtype=float32)>
```

最重要的，所有张量运算都可以执行：

```bash
>>> t + 10
<tf.Tensor: id=18, shape=(2, 3), dtype=float32, numpy=
array([[11., 12., 13.],
       [14., 15., 16.]], dtype=float32)>
>>> tf.square(t)
<tf.Tensor: id=20, shape=(2, 3), dtype=float32, numpy=
array([[ 1.,  4.,  9.],
       [16., 25., 36.]], dtype=float32)>
>>> t @ tf.transpose(t)
<tf.Tensor: id=24, shape=(2, 2), dtype=float32, numpy=
array([[14., 32.],
       [32., 77.]], dtype=float32)>
```

可以看到，`t + 10`等同于调用`tf.add(t, 10)`，`-`和`*`也支持。`@`运算符是在Python3.5中出现的，用于矩阵乘法，等同于调用函数`tf.matmul()`。

可以在tf中找到所有基本的数学运算（`tf.add()`、`tf.multiply()`、`tf.square()`、`tf.exp()`、`tf.sqrt()`），以及NumPy中的大部分运算（比如`tf.reshape()`、`tf.squeeze()`、`tf.tile()`）。

一些tf中的函数与NumPy中不同，例如，`tf.reduce_mean()`、`tf.reduce_sum()`、`tf.reduce_max()`、`tf.math.log()`等同于`np.mean()`、`np.sum()`、`np.max()`和`np.log()`。

当函数名不同时，通常都是有原因的。例如，TensorFlow中必须使用`tf.transpose(t)`，不能像NumPy中那样使用`t.T`。原因是函数`tf.transpose(t)`所做的和NumPy的属性`T`并不完全相同：在TensorFlow中，是使用转置数据的复制来生成张量的，而在NumPy中，`t.T`是数据的转置视图。相似的，`tf.reduce_sum()`操作之所以这么命名，是因为它的GPU核（即GPU实现）所采用的reduce算法不能保证元素相加的顺序，因为32位的浮点数精度有限，每次调用的结果可能会有细微的不同。`tf.reduce_mean()`也是这样（`tf.reduce_max()`结果是确定的）。

> 笔记：许多函数和类都有假名。比如，`tf.add()`和`tf.math.add()`是相同的。这可以让TensorFlow对于最常用的操作有简洁的名字，同时包可以有序安置。

> Keras的低级API
>  Keras API 有自己的低级API，位于`keras.backend`，包括：函数`square()`、`exp()`、`sqrt()`。在`tf.keras`中，这些函数通常通常只是调用对应的TensorFlow操作。如果你想写一些可以迁移到其它Keras实现上，就应该使用这些Keras函数。但是这些函数不多，所以这本书里就直接使用TensorFlow的运算了。下面是一个简单的使用了`keras.backend`的例子，简记为`k`：
>
> ```python
> >>> from tensorflow import keras
> >>> K = keras.backend
> >>> K.square(K.transpose(t)) + 10
> <tf.Tensor: id=39, shape=(3, 2), dtype=float32, numpy=
> array([[11., 26.],
>        [14., 35.],
>        [19., 46.]], dtype=float32)>
> ```



### 张量和NumPy

张量和NumPy融合地非常好：使用NumPy数组可以创建张量，张量也可以创建NumPy数组。可以在NumPy数组上运行TensorFlow运算，也可以在张量上运行NumPy运算：

```bash
>>> a = np.array([2., 4., 5.])
>>> tf.constant(a)
<tf.Tensor: id=111, shape=(3,), dtype=float64, numpy=array([2., 4., 5.])>
>>> t.numpy() # 或 np.array(t)
array([[1., 2., 3.],
       [4., 5., 6.]], dtype=float32)
>>> tf.square(a)
<tf.Tensor: id=116, shape=(3,), dtype=float64, numpy=array([4., 16., 25.])>
>>> np.square(t)
array([[ 1.,  4.,  9.],
       [16., 25., 36.]], dtype=float32)
```

> 警告：NumPy默认使用64位精度，TensorFlow默认用32位精度。这是因为32位精度通常对于神经网络就足够了，另外运行地更快，使用的内存更少。因此当你用NumPy数组创建张量时，一定要设置`dtype=tf.float32`。



### 类型转换

类型转换对性能的影响非常大，并且如果类型转换是自动完成的，不容易被注意到。为了避免这样，TensorFlow不会自动做任何类型转换：只是如果用不兼容的类型执行了张量运算，TensorFlow就会报异常。例如，不能用浮点型张量与整数型张量相加，也不能将32位张量与64位张量相加：

```python
>>> tf.constant(2.) + tf.constant(40)
Traceback[...]InvalidArgumentError[...]expected to be a float[...]
>>> tf.constant(2.) + tf.constant(40., dtype=tf.float64)
Traceback[...]InvalidArgumentError[...]expected to be a double[...]
```

这点可能一开始有点恼人，但是有其存在的理由。如果真的需要转换类型，可以使用`tf.cast()`：

```python
>>> t2 = tf.constant(40., dtype=tf.float64)
>>> tf.constant(2.0) + tf.cast(t2, tf.float32)
<tf.Tensor: id=136, shape=(), dtype=float32, numpy=42.0>
```



### 变量

到目前为止看到的`tf.Tensor`值都是不能修改的。意味着不能使用常规张量实现神经网络的权重，因为权重必须要能被反向传播调整。另外，其它的参数也需要随着时间调整（比如，动量优化器要跟踪过去的梯度）。此时需要的是`tf.Variable`：

```python
>>> v = tf.Variable([[1., 2., 3.], [4., 5., 6.]])
>>> v
<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=
array([[1., 2., 3.],
       [4., 5., 6.]], dtype=float32)>
```

`tf.Variable`和`tf.Tensor`很像：可以运行同样的运算，可以配合NumPy使用，也要注意类型。可以使用`assign()`方法对其就地修改（或`assign_add()`、`assign_sub()`）。使用切片的`assign()`方法可以修改独立的切片（直接赋值行不通），或使用`scatter_update()`、`scatter_nd_update()`方法：

```python
v.assign(2 * v)           # => [[2., 4., 6.], [8., 10., 12.]]
v[0, 1].assign(42)        # => [[2., 42., 6.], [8., 10., 12.]]
v[:, 2].assign([0., 1.])  # => [[2., 42., 0.], [8., 10., 1.]]
v.scatter_nd_update(indices=[[0, 0], [1, 2]], updates=[100., 200.])
                          # => [[100., 42., 0.], [8., 10., 200.]]
```

> 笔记：在实践中，很少需要手动创建变量，因为Keras有`add_weight()`方法可以自动来做。另外，模型参数通常会直接通过优化器更新，因此很少需要手动更新。



### 其它数据结构

TensorFlow还支持其它几种数据结构，如下（可以参考notebook的Tensors and Operations部分，或附录的F）：

稀疏张量（`tf.SparseTensor`）
 高效表示含有许多0的张量。`tf.sparse`包含有对稀疏张量的运算。

张量数组（`tf.TensorArray`）
 是张量的列表。有默认固定大小，但也可以做成动态的。列表中的张量必须形状相同，数据类型也相同。

嵌套张量（`tf.RaggedTensor`）
 张量列表的静态列表，张量的形状和数据结构相同。`tf.ragged`包里有嵌套张量的运算。

字符串张量
 类型是`tf.string`的常规张量，是字节串而不是Unicode字符串，因此如果你用Unicode字符串（比如，Python3字符串café）创建了一个字符串张量，就会自动被转换为UTF-8（b"caf\xc3\xa9"）。另外，也可以用`tf.int32`类型的张量表示Unicode字符串，其中每项表示一个Unicode码（比如，`[99, 97, 102, 233]`）。`tf.strings`包里有字节串和Unicode字符串的运算，以及二者转换的运算。要注意`tf.string`是原子性的，也就是说它的长度不出现在张量的形状中，一旦将其转换成了Unicode张量（即，含有Unicode码的`tf.int32`张量），长度才出现在形状中。

集合
 表示为常规张量（或稀疏张量）。例如`tf.constant([[1, 2], [3, 4]])`表示两个集合{1, 2}和{3, 4}。通常，用张量的最后一个轴的矢量表示集合。集合运算可以用`tf.sets`包。

队列
 用来在多个步骤之间保存张量。TensorFlow提供了多种队列。先进先出（FIFO）队列FIFOQueue，优先级队列PriorityQueue，随机队列RandomShuffleQueue，通过填充的不同形状的批次项队列PaddingFIFOQueue。这些队列都在`tf.queue`包中。

有了张量、运算、变量和各种数据结构，就可以开始自定义模型和训练算法啦！



## 自定义模型和训练算法

先从简单又常见的任务开始，创建一个自定义的损失函数。

### 自定义损失函数

假如你想训练一个回归模型，但训练集有噪音。你当然可以通过清除或修正异常值来清理数据集，但是这样还不够：数据集还是有噪音。此时，该用什么损失函数呢？均方差可能对大误差惩罚过重，导致模型不准确。均绝对值误差不会对异常值惩罚过重，但训练可能要比较长的时间才能收敛，训练模型也可能不准确。此时使用Huber损失（第10章介绍过）就比MSE好多了。目前官方Keras API中没有Huber损失，但tf.keras有（使用类`keras.losses.Huber`的实例）。就算tf.keras没有，实现也不难！只需创建一个函数，参数是标签和预测值，使用TensorFlow运算计算每个实例的损失：

```python
def huber_fn(y_true, y_pred):
    error = y_true - y_pred
    is_small_error = tf.abs(error) < 1
    squared_loss = tf.square(error) / 2
    linear_loss  = tf.abs(error) - 0.5
    return tf.where(is_small_error, squared_loss, linear_loss)
```

> 警告：要提高性能，应该像这个例子使用矢量。另外，如果想利用TensorFlow的图特性，则只能使用TensorFlow运算。

最好返回一个包含实例的张量，其中每个实例都有一个损失，而不是返回平均损失。这么做的话，Keras可以在需要时，使用类权重或样本权重（见第10章）。

现在，编译Keras模型时，就可以使用Huber损失来训练了：

```bash
model.compile(loss=huber_fn, optimizer="nadam")
model.fit(X_train, y_train, [...])
```

仅此而已！对于训练中的每个批次，Keras会调用函数`huber_fn()`计算损失，用损失来做梯度下降。另外，Keras会从一开始跟踪总损失，并展示平均损失。

在保存这个模型时，这个自定义损失会发生什么呢？



### 保存并加载包含自定义组件的模型

因为Keras可以保存函数名，保存含有自定义损失函数的模型也不成问题。当加载模型时，你需要提供一个字典，这个字典可以将函数名和真正的函数映射起来。一般说来，当加载一个含有自定义对象的模型时，你需要将名字映射到对象上：

```bash
model = keras.models.load_model("my_model_with_a_custom_loss.h5",
                                custom_objects={"huber_fn": huber_fn})
```

对于刚刚的代码，在-1和1之间的误差被认为是“小”误差。如果要改变阈值呢？一个解决方法是创建一个函数，它可以产生一个可配置的损失函数：

```python
def create_huber(threshold=1.0):
    def huber_fn(y_true, y_pred):
        error = y_true - y_pred
        is_small_error = tf.abs(error) < threshold
        squared_loss = tf.square(error) / 2
        linear_loss  = threshold * tf.abs(error) - threshold**2 / 2
        return tf.where(is_small_error, squared_loss, linear_loss)
    return huber_fn
model.compile(loss=create_huber(2.0), optimizer="nadam")
```

但在保存模型时，`threshold`不能被保存。这意味在加载模型时（注意，给Keras的函数名是“Huber_fn”，不是创造这个函数的函数名），必须要指定`threshold`的值：

```bash
model = keras.models.load_model("my_model_with_a_custom_loss_threshold_2.h5",
                                custom_objects={"huber_fn": create_huber(2.0)})
```

要解决这个问题，可以创建一个`keras.losses.Loss`类的子类，然后实现`get_config()`方法：

```ruby
class HuberLoss(keras.losses.Loss):
    def __init__(self, threshold=1.0, **kwargs):
        self.threshold = threshold
        super().__init__(**kwargs)
    def call(self, y_true, y_pred):
        error = y_true - y_pred
        is_small_error = tf.abs(error) < self.threshold
        squared_loss = tf.square(error) / 2
        linear_loss  = self.threshold * tf.abs(error) - self.threshold**2 / 2
        return tf.where(is_small_error, squared_loss, linear_loss)
    def get_config(self):
        base_config = super().get_config()
        return {**base_config, "threshold": self.threshold}
```

> 警告：Keras API目前只使用子类来定义层、模型、调回和正则器。如果使用子类创建其它组件（比如损失、指标、初始化器或约束），它们不能迁移到其它Keras实现上。可能Keras API经过更新，就会支持所有组件了。

逐行看下这段代码：

- 构造器接收`**kwargs`，并将其传递给父构造器，父构造器负责处理超参数：损失的`name`，要使用的、用于将单个实例的损失汇总的`reduction`算法。默认情况下是`"sum_over_batch_size"`，意思是损失是各个实例的损失之和，如果有样本权重，则做权重加权，再除以批次大小（不是除以权重之和，所以不是加权平均）。其它可能的值是`"sum"`和`None`。
- `call()`方法接受标签和预测值，计算所有实例的损失，并返回。
- `get_config()`方法返回一个字典，将每个超参数映射到值上。它首先调用父类的`get_config()`方法，然后将新的超参数加入字典（`{**x}语法是Python 3.5引入的`）。

当编译模型时，可以使用这个类的实例：

```bash
model.compile(loss=HuberLoss(2.), optimizer="nadam")
```

保存模型时，阈值会一起保存；加载模型时，只需将类名映射到具体的类上：

```bash
model = keras.models.load_model("my_model_with_a_custom_loss_class.h5",
                                custom_objects={"HuberLoss": HuberLoss})
```

保存模型时，Keras调用损失实例的`get_config()`方法，将配置以JSON的形式保存在HDF5中。当加载模型时，会调用`HuberLoss`类的`from_config()`方法：这个方法是父类`Loss`实现的，创建一个类`Loss`的实例，将`**config`传递给构造器。



### 自定义激活函数、初始化器、正则器和约束

Keras的大多数功能，比如损失、正则器、约束、初始化器、指标、激活函数、层，甚至是完整的模型，都可以用相似的方法做自定义。大多数时候，需要写一个简单的函数，带有合适的输入和输出。下面的例子是自定义激活函数（等价于`keras.activations.softplus()`或`tf.nn.softplus()`），自定义Glorot初始化器（等价于`keras.initializers.glorot_normal()`），自定义ℓ1正则化器（等价于`keras.regularizers.l1(0.01)`），可以保证权重都是正值的自定义约束（等价于`equivalent to keras.constraints.nonneg()`或`tf.nn.relu()`）：

```python
def my_softplus(z): # return value is just tf.nn.softplus(z)
    return tf.math.log(tf.exp(z) + 1.0)

def my_glorot_initializer(shape, dtype=tf.float32):
    stddev = tf.sqrt(2. / (shape[0] + shape[1]))
    return tf.random.normal(shape, stddev=stddev, dtype=dtype)

def my_l1_regularizer(weights):
    return tf.reduce_sum(tf.abs(0.01 * weights))

def my_positive_weights(weights): # return value is just tf.nn.relu(weights)
    return tf.where(weights < 0., tf.zeros_like(weights), weights)
```

可以看到，参数取决于自定义函数的类型。这些自定义函数可以如常使用，例如：

```python
layer = keras.layers.Dense(30, activation=my_softplus,
                           kernel_initializer=my_glorot_initializer,
                           kernel_regularizer=my_l1_regularizer,
                           kernel_constraint=my_positive_weights)
```

激活函数会应用到这个`Dense`层的输出上，结果会传递到下一层。层的权重会使用初始化器的返回值。在每个训练步骤，权重会传递给正则化函数以计算正则损失，这个损失会与主损失相加，得到训练的最终损失。最后，会在每个训练步骤结束后调用约束函数，经过约束的权重会替换层的权重。

如果函数有需要连同模型一起保存的超参数，需要对相应的类做子类，比如`keras.regularizers.Regularizer`，`keras.constraints.Constraint`，`keras.initializers.Initializer`，或 `keras.layers.Layer`（任意层，包括激活函数）。就像前面的自定义损失一样，下面是一个简单的ℓ1正则类，可以保存它的超参数`factor`（这次不必调用其父构造器或`get_config()`方法，因为它们不是父类定义的）：

```ruby
class MyL1Regularizer(keras.regularizers.Regularizer):
    def __init__(self, factor):
        self.factor = factor
    def __call__(self, weights):
        return tf.reduce_sum(tf.abs(self.factor * weights))
    def get_config(self):
        return {"factor": self.factor}
```

注意，你必须要实现损失、层（包括激活函数）和模型的`call()`方法，或正则化器、初始化器和约束的`__call__()`方法。对于指标，处理方法有所不同。



### 自定义指标

损失和指标的概念是不一样的：梯度下降使用损失（比如交叉熵损失）来训练模型，因此损失必须是可微分的（至少是在评估点可微分），梯度不能在所有地方都是0。另外，就算损失比较难解释也没有关系。相反的，指标（比如准确率）是用来评估模型的：指标的解释性一定要好，可以是不可微分的，或者可以在任何地方的梯度都是0。

但是，在多数情况下，定义一个自定义指标函数和定义一个自定义损失函数是完全一样的。事实上，刚才创建的Huber损失函数也可以用来当指标（持久化也是同样的，只需要保存函数名“Huber_fn”就成）：

```bash
model.compile(loss="mse", optimizer="nadam", metrics=[create_huber(2.0)])
```

对于训练中的每个批次，Keras能计算该指标，并跟踪自周期开始的指标平均值。大多数时候，这样没有问题。**但会有例外！比如，考虑一个二元分类器的准确性。**第3章介绍过，准确率是真正值除以正预测数（包括真正值和假正值）。假设模型在第一个批次做了5个正预测，其中4个是正确的，准确率就是80%。再假设模型在第二个批次做了3次正预测，但没有一个预测对，则准确率是0%。如果对这两个准确率做平均，则平均值是40%。但它不是模型在两个批次上的准确率！事实上，真正值总共有4个，正预测有8个，整体的准确率是50%。我们需要的是一个能跟踪真正值和正预测数的对象，用该对象计算准确率。这就是类`keras.metrics.Precision`所做的：

```xml
>>> precision = keras.metrics.Precision()
>>> precision([0, 1, 1, 1, 0, 1, 0, 1], [1, 1, 0, 1, 0, 1, 0, 1])
<tf.Tensor: id=581729, shape=(), dtype=float32, numpy=0.8>
>>> precision([0, 1, 0, 0, 1, 0, 1, 1], [1, 0, 1, 1, 0, 0, 0, 0])
<tf.Tensor: id=581780, shape=(), dtype=float32, numpy=0.5>
```

在这个例子中，我们创建了一个`Precision`对象，然后将其用作函数，将第一个批次的标签和预测传给它，然后传第二个批次的数据（这里也可以传样本权重）。数据和前面的真正值和正预测一样。第一个批次之后，正确率是80%；第二个批次之后，正确率是50%（这是完整过程的准确率，不是第二个批次的准确率）。这叫做流式指标（或者静态指标），因为他是一个批次接一个批次，逐次更新的。

任何时候，可以调用`result()`方法获取指标的当前值。还可以通过`variables`属性，查看指标的变量（跟踪正预测和负预测的数量），还可以用`reset_states()`方法重置变量：

```bash
>>> p.result()
<tf.Tensor: id=581794, shape=(), dtype=float32, numpy=0.5>
>>> p.variables
[<tf.Variable 'true_positives:0' [...] numpy=array([4.], dtype=float32)>,
 <tf.Variable 'false_positives:0' [...] numpy=array([4.], dtype=float32)>]
>>> p.reset_states() # both variables get reset to 0.0
```

如果想创建一个这样的**流式指标**，可以创建一个`keras.metrics.Metric`类的子类。下面的例子跟踪了完整的Huber损失，以及实例的数量。当查询结果时，就能返回比例值，该值就是平均Huber损失：

```ruby
class HuberMetric(keras.metrics.Metric):
    def __init__(self, threshold=1.0, **kwargs):
        super().__init__(**kwargs) # handles base args (e.g., dtype)
        self.threshold = threshold
        self.huber_fn = create_huber(threshold)
self.total = self.add_weight("total", initializer="zeros")
        self.count = self.add_weight("count", initializer="zeros")
    def update_state(self, y_true, y_pred, sample_weight=None):
        metric = self.huber_fn(y_true, y_pred)
        self.total.assign_add(tf.reduce_sum(metric))
        self.count.assign_add(tf.cast(tf.size(y_true), tf.float32))
    def result(self):
        return self.total / self.count
    def get_config(self):
        base_config = super().get_config()
        return {**base_config, "threshold": self.threshold}
```

逐行看下代码：

- 构造器使用`add_weight()`方法来创建用来跟踪多个批次的变量 —— 在这个例子中，就是Huber损失的和（`total`）和实例的数量（`count`）。如果愿意的话，可以手动创建变量。Keras会跟踪任何被设为属性的`tf.Variable`（更一般的讲，任何“可追踪对象”，比如层和模型）。
- 当将这个类的实例当做函数使用时会调用`update_state()`方法（正如`Precision`对象）。它能用每个批次的标签和预测值（还有样本权重，但这个例子忽略了样本权重）来更新变量。
- `result()`方法计算并返回最终值，在这个例子中，是返回所有实例的平均Huber损失。当你将指标用作函数时，`update_state()`方法先被调用，然后调用`result()`方法，最后返回输出。
- 还实现了`get_config()`方法，用以确保`threshold`和模型一起存储。
- `reset_states()`方法默认将所有值重置为0.0（也可以改为其它值）。

> 笔记：Keras能无缝处理变量持久化。

当用简单函数定义指标时，Keras会在每个批次自动调用它，还能跟踪平均值，就和刚才的手工处理一模一样。因此，`HuberMetric`类的唯一好处是`threshold`可以进行保存。当然，一些指标，比如准确率，不能简单的平均化；对于这些例子，只能实现一个流式指标。

创建好了流式指标，再创建自定义层就很简单了。



### 自定义层

有时候你可能想搭建一个架构，但TensorFlow没有提供默认实现。这种情况下，就需要创建自定义层。否则只能搭建出的架构会是简单重复的，包含相同且重复的层块，每个层块实际上就是一个层而已。比如，如果模型的层顺序是A、B、C、A、B、C、A、B、C，则完全可以创建一个包含A、B、C的自定义层D，模型就可以简化为D、D、D。

如何创建自定义层呢？首先，一些层没有权重，比如`keras.layers.Flatten`或`keras.layers.ReLU`。如果想创建一个没有任何权重的自定义层，最简单的方法是写一个函数，将其包装进`keras.layers.Lambda`层。比如，下面的层会对输入做指数运算：

```cpp
exponential_layer = keras.layers.Lambda(lambda x: tf.exp(x))
```

这个自定义层可以像任何其它层一样使用Sequential API、Functional API 或 Subclassing API。你还可以将其用作激活函数（或者使用`activation=tf.exp`，`activation=keras.activations.exponential`，或者`activation="exponential"`）。当预测值的数量级不同时，指数层有时用在回归模型的输出层。

你可能猜到了，要创建自定义状态层（即，有权重的层），需要创建`keras.layers.Layer`类的子类。例如，下面的类实现了一个紧密层的简化版本：

```ruby
class MyDense(keras.layers.Layer):
    def __init__(self, units, activation=None, **kwargs):
        super().__init__(**kwargs)
        self.units = units
        self.activation = keras.activations.get(activation)

    def build(self, batch_input_shape):
        self.kernel = self.add_weight(
            name="kernel", shape=[batch_input_shape[-1], self.units],
            initializer="glorot_normal")
        self.bias = self.add_weight(
            name="bias", shape=[self.units], initializer="zeros")
        super().build(batch_input_shape) # must be at the end

    def call(self, X):
        return self.activation(X @ self.kernel + self.bias)

    def compute_output_shape(self, batch_input_shape):
        return tf.TensorShape(batch_input_shape.as_list()[:-1] + [self.units])

    def get_config(self):
        base_config = super().get_config()
        return {**base_config, "units": self.units,
                "activation": keras.activations.serialize(self.activation)}
```

逐行看下代码：

- 构造器将所有超参数作为参数（这个例子中，是`units`和`activation`），更重要的，它还接收一个`**kwargs`参数。接着初始化了父类，传给父类`kwargs`：它负责标准参数，比如`input_shape`、`trainable`和`name`。然后将超参数存为属性，使用`keras.activations.get()`函数（这个函数接收函数、标准字符串，比如“relu”、“selu”、或“None”），将`activation`参数转换为合适的激活函数。
- `build()`方法通过对每个权重调用`add_weight()`方法，创建层的变量。层第一次被使用时，调用`build()`方法。此时，Keras能知道该层输入的形状，并传入`build()`方法，这对创建权重是必要的。例如，需要知道前一层的神经元数量，来创建连接权重矩阵（即，`"kernel"`）：对应的是输入的最后一维的大小。在`build()`方法最后（也只是在最后），必须调用父类的`build()`方法：这步告诉Keras这个层建好了（或者设定`self.built=True`）。
- `call()`方法执行预想操作。在这个例子中，计算了输入`X`和层的核的矩阵乘法，加上了偏置矢量，对结果使用了激活函数，得到了该层的输出。
- `compute_output_shape()`方法只是返回了该层输出的形状。在这个例子中，输出和输入的形状相同，除了最后一维被替换成了层的神经元数。在tf.keras中，形状是`tf.TensorShape`类的实例，可以用`as_list()`转换为Python列表。
- `get_config()`方法和前面的自定义类很像。注意是通过调用`keras.activations.serialize()`，保存了激活函数的完整配置。

现在，就可以像其它层一样，使用`MyDense`层了！

> 笔记：一般情况下，可以忽略`compute_output_shape()`方法，因为tf.keras能自动推断输出的形状，除非层是动态的（后面会看到动态层）。在其它Keras实现中，要么需要`compute_output_shape()`方法，要么默认输出形状和输入形状相同。

要创建一个有多个输入（比如`Concatenate`）的层，`call()`方法的参数应该是包含所有输入的元组。相似的，`compute_output_shape()`方法的参数应该是一个包含每个输入的批次形状的元组。要创建一个有多输出的层，`call()`方法要返回输出的列表，`compute_output_shape()`方法要返回批次输出形状的列表（每个输出一个形状）。例如，下面的层有两个输入和三个输出：

```ruby
class MyMultiLayer(keras.layers.Layer):
    def call(self, X):
        X1, X2 = X
        return [X1 + X2, X1 * X2, X1 / X2]

    def compute_output_shape(self, batch_input_shape):
        b1, b2 = batch_input_shape
        return [b1, b1, b1] # 可能需要处理广播规则
```

这个层现在就可以像其它层一样使用了，但只能使用Functional和Subclassing API，Sequential API不成（只能使用单输入和单输出的层）。

如果你的层需要在训练和测试时有不同的行为（比如，如果使用`Dropout` 或 `BatchNormalization`层），那么必须给`call()`方法加上`training`参数，用这个参数确定该做什么。比如，创建一个在训练中（为了正则）添加高斯噪音的层，但不改动训练（Keras有一个层做了同样的事，`keras.layers.GaussianNoise`）：

```ruby
class MyGaussianNoise(keras.layers.Layer):
    def __init__(self, stddev, **kwargs):
        super().__init__(**kwargs)
        self.stddev = stddev

    def call(self, X, training=None):
        if training:
            noise = tf.random.normal(tf.shape(X), stddev=self.stddev)
            return X + noise
        else:
            return X

    def compute_output_shape(self, batch_input_shape):
        return batch_input_shape
```

上面这些就能让你创建自定义层了！接下来看看如何创建自定义模型。



### 自定义模型

第10章在讨论Subclassing API时，接触过创建自定义模型的类。说白了：创建`keras.Model`类的子类，创建层和变量，用`call()`方法完成模型想做的任何事。假设你想搭建一个图12-3中的模型。

![img](https:////upload-images.jianshu.io/upload_images/7178691-647e362f4abb081e.png?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp)

图12-3 自定义模型案例：包含残差块层，残块层含有跳连接

输入先进入一个紧密层，然后进入包含两个紧密层和一个添加操作的残差块（第14章会看见，残差块将输入和输出相加），经过3次同样的残差块，再通过第二个残差块，最终结果通过一个紧密输出层。这个模型没什么意义，只是一个搭建任意结构（包含循环和跳连接）模型的例子。要实现这个模型，最好先创建`ResidualBlock`层，因为这个层要用好几次：

```ruby
class ResidualBlock(keras.layers.Layer):
    def __init__(self, n_layers, n_neurons, **kwargs):
        super().__init__(**kwargs)
        self.hidden = [keras.layers.Dense(n_neurons, activation="elu",
                                          kernel_initializer="he_normal")
                       for _ in range(n_layers)]

    def call(self, inputs):
        Z = inputs
        for layer in self.hidden:
            Z = layer(Z)
        return inputs + Z
```

这个层稍微有点特殊，因为它包含了其它层。用Keras来实现：自动检测`hidden`属性包含可追踪对象（即，层），内含层的变量可以自动添加到整层的变量列表中。类的其它部分很好懂。接下来，使用Subclassing API定义模型：

```ruby
class ResidualRegressor(keras.Model):
    def __init__(self, output_dim, **kwargs):
        super().__init__(**kwargs)
        self.hidden1 = keras.layers.Dense(30, activation="elu",
                                          kernel_initializer="he_normal")
        self.block1 = ResidualBlock(2, 30)
        self.block2 = ResidualBlock(2, 30)
        self.out = keras.layers.Dense(output_dim)

    def call(self, inputs):
        Z = self.hidden1(inputs)
        for _ in range(1 + 3):
            Z = self.block1(Z)
        Z = self.block2(Z)
        return self.out(Z)
```

在构造器中创建层，在`call()`方法中使用。这个模型可以像其它模型那样来使用（编译、拟合、评估、预测）。如果你还想使用`save()`方法保存模型，使用`keras.models.load_model()`方法加载模型，则必须在`ResidualBlock`类和`ResidualRegressor`类中实现`get_config()`方法。另外，可以使用`save_weights()`方法和`load_weights()`方法保存和加载权重。

`Model`类是`Layer`类的子类，因此模型可以像层一样定义和使用。但是模型还有一些其它的功能，包括`compile()`、`fit()`、`evaluate()` 和`predict()`（还有一些变量），还有`get_layers()`方法（它能通过名字或序号返回模型的任意层）、`save()`方法（支持`keras.models.load_model()`和`keras.models.clone_model()`）。

> 提示：如果模型提供的功能比层多，为什么不讲每一个层定义为模型呢？技术上当然可以这么做，但对内部组件和模型（即，层或可重复使用的层块）加以区别，可以更加清晰。前者应该是`Layer`类的子类，后者应该是`Model`类的子类。

掌握了上面的方法，你就可以使用Sequential API、Functional API、Subclassing API搭建几乎任何文章上的模型了。为什么是“几乎”？因为还有些内容需要掌握：首先，如何基于模型内部定义损失或指标，第二，如何搭建自定义训练循环。



### 基于模型内部的损失和指标

前面的自定义损失和指标都是基于标签和预测（或者还有样本权重）。有时，你可能想基于模型的其它部分定义损失，比如隐藏层的权重或激活函数。这么做，可以是处于正则的目的，或监督模型的内部。

要基于模型内部自定义损失，需要先做基于这些组件的计算，然后将结果传递给`add_loss()`方法。例如，自定义一个包含五个隐藏层加一个输出层的回归MLP模型。这个自定义模型基于上层的隐藏层，还有一个辅助的输出。和辅助输出关联的损失，被称为重建损失（见第17章）：它是重建和输入的均方差。**通过将重建误差添加到主损失上，可以鼓励模型通过隐藏层保留尽量多的信息，即便是那些对回归任务没有直接帮助的信息。在实际中，重建损失有助于提高泛化能力（它是一个正则损失）。**下面是含有自定义重建损失的自定义模型：

```ruby
class ReconstructingRegressor(keras.Model):
    def __init__(self, output_dim, **kwargs):
        super().__init__(**kwargs)
        self.hidden = [keras.layers.Dense(30, activation="selu",
                                          kernel_initializer="lecun_normal")
                       for _ in range(5)]
        self.out = keras.layers.Dense(output_dim)

    def build(self, batch_input_shape):
        n_inputs = batch_input_shape[-1]
        self.reconstruct = keras.layers.Dense(n_inputs)
        super().build(batch_input_shape)

    def call(self, inputs):
        Z = inputs
        for layer in self.hidden:
            Z = layer(Z)
        reconstruction = self.reconstruct(Z)
        recon_loss = tf.reduce_mean(tf.square(reconstruction - inputs))
        self.add_loss(0.05 * recon_loss)
        return self.out(Z)
```

逐行看下代码：

- 构造器搭建了一个有五个紧密层和一个紧密输出层的DNN。
- `build()`方法创建了另一个紧密层，可以重建模型的输入。必须要在这里创建`build()`方法的原因，是单元的数量必须等于输入数，而输入数在调用`build()`方法之前是不知道的。
- `call()`方法处理所有五个隐藏层的输入，然后将结果传给重建层，重建层产生重建。
- `call()`方法然后计算重建损失（重建和输入的均方差），然后使用`add_loss()`方法，将其加到模型的损失列表上。注意，这里对重建损失乘以了0.05（这是个可调节的超参数），做了缩小，以确保重建损失不主导主损失。
- 最后，`call()`方法将隐藏层的输出传递给输出层，然后返回输出。

相似的，可以加上一个基于模型内部的自定义指标。例如，可以在构造器中创建一个`keras.metrics.Mean`对象，然后在`call()`方法中调用它，传递给它`recon_loss`，最后通过`add_metric()`方法，将其添加到模型上。使用这种方式，在训练模型时，Keras能展示每个周期的平均损失（损失是主损失加上0，05乘以重建损失），和平均重建误差。两者都会在训练过程中下降：

```csharp
Epoch 1/5
11610/11610 [=============] [...] loss: 4.3092 - reconstruction_error: 1.7360
Epoch 2/5
11610/11610 [=============] [...] loss: 1.1232 - reconstruction_error: 0.8964
[...]
```

在超过99%的情况中，前面所讨论的内容已经足够搭建你想要的模型了，就算是包含复杂架构、损失和指标也行。但是，在某些极端情况，你还需要自定义训练循环。介绍之前，先来看看TensorFlow如何自动计算梯度。



### 使用自动微分计算梯度

要搞懂如何使用自动微分自动计算梯度，来看一个例子：

```python
def f(w1, w2):
    return 3 * w1 ** 2 + 2 * w1 * w2
```

如果你会微积分，就能算出这个函数对`w1`的偏导是`6 * w1 + 2 * w2`，还能算出它对`w2`的偏导是`2 * w1`。例如，在点`(w1, w2) = (5, 3)`，这两个偏导数分别是36和10，在这个点的梯度矢量就是（36, 10）。但对于神经网络来说，函数会复杂得多，可能会有上万个参数，用手算偏导几乎是不可能的任务。一个解决方法是计算每个偏导的大概值，通过调节参数，查看输出的变化：

```python
>>> w1, w2 = 5, 3
>>> eps = 1e-6
>>> (f(w1 + eps, w2) - f(w1, w2)) / eps
36.000003007075065
>>> (f(w1, w2 + eps) - f(w1, w2)) / eps
10.000000003174137
```

这种方法很容易实现，但只是大概。重要的是，需要对每个参数至少要调用一次`f()`（不是至少两次，因为可以只计算一次`f(w1, w2)`）。这样，对于大神经网络，就不怎么可控。所以，应该使用自动微分。TensorFlow的实现很简单：

```python
w1, w2 = tf.Variable(5.), tf.Variable(3.)
with tf.GradientTape() as tape:
    z = f(w1, w2)

gradients = tape.gradient(z, [w1, w2])
```

先定义了两个变量`w1` 和 `w2`，然后创建了一个`tf.GradientTape`上下文，它能自动记录变量的每个操作，最后使用它算出结果`z`关于两个变量`[w1, w2]`的梯度。TensorFlow计算的梯度如下：

```python
>>> gradients
[<tf.Tensor: id=828234, shape=(), dtype=float32, numpy=36.0>,
 <tf.Tensor: id=828229, shape=(), dtype=float32, numpy=10.0>]
```

很好！不仅结果是正确的（准确度只受浮点误差限制），`gradient()`方法只逆向算了一次，无论有多少个变量，效率很高。

> 提示：为了节省内存，只将严格的最小值放在`tf.GradientTape()`中。另外，通过`在tf.GradientTape()`中创建一个`tape.stop_recording()`来暂停记录。

当调用记录器的`gradient()`方法时，记录器会自动清零，所以调用两次`gradient()`就会报错：

```python
with tf.GradientTape() as tape:
    z = f(w1, w2)

dz_dw1 = tape.gradient(z, w1) # => tensor 36.0
dz_dw2 = tape.gradient(z, w2) # 运行时错误
```

如果需要调用`gradient()`一次以上，比续将记录器持久化，并在每次用完之后删除，释放资源：

```python
with tf.GradientTape(persistent=True) as tape:
    z = f(w1, w2)

dz_dw1 = tape.gradient(z, w1) # => tensor 36.0
dz_dw2 = tape.gradient(z, w2) # => tensor 10.0, works fine now!
del tape
```

默认情况下，记录器只会跟踪包含变量的操作，所以如果是计算`z`的梯度，`z`和变量没关系，结果就会是None：

```python
c1, c2 = tf.constant(5.), tf.constant(3.)
with tf.GradientTape() as tape:
    z = f(c1, c2)

gradients = tape.gradient(z, [c1, c2]) # returns [None, None]
```

但是，你也可以强制记录器监视任何你想监视的张量，将它们当做变量来计算梯度：

```python
with tf.GradientTape() as tape:
BL.INK - Login(c1)
BL.INK - Login(c2)
    z = f(c1, c2)

gradients = tape.gradient(z, [c1, c2]) # returns [tensor 36., tensor 10.]
```

在某些情况下，这么做会有帮助，比如当输入的波动很小，而激活函数结果波动很大时，要实现一个正则损失，就可以这么做：损失会基于激活函数结果，激活函数结果会基于输入。因为输入不是变量，就需要记录器监视输入。

大多数时候，梯度记录器被用来计算单一值（通常是损失）的梯度。这就是自动微分发挥长度的地方了。因为自动微分只需要一次向前传播一次向后传播，就能计算所有梯度。如果你想计算一个矢量的梯度，比如一个包含多个损失的矢量，TensorFlow就会计算矢量和的梯度。因此，如果你需要计算单个梯度的话（比如每个损失相对于模型参数的梯度），你必须调用记录器的`jabobian()`方法：它能做反向模式的自动微分，一次计算完矢量中的所有损失（默认是并行的）。甚至还可以计算二级偏导，但在实际中用的不多（见notebook中的“自动微分计算梯度部分”）。

某些情况下，你可能想让梯度在部分神经网络停止传播。要这么做的话，必须使用`tf.stop_gradient()`函数。它能在前向传播中（比如`tf.identity()`）返回输入，并能阻止梯度反向传播（就像常量一样）：

```python
def f(w1, w2):
    return 3 * w1 ** 2 + tf.stop_gradient(2 * w1 * w2)

with tf.GradientTape() as tape:
    z = f(w1, w2) # same result as without stop_gradient()

gradients = tape.gradient(z, [w1, w2]) # => returns [tensor 30., None]
```

最后，在计算梯度时可能还会碰到数值问题。例如，如果对于很大的输入，计算`my_softplus()`函数的梯度，结果会是NaN：

```python
>>> x = tf.Variable([100.])
>>> with tf.GradientTape() as tape:
...     z = my_softplus(x)
...
>>> tape.gradient(z, [x])
<tf.Tensor: [...] numpy=array([nan], dtype=float32)>
```

这是因为使用自动微分计算这个函数的梯度，会有些数值方面的难点：因为浮点数的精度误差，自动微分最后会变成无穷除以无穷（结果是NaN）。幸好，softplus函数的导数是`1 / (1 + 1 / exp(x))`，它是数值稳定的。接着，让TensorFlow使用这个稳定的函数，通过装饰器`@tf.custom_gradient`计算`my_softplus()`的梯度，既返回正常输出，也返回计算导数的函数（注意：它会接收的输入是反向传播的梯度；根据链式规则，应该乘以函数的梯度）：

```python
@tf.custom_gradient
def my_better_softplus(z):
    exp = tf.exp(z)
    def my_softplus_gradients(grad):
        return grad / (1 + 1 / exp)
    return tf.math.log(exp + 1), my_softplus_gradients
```

计算好了`my_better_softplus()`的梯度，就算对于特别大的输入值，也能得到正确的结果（但是，因为指数运算，主输出还是会发生爆炸；绕过的方法是，当输出很大时，使用`tf.where()`返回输入）。

祝贺你！现在你就可以计算任何函数的梯度（只要函数在计算点可微就行），甚至可以阻止反向传播，还能写自己的梯度函数！TensorFlow的灵活性还能让你编写自定义的训练循环。



### 自定义训练循环

在某些特殊情况下，`fit()`方法可能不够灵活。例如，第10章讨论过的Wide & Deep论文使用了两个优化器：一个用于宽路线，一个用于深路线。因为`fit()`方法只能使用一个优化器（编译时设置的优化器），要实现这篇论文就需要写自定义循环。

你可能还想写自定义的训练循环，只是想让训练过程更加可控（也许你对`fit()`方法的细节并不确定）。但是，自定义训练循环会让代码变长、更容易出错、也难以维护。

> 提示：除非真的需要自定义，最好还是使用`fit()`方法，而不是自定义训练循环，特别是当你是在一个团队之中时。

首先，搭建一个简单的模型。不用编译，因为是要手动处理训练循环：

```bash
l2_reg = keras.regularizers.l2(0.05)
model = keras.models.Sequential([
    keras.layers.Dense(30, activation="elu", kernel_initializer="he_normal",
                       kernel_regularizer=l2_reg),
    keras.layers.Dense(1, kernel_regularizer=l2_reg)
])
```

接着，创建一个小函数，它能从训练集随机采样一个批次的实例（第13章会讨论更便捷的Data API）：

```python
def random_batch(X, y, batch_size=32):
    idx = np.random.randint(len(X), size=batch_size)
    return X[idx], y[idx]
```

再定义一个可以展示训练状态的函数，包括步骤数、总步骤数、平均损失（用`Mean`指标计算），和其它指标：

```python
def print_status_bar(iteration, total, loss, metrics=None):
    metrics = " - ".join(["{}: {:.4f}".format(m.name, m.result())
                         for m in [loss] + (metrics or [])])
    end = "" if iteration < total else "\n"
    print("\r{}/{} - ".format(iteration, total) + metrics,
          end=end)
```

这段代码不难，除非你对Python字符串的`{:.4f}`不熟：它的作用是保留四位小数。使用`\r`（回车）和`end=""`连用，保证状态条总是打印在一条线上。notebook中，`print_status_bar()`函数包括进度条，也可以使用`tqdm`库。

有了这些准备，就可以开干了！首先，我们定义超参数、选择优化器、损失函数和指标（这个例子中是MAE）：

```python
n_epochs = 5
batch_size = 32
n_steps = len(X_train) // batch_size
optimizer = keras.optimizers.Nadam(lr=0.01)
loss_fn = keras.losses.mean_squared_error
mean_loss = keras.metrics.Mean()
metrics = [keras.metrics.MeanAbsoluteError()]
```

可以搭建自定义循环了：

```python
for epoch in range(1, n_epochs + 1):
    print("Epoch {}/{}".format(epoch, n_epochs))
    for step in range(1, n_steps + 1):
        X_batch, y_batch = random_batch(X_train_scaled, y_train)
        with tf.GradientTape() as tape:
            y_pred = model(X_batch, training=True)
            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))
            loss = tf.add_n([main_loss] + model.losses)
        gradients = tape.gradient(loss, model.trainable_variables)
        optimizer.apply_gradients(zip(gradients, model.trainable_variables))
        mean_loss(loss)
        for metric in metrics:
            metric(y_batch, y_pred)
        print_status_bar(step * batch_size, len(y_train), mean_loss, metrics)
    print_status_bar(len(y_train), len(y_train), mean_loss, metrics)
    for metric in [mean_loss] + metrics:
        metric.reset_states()
```

逐行看下代码：

- 创建了两个嵌套循环：一个是给周期的，一个是给周期里面的批次的。
- 然后从训练集随机批次采样。
- 在`tf.GradientTape()`内部，对一个批次做了预测（将模型用作函数），计算其损失：损失等于主损失加上其它损失（在这个模型中，每层有一个正则损失）。因为`mean_squared_error()`函数给每个实例返回一个损失，使用`tf.reduce_mean()`计算平均值（如果愿意的话，每个实例可以用不同的权重）。正则损失已经转变为单个的标量，所以只需求和就成（使用`tf.add_n()`，它能将相同形状和数据类型的张量求和）。
- 接着，让记录器计算损失相对于每个可训练变量的梯度（不是所有的变量！），然后用优化器对梯度做梯度下降。
- 然后，更新（当前周期）平均损失和平均指标，显示状态条。
- 在每个周期结束后，再次展示状态条，使其完整，然后换行，重置平均损失和平均指标。

如果设定优化器的`clipnorm`或`clipvalue`超参数，就可以自动重置。如果你想对梯度做任何其它变换，在调用`apply_gradients()`方法之前，做变换就行。

如果你对模型添加了权重约束（例如，添加层时设置`kernel_constraint`或`bias_constraint`），你需要在`apply_gradients()`之后，更新训练循环，以应用这些约束：

```python
for variable in model.variables:
    if variable.constraint is not None:
        variable.assign(variable.constraint(variable))
```

最重要的，这个训练循环没有处理训练和测试过程中，行为不一样的层（例如，`BatchNormalization`或`Dropout`）。要处理的话，需要调用模型，令`training=True`，并传播到需要这么设置的每一层。

可以看到，有这么多步骤都要做对才成，很容易出错。但另一方面，训练的控制权完全在你手里。

现在你知道如何自定义模型中的任何部分了，也知道如何训练算法了，接下来看看如何使用TensorFlow的自动图生成特征：它能显著提高自定义代码的速度，并且还是可迁移的（见第19章）。



## TensorFlow的函数和图

在TensorFlow 1 中，图是绕不过去的（同时图也很复杂），因为图是TensorFlow的API的核心。在TensorFlow 2 中，图还在，但不是核心了，使用也简单多了。为了演示其易用性，从一个三次方函数开始：

```python
def cube(x):
    return x ** 3
```

可以用一个值调用这个函数，整数、浮点数都成，或者用张量来调用：

```python
>>> cube(2)
8
>>> cube(tf.constant(2.0))
<tf.Tensor: id=18634148, shape=(), dtype=float32, numpy=8.0>
```

现在，使用`tf.function()`将这个Python函数变为TensorFlow函数：

```python
>>> tf_cube = tf.function(cube)
>>> tf_cube
<tensorflow.python.eager.def_function.Function at 0x1546fc080>
```

可以像原生Python函数一样使用这个TF函数，可以返回同样的结果（张量）：

```python
>>> tf_cube(2)
<tf.Tensor: id=18634201, shape=(), dtype=int32, numpy=8>
>>> tf_cube(tf.constant(2.0))
<tf.Tensor: id=18634211, shape=(), dtype=float32, numpy=8.0>
```

`tf.function()`在底层分析了`cube()`函数的计算，然后生成了一个等价的计算图！可以看到，过程十分简单（下面会讲解过程）。另外，也可以使用`tf.function`作为装饰器，更常见一些：

```python
@tf.function
def tf_cube(x):
    return x ** 3
```

原生的Python函数通过TF函数的`python_function`属性仍然可用：

```python
>>> tf_cube.python_function(2)
8
```

TensorFlow优化了计算图，删掉了没用的节点，简化了表达式（比如，1 + 2会替换为3），等等。当优化好的计算图准备好之后，TF函数可以在图中，按合适的顺序高效执行运算（该并行的时候就并行）。作为结果，TF函数比普通的Python函数快的多，特别是在做复杂计算时。大多数时候，根本没必要知道底层到底发生了什么，如果需要对Python函数加速，将其转换为TF函数就行。

另外，当你写的自定义损失函数、自定义指标、自定义层或任何其它自定义函数，并在Keras模型中使用的，Keras都自动将其转换成了TF函数，不用使用`tf.function()`。

> 提示：创建自定义层或模型时，设置`dynamic=True`，可以让Keras不转化你的Python函数。另外，当调用模型的`compile()`方法时，可以设置`run_eagerly=True`。

默认时，TF函数对每个独立输入的形状和数据类型的集合，生成了一个新的计算图，并缓存以备后续使用。例如，如果你调用`tf_cube(tf.constant(10))`，就会生成一个int32张量、形状是[]的计算图。如果你调用`tf_cube(tf.constant(20))`，会使用相同的计算图。但如果调用`tf_cube(tf.constant([10, 20]))`，就会生成一个int32、形状是[2]的新计算图。这就是TF如何处理多态的（即变化的参数类型和形状）。但是，这只适用于张量参数：如果你将Python数值传给TF，就会为每个独立值创建一个计算图：比如，调用`tf_cube(10)`和`tf_cube(20)`会产生两个计算图。

> 警告：如果用多个不同的Python数值调用TF函数，就会产生多个计算图，这样会减慢程序，使用很多的内存（必须删掉TF函数才能释放）。Python的值应该赋值给尽量重复的参数，比如超参数，每层有多少个神经元。这可以让TensorFlow更好的优化模型中的变量。



### 自动图和跟踪

TensorFlow是如何生成计算图的呢？它先分析了Python函数源码，得出所有的数据流控制语句，比如for循环，while循环，if条件，还有break、continue、return。这个第一步被称为自动图（AutoGraph）。TensorFlow之所以要分析源码，试分析Python没有提供任何其它的方式来获取控制流语句：Python提供了`__add__()`和`__mul__()`这样的魔术方法，但没有`__while__()`或`__if__()`这样的魔术方法。分析完源码之后，自动图中的所有控制流语句都被替换成相应的TensorFlow方法，比如`tf.while_loop()`（while循环）和`tf.cond()`（if判断）。例如，见图12-4，自动图分析了Python函数`sum_squares()`的源码，然后变为函数`tf__sum_squares()`。在这个函数中，for循环被替换成了`loop_body()`（包括原生的for循环）。然后是函数`for_stmt()`，调用这个函数会形成运算`tf.while_loop()`。

![img](https:////upload-images.jianshu.io/upload_images/7178691-cd0d53808f2cf8a8.png?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp)

图12-4 TensorFlow是如何使用自动图和跟踪生成计算图的？

然后，TensorFlow调用这个“升级”方法，但没有向其传递参数，而是传递一个符号张量（symbolic tensor）——一个没有任何真实值的张量，只有名字、数据类型和形状。例如，如果调用`sum_squares(tf.constant(10))`，然后会调用`tf__sum_squares()`，其符号张量的类型是int32，形状是[]。函数会以图模式运行，意味着每个TensorFlow运算会在图中添加一个表示自身的节点，然后输出`tensor(s)`（与常规模式相对，这被称为动态图执行，或动态模式）。在图模式中，TF运算不做任何计算。如果你懂TensorFlow 1，这应该很熟悉，因为图模式是默认模式。在图12-4中，可以看到`tf__sum_squares()`函数被调用，参数是符号张量，最后的图是跟踪中生成的。节点表示运算，箭头表示张量（生成的函数和图都简化了）。

> 提示：想看生成出来的函数源码的话，可以调用`tf.autograph.to_code(sum_squares.python_function)`。源码不美观，但可以用来调试。



### TF 函数规则

大多数时候，将Python函数转换为TF函数是琐碎的：要用`@tf.function`装饰，或让Keras来负责。但是，也有一些规则：

- 如果调用任何外部库，包括NumPy，甚至是标准库，调用只会在跟踪中运行，不会是图的一部分。事实上，TensorFlow图只能包括TensorFlow的构件（张量、运算、变量、数据集，等等）。因此，要确保使用的是`tf.reduce_sum()`而不是`np.sum()`，使用的是`tf.sort()`而不是内置的`sorted()`，等等。还要注意：

1. 如果定义了一个TF函数`f(x)`，它只返回`np.random.rand()`，当函数被追踪时，生成的是个随机数，因此`f(tf.constant(2.))`和`f(tf.constant(3.))`会返回同样的随机数，但`f(tf.constant([2., 3.]))`会返回不同的数。如果将`np.random.rand()`替换为`tf.random.uniform([])`，每次调用都会返回新的随机数，因为运算是图的一部分。
2. 如果你的非TensorFlow代码有副作用（比如日志，或更新Python计数器），则TF函数被调用时，副作用不一定发生，因为只有函数被追踪时才有效。
3. 你可以在`tf.py_function()`运算中包装任意的Python代码，但这么做的话会使性能下降，因为TensorFlow不能做任何图优化。还会破坏移植性，因为这样图只能在有Python的平台上跑起来（且安装上正确的库）。

- 你可以调用其它Python函数或TF函数，但是它们要遵守相同的规则，因为TensorFlow会在计算图中记录它们的运算。注意，其它函数不需要用`@tf.function`装饰。
- 如果函数创建了一个TensorFlow变量（或任意其它静态TensorFlow对象，比如数据集或队列），它必须在第一次被调用时创建TF函数，否则会导致异常。通常，最好在TF函数的外部创建变量（比如在自定义层的`build()`方法中）。如果你想将一个新值赋值给变量，要确保调用它的`assign()`方法，而不是使用`=`。
- Python的源码可以被TensorFlow使用。如果源码用不了（比如，如果是在Python shell中定义函数，源码就访问不了，或者部署的是编译文件`*.pyc`），图的生成就会失败或者缺失功能。
- TensorFlow只能捕获迭代张量或数据集的for循环。因此要确保使用`for i in tf.range(x)`，而不是`for i in range(x)`，否则循环不能在图中捕获，而是在追踪中运行。（如果for循环使用创建计算图的，这可能是你想要的，比如创建神经网络中的每一层）。
- 出于性能原因，最好使用矢量化的实现方式，而不是使用循环。

总结一下，这一章一开始介绍了TensorFlow，然后是TensorFlow的低级API，包括张量、运算、变量和特殊的数据结构。然后使用这些工具自定义了tf.keras中的几乎每个组件。最后，学习了TF函数如何提升性能，计算图是如何通过自动图和追踪生成的，在写TF函数时要遵守什么规则。（附录G介绍了生成图的内部黑箱）



##  加载和预处理数据 

目前为止，我们只是使用了存放在内存中的数据集，但深度学习系统经常需要在大数据集上训练，而内存放不下大数据集。其它的深度学习库通过对大数据集做预处理，绕过了内存限制，但TensorFlow通过Data API，使一切都容易了：只需要创建一个数据集对象，告诉它去哪里拿数据，以及如何做转换就行。TensorFlow负责所有的实现细节，比如多线程、队列、批次和预提取。另外，Data API和tf.keras可以无缝配合！

- Data API还可以从现成的文件（比如CSV文件）、固定大小的二进制文件、使用TensorFlow的TFRecord格式的文件（支持大小可变的记录）读取数据。TFRecord是一个灵活高效的二进制格式，基于Protocol Buffers（一个开源二进制格式）。
- Data API还支持从SQL数据库读取数据。
- 另外，许多开源插件也可以用来从各种数据源读取数据，包括谷歌的BigQuery。

高效读取大数据集不是唯一的难点：数据还需要进行预处理，通常是归一化。另外，数据集中并不是只有数值字段：可能还有文本特征、类型特征，等等。这些特征需要编码，比如使用独热编码或嵌入（后面会看到，嵌入是用来标识类型或token的紧密矢量）。**预处理的一种方式是写自己的自定义预处理层，另一种是使用Kera的标准预处理层。**

本章中，我们会介绍Data API，TFRecord格式，以及如何创建自定义预处理层，和使用Keras的预处理层。还会快速学习TensorFlow生态的一些项目：

- TF Transform (tf.Transform)：可以用来编写单独的预处理函数，它可以在真正训练前，运行在完整训练集的批模式中，然后输出到TF Function，插入到训练好的模型中。只要模型在生产环境中部署好了，就能随时预处理新的实例。
- TF Datasets (TFDS)。提供了下载许多常见数据集的函数，包括ImageNet，和数据集对象（可用Data API操作）。



### Data API

整个Data API都是围绕数据集`dataset`的概念展开的：可以猜得到，数据集表示一连串数据项。通常你用的数据集是从硬盘里逐次读取数据的，简单起见，我们是用`tf.data.Dataset.from_tensor_slices()`创建一个存储于内存中的数据集：

```python
>>> X = tf.range(10)  # any data tensor
>>> dataset = tf.data.Dataset.from_tensor_slices(X)
>>> dataset
<TensorSliceDataset shapes: (), types: tf.int32>
```

函数`from_tensor_slices()`取出一个张量，创建了一个`tf.data.Dataset`，它的元素是`X`的全部切片，因此这个数据集包括10项：张量 0、1、2、。。。、9。在这个例子中，使用`tf.data.Dataset.range(10)`也能达到同样的效果。

可以像下面这样对这个数据集迭代：

```python
>>> for item in dataset:
...     print(item)
...
tf.Tensor(0, shape=(), dtype=int32)
tf.Tensor(1, shape=(), dtype=int32)
tf.Tensor(2, shape=(), dtype=int32)
[...]
tf.Tensor(9, shape=(), dtype=int32)
```



### 链式转换

有了数据集之后，通过调用转换方法，可以对数据集做各种转换。每个方法会返回一个新的数据集，因此可以将转换像下面这样链接起来（见图13-1）：

```python
>>> dataset = dataset.repeat(3).batch(7)
>>> for item in dataset:
...     print(item)
...
tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)
tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)
tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)
tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)
tf.Tensor([8 9], shape=(2,), dtype=int32)
```

![img](https:////upload-images.jianshu.io/upload_images/7178691-e2753af12c9e73a0.png?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp)

​																图13-1 链接数据集转换

在这个例子中，我们先在原始数据集上调用了`repeat()`方法，返回了一个重复了原始数据集3次的新数据集。当然，这步不会复制数据集中的数据三次（如果调用这个方法时没有加参数，新数据集会一直重复源数据集，必须让迭代代码决定何时退出）。然后我们在新数据集上调用了`batch()`方法，这步又产生了一个新数据集。这一步会将上一个数据集分成7个一批次。最后，做一下迭代。可以看到，最后的批次只有两个元素，可以设置`drop_remainder=True`，丢弃最后的两项，将数据对齐。

> 警告：数据集方法不修改数据集，只是生成新的数据集而已，所以要做新数据集的赋值（即使用`dataset = ...`）。

还可以通过`map()`方法转换元素。比如，下面的代码创建了一个每个元素都翻倍的新数据集：

```python
>>> dataset = dataset.map(lambda x: x * 2) # Items: [0,2,4,6,8,10,12]
```

这个函数可以用来对数据做预处理。有时可能会涉及复杂的计算，比如改变形状或旋转图片，所以通常需要多线程来加速：只需设置参数`num_parallel_calls`就行。注意，传递给`map()`方法的函数必须是可以转换为TF Function。

`map()`方法是对每个元素做转换的，`apply()`方法是对数据整体做转换的。例如，下面的代码对数据集应用了`unbatch()`函数（这个函数目前是试验性的，但很有可能加入到以后的版本中）。新数据集中的每个元素都是一个单整数张量，而不是批次大小为7的整数。

```python
>>> dataset = dataset.apply(tf.data.experimental.unbatch()) # Items: 0,2,4,...
```

还可以用`filter()`方法做过滤：

```python
>>> dataset = dataset.filter(lambda x: x < 10) # Items: 0 2 4 6 8 0 2 4 6...
```

`take()`方法可以用来查看数据：

```python
>>> for item in dataset.take(3):
...     print(item)
...
tf.Tensor(0, shape=(), dtype=int64)
tf.Tensor(2, shape=(), dtype=int64)
tf.Tensor(4, shape=(), dtype=int64)
```



### 打散数据

当训练集中的实例是独立同分布时，梯度下降的效果最好（见第4章）。实现独立同分布的一个简单方法是使用`shuffle()`方法。它能创建一个新数据集，新数据集的前面是一个缓存，缓存中是源数据集的开头元素。然后，无论什么时候取元素，就会从缓存中随便随机取出一个元素，从源数据集中取一个新元素替换。从缓冲器取元素，直到缓存为空。必须要指定缓存的大小，最好大一点，否则随机效果不明显。不要超出内存大小，即使内存够用，缓存超过数据集也是没有意义的。可以提供一个随机种子，如果希望随机的顺序是固定的。例如，下面的代码创建并显示了一个包括0到9的数据集，重复3次，用大小为5的缓存做随机，随机种子是42，批次大小是7：

```python
>>> dataset = tf.data.Dataset.range(10).repeat(3) # 0 to 9, three times
>>> dataset = dataset.shuffle(buffer_size=5, seed=42).batch(7)
>>> for item in dataset:
...     print(item)
...
tf.Tensor([0 2 3 6 7 9 4], shape=(7,), dtype=int64)
tf.Tensor([5 0 1 1 8 6 5], shape=(7,), dtype=int64)
tf.Tensor([4 8 7 1 2 3 0], shape=(7,), dtype=int64)
tf.Tensor([5 4 2 7 8 9 9], shape=(7,), dtype=int64)
tf.Tensor([3 6], shape=(2,), dtype=int64)
```

> 提示：如果在随机数据集上调用`repeat()`方法，默认下，每次迭代的顺序都是新的。通常这样没有问题，但如果你想让每次迭代的顺序一样（比如，测试或调试），可以设置`reshuffle_each_iteration=False`。

对于内存放不下的大数据集，这个简单的随机缓存方法就不成了，因为缓存相比于数据集就小太多了。一个解决方法是将源数据本身打乱（例如，Linux可以用`shuf`命令打散文本文件）。这样肯定能提高打散的效果！即使源数据打散了，你可能还想再打散一点，否则每个周期可能还会出现同样的顺序，模型最后可能是偏的（比如，源数据顺序偶然导致的假模式）。为了将实例进一步打散，一个常用的方法是将源数据分成多个文件，训练时随机顺序读取。但是，相同文件中的实例仍然靠的太近。为了避免这点，可以同时随机读取多个文件，做交叉。在最顶层，可以用`shuffle()`加一个随机缓存。如果这听起来很麻烦，不用担心：Data API都为你实现了，几行代码就行。



### 多行数据交叉

首先，假设加载了加州房价数据集，打散它（除非已经打散了），分成训练集、验证集、测试集。然后将每个数据集分成多个csv文件，每个如下所示（每行包含8个输入特征加上目标中位房价）：

```shell
MedInc,HouseAge,AveRooms,AveBedrms,Popul,AveOccup,Lat,Long,MedianHouseValue
3.5214,15.0,3.0499,1.1065,1447.0,1.6059,37.63,-122.43,1.442
5.3275,5.0,6.4900,0.9910,3464.0,3.4433,33.69,-117.39,1.687
3.1,29.0,7.5423,1.5915,1328.0,2.2508,38.44,-122.98,1.621
[...]
```

再假设`train_filepaths`包括了训练文件路径的列表（还要`valid_filepaths`和`test_filepaths`）：

```python
>>> train_filepaths
['datasets/housing/my_train_00.csv', 'datasets/housing/my_train_01.csv',...]
```

另外，可以使用文件模板，比如`train_filepaths = "datasets/housing/my_train_*.csv"`。现在，创建一个数据集，包括这些文件路径：

```kotlin
filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42)
```

默认，`list_files()`函数返回一个文件路径打散的数据集。也可以设置`shuffle=False`，文件路径就不打散了。

然后，可以调用`leave()`方法，一次读取5个文件，做交叉操作（跳过第一行表头，使用`skip()`方法）：

```kotlin
n_readers = 5
dataset = filepath_dataset.interleave(
    lambda filepath: tf.data.TextLineDataset(filepath).skip(1),
    cycle_length=n_readers)
```

`interleave()`方法会创建一个数据集，它从`filepath_dataset`读5条文件路径，对每条路径调用函数（例子中是用的匿名函数）来创建数据集（例子中是`TextLineDataset`）。为了更清楚点，这一步总欧诺个由七个数据集：文件路径数据集，交叉数据集，和五个`TextLineDatasets`数据集。当迭代交叉数据集时，会循环`TextLineDatasets`，每次读取一行，知道数据集为空。然后会从`filepath_dataset`再获取五个文件路径，做同样的交叉，直到文件路径为空。

> 提示：为了交叉得更好，最好让文件有相同的长度，否则长文件的尾部不会交叉。

默认情况下，`interleave()`不是并行的，只是顺序从每个文件读取一行。如果想变成并行读取文件，可以设定参数`num_parallel_calls`为想要的线程数（`map()`方法也有这个参数）。还可以将其设置为`tf.data.experimental.AUTOTUNE`，让TensorFlow根据CPU自己找到合适的线程数（目前这是个试验性的功能）。看看目前数据集包含什么：

```py
>>> for line in dataset.take(5):
...     print(line.numpy())
...
b'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782'
b'4.1812,52.0,5.7013,0.9965,692.0,2.4027,33.73,-118.31,3.215'
b'3.6875,44.0,4.5244,0.9930,457.0,3.1958,34.04,-118.15,1.625'
b'3.3456,37.0,4.5140,0.9084,458.0,3.2253,36.67,-121.7,2.526'
b'3.5214,15.0,3.0499,1.1065,1447.0,1.6059,37.63,-122.43,1.442'
```

忽略表头行，这是五个csv文件的第一行，随机选取的。看起来不错。但是也看到了，都是字节串，需要解析数据，缩放数据。



### 预处理数据

实现一个小函数来做预处理：

```ruby
X_mean, X_std = [...] # mean and scale of each feature in the training set
n_inputs = 8

def preprocess(line):
  defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]
  fields = tf.io.decode_csv(line, record_defaults=defs)
  x = tf.stack(fields[:-1])
  y = tf.stack(fields[-1:])
  return (x - X_mean) / X_std, y
```

逐行看下代码：

- 首先，代码假定已经算好了训练集中每个特征的平均值和标准差。`X_mean`和`X_std`是1D张量（或NumPy数组），包含八个浮点数，每个都是特征。
- `preprocess()`函数从csv取一行，开始解析。使用`tf.io.decode_csv()`函数，接收两个参数，第一个是要解析的行，第二个是一个数组，包含csv文件每列的默认值。这个数组不仅告诉TensorFlow每列的默认值，还有总列数和数据类型。在这个例子中，是告诉TensorFlow，所有特征列都是浮点数，缺失值默认为0，但提供了一个类型是`tf.float32`的空数组，**作为最后一列（标签）的默认值：数组告诉TensorFlow这一列包含浮点数，但没有默认值，所以碰到空值时会报异常。**
- `decode_csv()`函数返回一个标量张量（每列一个）的列表，但应该返回1D张量数组。所以在所有张量上调用了`tf.stack()`，除了最后一个。然后对目标值做同样的操作（让其成为只包含一个值，而不是标量张量的1D张量数组）。
- 最后，对特征做缩放，减去平均值，除以标准差，然后返回包含缩放特征和目标值的元组。

测试这个预处理函数：

```python
>>> preprocess(b'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782')
(<tf.Tensor: id=6227, shape=(8,), dtype=float32, numpy=
 array([ 0.16579159,  1.216324  , -0.05204564, -0.39215982, -0.5277444 ,
        -0.2633488 ,  0.8543046 , -1.3072058 ], dtype=float32)>,
 <tf.Tensor: [...], numpy=array([2.782], dtype=float32)>)
```

很好，接下来将函数应用到数据集上。



### 整合

为了让代码可复用，将前面所有讨论过的东西变成一个小函数：创建并返回一个数据集，可以高效从多个csv文件加载加州房价数据集，做预处理、打散、选择性重复，做批次（见图3-2）：

```python
def csv_reader_dataset(filepaths, repeat=1, n_readers=5,
                       n_read_threads=None, shuffle_buffer_size=10000,
                       n_parse_threads=5, batch_size=32):
    dataset = tf.data.Dataset.list_files(filepaths)
    dataset = dataset.interleave(
        lambda filepath: tf.data.TextLineDataset(filepath).skip(1),
        cycle_length=n_readers, num_parallel_calls=n_read_threads)
    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)
    dataset = dataset.shuffle(shuffle_buffer_size).repeat(repeat)
    return dataset.batch(batch_size).prefetch(1)
```

代码条理很清晰，除了最后一行的`prefetch(1)`，对于提升性能很关键。



### 预提取

通过调用`prefetch(1)`，创建了一个高效的数据集，总能提前一个批次。换句话说，当训练算法在一个批次上工作时，数据集已经准备好下一个批次了（从硬盘读取数据并做预处理）。这样可以极大提升性能，解释见图13-3。如果加载和预处理还要是多线程的（通过设置`interleave()`和`map()`的`num_parallel_calls`），可以利用多CPU，准备批次数据可以比在GPU上训练还快：这样GPU就可以100%利用起来了（排除数据从CPU传输到GPU的时间），训练可以快很多。

![img](https:////upload-images.jianshu.io/upload_images/7178691-86355457e5a7a1c2.png?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp)

图13-3 通过预提取，让CPU和GPU并行工作：GPU在一个批次上工作时，CPU准备下一个批次

> 提示：如果想买一块GPU显卡的话，它的处理能力和显存都是非常重要的。另一个同样重要的，是显存带宽，即每秒可以进入或流出内存的GB数。

如果数据集不大，内存放得下，可以使用数据集的`cache()`方法将数据集存入内存。通常这步是在加载和预处理数据之后，在打散、重复、分批次之前。这样做的话，每个实例只需做一次读取和处理，下一个批次仍能提前准备。

你现在知道如何搭建高效输入管道，从多个文件加载和预处理数据了。我们讨论了最常用的数据集方法，但还有一些你可能感兴趣：`concatenate()`、`zip()`、`window()`、`reduce()`、`shard()`、`flat_map()`、和`padded_batch()`。还有两个类方法：`from_generator()`和`from_tensors()`，它们能从Python生成器或张量列表创建数据集。更多细节请查看API文档。`tf.data.experimental`中还有试验性功能，其中许多功能可能会添加到未来版本中。



### tf.keras使用数据集**

现在可以使用`csv_reader_dataset()`函数为训练集创建数据集了。注意，不需要将数据重复，tf.keras会做重复。还为验证集和测试集创建了数据集：

```python
train_set = csv_reader_dataset(train_filepaths)
valid_set = csv_reader_dataset(valid_filepaths)
test_set = csv_reader_dataset(test_filepaths)
```

现在就可以利用这些数据集来搭建和训练Keras模型了。我们要做的就是将训练和验证集传递给`fit()`方法，而不是`X_train`、`y_train`、`X_valid`、`y_valid`：

```python
model = keras.models.Sequential([...])
model.compile([...])
model.fit(train_set, epochs=10, validation_data=valid_set)
```

相似的，可以将数据集传递给`evaluate()`和`predict()`方法：

```python
model.evaluate(test_set)
new_set = test_set.take(3).map(lambda X, y: X) # pretend we have 3 new instances
model.predict(new_set) # a dataset containing new instances
```

跟其它集合不同，`new_set`通常不包含标签（如果包含标签，也会被Keras忽略）。注意，在所有这些情况下，还可以使用NumPy数组（但仍需要加载和预处理）。

如果你想创建自定义训练循环（就像12章那样），你可以在训练集上迭代：

```csharp
for X_batch, y_batch in train_set:
    [...] # perform one Gradient Descent step
```

事实上，还可以创建一个TF函数（见第12章）来完成整个训练循环：

```python
@tf.function
def train(model, optimizer, loss_fn, n_epochs, [...]):
    train_set = csv_reader_dataset(train_filepaths, repeat=n_epochs, [...])
for X_batch, y_batch in train_set:
        with tf.GradientTape() as tape:
            y_pred = model(X_batch)
            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))
            loss = tf.add_n([main_loss] + model.losses)
        grads = tape.gradient(loss, model.trainable_variables)
        optimizer.apply_gradients(zip(grads, model.trainable_variables))
```

祝贺，你现在知道如何使用Data API创建强大的输入管道了！但是，目前为止我们使用的CSV文件，虽然常见又简单方便，但不够高效，不支持大或复杂的数据结构（比如图片或音频）。这就是TFRecord要解决的。

> 提示：如果你对csv文件感到满意（或其它任意格式），就不必使用TFRecord。就像老话说的，只要没坏就别修！TFRecord是为解决训练过程中加载和解析数据时碰到的瓶颈。



## TFRecord格式

TFRecord格式是TensorFlow偏爱的存储大量数据并高效读取的数据。它是非常简单的二进制格式，只包含不同大小的二进制记录的数据（每个记录包括一个长度、一个CRC校验和，校验和用于检查长度是否正确，真实的数据，和一个数据的CRC校验和，用于检查数据是否正确）。可以使用`tf.io.TFRecordWriter`类轻松创建TFRecord文件：

```css
with tf.io.TFRecordWriter("my_data.tfrecord") as f:
    f.write(b"This is the first record")
    f.write(b"And this is the second record")
```

然后可以使用`tf.data.TFRecordDataset`来读取一个或多个TFRecord文件：

```csharp
filepaths = ["my_data.tfrecord"]
dataset = tf.data.TFRecordDataset(filepaths)
for item in dataset:
    print(item)
```

输出是：

```csharp
tf.Tensor(b'This is the first record', shape=(), dtype=string)
tf.Tensor(b'And this is the second record', shape=(), dtype=string)
```

> 提示：默认情况下，`TFRecordDataset`会逐一读取数据，但通过设定`num_parallel_reads`可以并行读取并交叉数据。另外，你可以使用`list_files()`和`interleave()`获得同样的结果。



### 压缩TFRecord文件

有的时候压缩TFRecord文件很有必要，特别是当需要网络传输的时候。你可以通过设定`options`参数，创建压缩的TFRecord文件：

```csharp
options = tf.io.TFRecordOptions(compression_type="GZIP")
with tf.io.TFRecordWriter("my_compressed.tfrecord", options) as f:
  [...]
```

当读取压缩TFRecord文件时，需要指定压缩类型：

```kotlin
dataset = tf.data.TFRecordDataset(["my_compressed.tfrecord"],
                                  compression_type="GZIP")
```



### 简要介绍协议缓存

即便每条记录可以使用任何二进制格式，TFRecord文件通常包括序列化的协议缓存（也称为protobuf）。这是一种可移植、可扩展的高效二进制格式，是谷歌在2001年开发，并在2008年开源的；协议缓存现在使用广泛，特别是在gRPC，谷歌的远程调用系统中。定义语言如下：

```go
syntax = "proto3";
message Person {
  string name = 1;
  int32 id = 2;
  repeated string email = 3;
}
```

定义写道，使用的是协议缓存的版本3，指定每个`Person`对象可以有一个`name`，类型是字符串，类型是int32的`id`，0个或多个`email`字段，每个都是字符串。数字1、2、3是字段标识符：用于每条数据的二进制表示。当你在`.proto`文件中有了一个定义，就可以编译了。这就需要`protoc`，协议缓存编译器，来生成Python（或其它语言）的访问类。注意，要使用的缓存协议的定义已经编译好了，它们的Python类是TensorFlow的一部分，所以就不必使用`protoc`了。你需要知道的知识如何使用Python的缓存协议访问类。为了讲解，看一个简单的例子，使用访问类来生成`Person`缓存协议：

```python
>>> from person_pb2 import Person  # 引入生成的访问类
>>> person = Person(name="Al", id=123, email=["a@b.com"])  # 创建一个Person
>>> print(person)  # 展示Person
name: "Al"
id: 123
email: "a@b.com"
>>> person.name  # 读取一个字段
"Al"
>>> person.name = "Alice"  # 修改一个字段
>>> person.email - 这个网站可出售。 - 最佳的person 来源和相关信息。[0]  # 重复的字段可以像数组一样访问
"a@b.com"
>>> person.email.append("c@d.com")  # 添加email地址
>>> s = person.SerializeToString()  # 将对象序列化为字节串
>>> s
b'\n\x05Alice\x10{\x1a\x07a@b.com\x1a\x07c@d.com'
>>> person2 = Person()  # 创建一个新Person
>>> person2.ParseFromString(s)  #解析字节串（字节长度27）
27
>>> person == person2  # 现在相等
True
```

简而言之，我们引入了`protoc`生成的类`Person`，创建了一个实例，展示、读取、并写入新字段，然后使用`SerializeToString()`将其序列化。序列化的数据就可以保存或通过网络传输了。当读取或接收二进制数据时，可以使用`ParseFromString()`方法来解析，就得到了序列化对象的复制。

可以将序列化的`Person`对象存储为TFRecord文件，然后可以加载和解析。但是`SerializeToString()`和`ParseFromString()`不是TensorFlow运算（这段代码中的其它代码也不是TensorFlow运算），因此TensorFlow函数中不能含有这两个方法（除非将其包装进`tf.py_function()`运算，但会使代码速度变慢，移植性变差）。幸好，TensorFlow还有提供了解析运算的特殊协议缓存。



### TensorFlow协议缓存

TFRecord文件主要使用的协议缓存是`Example`，它表示数据集中的一个实例，包括命名特征的列表，每个特征可以是字节串列表、或浮点列表、或整数列表。下面是一个协议缓存的定义：

```csharp
syntax = "proto3";
message BytesList { repeated bytes value = 1; }
message FloatList { repeated float value = 1 [packed = true]; }
message Int64List { repeated int64 value = 1 [packed = true]; }
message Feature {
    oneof kind {
        BytesList bytes_list = 1;
        FloatList float_list = 2;
        Int64List int64_list = 3;
    }
};
message Features { map<string, Feature> feature = 1; };
message Example { Features features = 1; };
```

`BytesList`、`FloatList`、`Int64List`的定义都很清楚。注意，重复的数值字段使用了`[packed = true]`，目的是高效编码。`Feature`包含的是`BytesList`、`FloatList`、`Int64List`三者之一。`Features`（带s）是包含特征名和对应特征值的字典。最后，一个`Example`值包含一个`Features`对象。下面是一个如何创建`tf.train.Example`的例子，表示的是之前同样的人，并存储为TFRecord文件：

```csharp
from tensorflow.train import BytesList, FloatList, Int64List
from tensorflow.train import Feature, Features, Example

person_example = Example(
    features=Features(
        feature={
            "name": Feature(bytes_list=BytesList(value=[b"Alice"])),
            "id": Feature(int64_list=Int64List(value=[123])),
            "emails": Feature(bytes_list=BytesList(value=[b"a@b.com",
b"c@d.com"]))
        }))
```

这段代码有点冗长和重复，但很清晰（可以很容易将其包装起来）。现在有了`Example`协议缓存，可以调用`SerializeToString()`方法将其序列化，然后将结果数据存入TFRecord文件：

```css
with tf.io.TFRecordWriter("my_contacts.tfrecord") as f:
    f.write(person_example.SerializeToString())
```

通常需要写不止一个`Example`！一般来说，你需要写一个转换脚本，读取当前格式（例如csv），为每个实例创建`Example`协议缓存，序列化并存储到若干TFRecord文件中，最好再打散。这些需要花费不少时间，如有必要再这么做（也许CSV文件就足够了）。

有了序列化好的`Example`TFRecord文件之后，就可以加载了。



### 加载和解析Example

要加载序列化的`Example`协议缓存，需要再次使用`tf.data.TFRecordDataset`，使用`tf.io.parse_single_example()`解析每个`Example`。这是一个TensorFlow运算，所以可以包装进TF函数。它至少需要两个参数：一个包含序列化数据的字符串标量张量，和每个特征的描述。描述是一个字典，将每个特征名映射到`tf.io.FixedLenFeature`描述符，描述符指明特征的形状、类型和默认值，或（当特征列表长度可能变化时，比如`"email"特征`）映射到`tf.io.VarLenFeature`描述符，它只指向类型。

下面的代码定义了描述字典，然后迭代`TFRecordDataset`，解析序列化的`Example`协议缓存：

```go
feature_description = {
    "name": tf.io.FixedLenFeature([], tf.string, default_value=""),
    "id": tf.io.FixedLenFeature([], tf.int64, default_value=0),
    "emails": tf.io.VarLenFeature(tf.string),
}

for serialized_example in tf.data.TFRecordDataset(["my_contacts.tfrecord"]):
    parsed_example = tf.io.parse_single_example(serialized_example,
                                                feature_description)
```

长度固定的特征会像常规张量那样解析，而长度可变的特征会作为稀疏张量解析。可以使用`tf.sparse.to_dense()`将稀疏张量转变为紧密张量，但只是简化了值的访问：

```py
>>> tf.sparse.to_dense(parsed_example["emails"], default_value=b"")
<tf.Tensor: [...] dtype=string, numpy=array([b'a@b.com', b'c@d.com'], [...])>
>>> parsed_example["emails"].values
<tf.Tensor: [...] dtype=string, numpy=array([b'a@b.com', b'c@d.com'], [...])>
```

`BytesList`可以包含任意二进制数据，序列化对象也成。例如，可以使用`tf.io.encode_jpeg()`将图片编码为JPEG格式，然后将二进制数据放入`BytesList`。然后，当代码读取`TFRecord`时，会从解析`Example`开始，再调用`tf.io.decode_jpeg()`解析数据，得到原始图片（或者可以使用`tf.io.decode_image()`，它能解析任意`BMP`、`GIF`、`JPEG`、`PNG`格式）。你还可以通过`tf.io.serialize_tensor()`序列化张量，将结果字节串放入`BytesList`特征，将任意张量存储在`BytesList`中。之后，当解析`TFRecord`时，可以使用`tf.io.parse_tensor()`解析数据。

除了使用`tf.io.parse_single_example()`逐一解析`Example`，你还可以通过`tf.io.parse_example()`逐批次解析：

```kotlin
dataset = tf.data.TFRecordDataset(["my_contacts.tfrecord"]).batch(10)
for serialized_examples in dataset:
    parsed_examples = tf.io.parse_example(serialized_examples,
                                          feature_description)
```

可以看到`Example`协议缓存对大多数情况就足够了。但是，如果处理的是嵌套列表，就会比较麻烦。比如，假设你想分类文本文档。每个文档可能都是句子的列表，而每个句子又是词的列表。每个文档可能还有评论列表，评论又是词的列表。可能还有上下文数据，比如文档的作者、标题和出版日期。TensorFlow的`SequenceExample`协议缓存就是为了处理这种情况的。



### 使用`SequenceExample`协议缓存处理嵌套列表

下面是`SequenceExample`协议缓存的定义：

```cpp
message FeatureList { repeated Feature feature = 1; };
message FeatureLists { map<string, FeatureList> feature_list = 1; };
message SequenceExample {
    Features context = 1;
    FeatureLists feature_lists = 2;
};
```

`SequenceExample`包括一个上下文数据的`Features`对象，和一个包括一个或多个命名`FeatureList`对象（比如，一个`FeatureList`命名为`"content"`，另一个命名为`"comments"`）的`FeatureLists`对象。每个`FeatureList`包含`Feature`对象的列表，每个`Feature`对象可能是字节串、64位整数或浮点数的列表（这个例子中，每个`Feature`表示的是一个句子或一条评论，格式或许是词的列表）。创建`SequenceExample`，将其序列化、解析，和创建、序列化、解析`Example`很像，但必须要使用`tf.io.parse_single_sequence_example()`来解析单个的`SequenceExample`或用`tf.io.parse_sequence_example()`解析一个批次。两个函数都是返回一个包含上下文特征（字典）和特征列表（也是字典）的元组。如果特征列表包含大小可变的序列（就像前面的例子），可以将其转化为嵌套张量，使用`tf.RaggedTensor.from_sparse()`：

```bash
parsed_context, parsed_feature_lists = tf.io.parse_single_sequence_example(
    serialized_sequence_example, context_feature_descriptions,
    sequence_feature_descriptions)
parsed_content = tf.RaggedTensor.from_sparse(parsed_feature_lists["content"])
```

现在你就知道如何高效存储、加载和解析数据了，下一步是准备数据。



## 预处理输入特征**

为神经网络准备数据需要将所有特征转变为数值特征，做一些归一化工作等等。特别的，如果数据包括类型特征或文本特征，也需要转变为数字。这些工作可以在准备数据文件的时候做，使用NumPy、Pandas、Scikit-Learn这样的工作。或者，可以在用Data API加载数据时，实时预处理数据（比如，使用数据集的`map()`方法，就像前面的例子），或者可以给模型加一个预处理层。接下来，来看最后一种方法。

例如，这个例子是使用`Lambda`层实现标准化层。对于每个特征，减去其平均值，再除以标准差（再加上一个平滑项，避免0除）：

```python
means = np.mean(X_train, axis=0, keepdims=True)
stds = np.std(X_train, axis=0, keepdims=True)
eps = keras.backend.epsilon()
model = keras.models.Sequential([
    keras.layers.Lambda(lambda inputs: (inputs - means) / (stds + eps)),
    [...] # 其它层
])
```

并不难。但是，你也许更想要一个独立的自定义层（就像Scikit-Learn的`StandardScaler`），而不是像`means`和`stds`这样的全局变量：

```ruby
class Standardization(keras.layers.Layer):
    def adapt(self, data_sample):
        self.means_ = np.mean(data_sample, axis=0, keepdims=True)
        self.stds_ = np.std(data_sample, axis=0, keepdims=True)
    def call(self, inputs):
        return (inputs - self.means_) / (self.stds_ + keras.backend.epsilon())
```

使用这个标准化层之前，你需要使用`adapt()`方法将其适配到数据集样本。这么做就能使用每个特征的平均值和标准差：

```undefined
std_layer = Standardization()
std_layer.adapt(data_sample)
```

这个样本必须足够大，可以代表数据集，但不必是完整的训练集：通常几百个随机实例就够了（但还是要取决于任务）。然后，就可以像普通层一样使用这个预处理层了：

```bash
model = keras.Sequential()
model.add(std_layer)
[...] # create the rest of the model
model.compile([...])
model.fit([...])
```

可能以后还会有`keras.layers.Normalization`层，和这个自定义`Standardization`层差不多：先创建层，然后对数据集做适配（向`adapt()`方法传递样本），最后像普通层一样使用。

接下来看看类型特征。先将其编码为独热矢量。



### 使用独热矢量编码类型特征

考虑下第2章中的加州房价数据集的`ocean_proximity`特征：这是一个类型特征，有五个值：`"<1H OCEAN"`、`"INLAND"`、`"NEAR OCEAN"`、`"NEAR BAY"`、`"ISLAND"`。输入给神经网络之前，需要对其进行编码。因为类型不多，可以使用独热编码。先将每个类型映射为索引（0到4），使用一张查询表：

```go
vocab = ["<1H OCEAN", "INLAND", "NEAR OCEAN", "NEAR BAY", "ISLAND"]
indices = tf.range(len(vocab), dtype=tf.int64)
table_init = tf.lookup.KeyValueTensorInitializer(vocab, indices)
num_oov_buckets = 2
table = tf.lookup.StaticVocabularyTable(table_init, num_oov_buckets)
```

逐行看下代码：

- 先定义词典：也就是所有类型的列表。
- 然后创建张量，具有索引0到4。
- 接着，创建查找表的初始化器，传入类型列表和对应索引。在这个例子中，因为已经有了数据，所以直接用`KeyValueTensorInitializer`就成了；但如果类型是在文本中（一行一个类型），就要使用`TextFileInitializer`。
- 最后两行创建了查找表，传入初始化器并指明未登录词（oov）桶的数量。如果查找的类型不在词典中，查找表会计算这个类型的哈希，使用哈希分配一个未知的类型给未登录词桶。索引序号接着现有序号，所以这个例子中的两个未登录词的索引是5和6。

为什么使用桶呢？如果类型数足够大（例如，邮编、城市、词、产品、或用户），数据集也足够大，或者数据集持续变化，这样的话，获取类型的完整列表就不容易了。一个解决方法是根据数据样本定义（而不是整个训练集），为其它不在样本中的类型加上一些未登录词桶。训练中碰到的未知类型越多，要使用的未登录词桶就要越多。事实上，如果未登录词桶的数量不够，就会发生碰撞：不同的类型会出现在同一个桶中，所以神经网络就无法区分了。

现在用查找表将小批次的类型特征编码为独热矢量：

```py
>>> categories = tf.constant(["NEAR BAY", "DESERT", "INLAND", "INLAND"])
>>> cat_indices = table.lookup(categories)
>>> cat_indices
<tf.Tensor: id=514, shape=(4,), dtype=int64, numpy=array([3, 5, 1, 1])>
>>> cat_one_hot = tf.one_hot(cat_indices, depth=len(vocab) + num_oov_buckets)
>>> cat_one_hot
<tf.Tensor: id=524, shape=(4, 7), dtype=float32, numpy=
array([[0., 0., 0., 1., 0., 0., 0.],
       [0., 0., 0., 0., 0., 1., 0.],
       [0., 1., 0., 0., 0., 0., 0.],
       [0., 1., 0., 0., 0., 0., 0.]], dtype=float32)>
```

可以看到，`"NEAR BAY"`映射到了索引3，未知类型`"DESERT"`映射到了两个未登录词桶之一（索引5），`"INLAND"`映射到了索引1两次。然后使用`tf.one_hot()`来做独热编码。注意，需要告诉该函数索引的总数量，索引总数等于词典大小加上未登录词桶的数量。现在你就知道如何用TensorFlow将类型特征编码为独热矢量了。

和之前一样，将这些操作写成一个独立的类并不难。`adapt()`方法接收一个数据样本，提取其中的所有类型。创建一张查找表，将类型和索引映射起来。`call()`方法会使用查找表将输入类型和索引建立映射。目前，Keras已经有了一个名为`keras.layers.TextVectorization`的层，它的功能就是上面这样：`adapt()`从样本中提取词表，`call()`将每个类型映射到词表的索引。如果要将索引变为独热矢量的话，可以将这个层添加到模型开始的地方，后面根生一个可以用`tf.one_hot()`的`Lambda`层。

这可能不是最佳解决方法。每个独热矢量的大小是词表长度加上未登录词桶的大小。当类型不多时，这么做可以，但如果词表很大，最好使用“嵌入“来做。

> 提示：一个重要的原则，如果类型数小于10，可以使用独热编码。如果类型超过50个（使用哈希桶时通常如此），最好使用嵌入。类型数在10和50之间时，最好对两种方法做个试验，看哪个更合适。



### 使用嵌入编码类型特征

"嵌入"是一个可训练的表示类型的紧密矢量。默认时，嵌入是随机初始化的，`"NEAR BAY"`可能初始化为`[0.131, 0.890]`，`"NEAR OCEAN"`可能初始化为`[0.631, 0.791]`。

这个例子中，使用的是2D嵌入，维度是一个可调节的超参数。因为嵌入是可以训练的，它能在训练中提高性能；当嵌入表示相似的类时，梯度下降会使相似的嵌入靠的更近，而`"INLAND"`会偏的更远（见图13-4）。事实上，表征的越好，越利于神经网络做出准确的预测，而训练会让嵌入更好的表征类型，这被称为**表征学习**（第17章会介绍其它类型的表征学习）。

![img](https:////upload-images.jianshu.io/upload_images/7178691-f98cfc35ed08577f.png?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp)

图13-4 嵌入的表征会在训练中提高

> 词嵌入
>
> 嵌入不仅可以实现当前任务的表征，同样的嵌入也可以用于其它的任务。最常见的例子是词嵌入（即，单个词的嵌入）：对于自然语言处理任务，最好使用预训练的词嵌入，而不是使用自己训练的。
>
> 使用矢量表征词可以追溯到1960年代，许多复杂的技术用于生成向量，包括使用神经网络。进步发生在2013年，Tomáš Mikolov和谷歌其它的研究院发表了一篇论文《Distributed Representations of Words and Phrases and their Compositionality》（[Distributed Representations of Words and Phrases and their Compositionality](简书)），介绍了一种用神经网络学习词嵌入的技术，效果远超以前的技术。可以实现在大文本语料上学习嵌入：用神经网络预测给定词附近的词，得到了非常好的词嵌入。例如，同义词有非常相近的词嵌入，语义相近的词，比如法国、西班牙和意大利靠的也很近。
>
> 不止是相近：词嵌入在嵌入空间的轴上的分布也是有意义的。下面是一个著名的例子：如果计算 King – Man + Woman，结果与Queen非常相近（见图13-5）。换句话，词嵌入编码了性别。相似的，可以计算 Madrid – Spain + France，结果和Paris很近。
>
> ![img](https:////upload-images.jianshu.io/upload_images/7178691-df9ac2d9cce439cf.png?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp)
>
> 图13-5 相似词的词嵌入也相近，一些轴编码了概念
>
> 但是，词嵌入有时偏差很大。例如，尽管词嵌入学习到了男人是国王，女人是王后，词嵌入还学到了男人是医生、女人是护士。这是非常大的性别偏差。

来看下如何手动实现嵌入。首先，需要创建一个包含每个类型嵌入（随机初始化）的嵌入矩阵。每个类型就有一行，每个未登录词桶就有一行，每个嵌入维度就有一列：

```go
embedding_dim = 2
embed_init = tf.random.uniform([len(vocab) + num_oov_buckets, embedding_dim])
embedding_matrix = tf.Variable(embed_init)
```

这个例子用的是2D嵌入，通常的嵌入是10到300维，取决于任务和词表大小（需要调节词表大小超参数）。

嵌入矩阵是一个随机的6 × 2矩阵，存入一个变量（因此可以在训练中被梯度下降调节）：

```py
>>> embedding_matrix
<tf.Variable 'Variable:0' shape=(6, 2) dtype=float32, numpy=
array([[0.6645621 , 0.44100678],
       [0.3528825 , 0.46448255],
       [0.03366041, 0.68467236],
       [0.74011743, 0.8724445 ],
       [0.22632635, 0.22319686],
       [0.3103881 , 0.7223358 ]], dtype=float32)>
```

使用嵌入编码之前的类型特征：

```py
>>> categories = tf.constant(["NEAR BAY", "DESERT", "INLAND", "INLAND"])
>>> cat_indices = table.lookup(categories)
>>> cat_indices
<tf.Tensor: id=741, shape=(4,), dtype=int64, numpy=array([3, 5, 1, 1])>
>>> tf.nn.embedding_lookup(embedding_matrix, cat_indices)
<tf.Tensor: id=864, shape=(4, 2), dtype=float32, numpy=
array([[0.74011743, 0.8724445 ],
       [0.3103881 , 0.7223358 ],
       [0.3528825 , 0.46448255],
       [0.3528825 , 0.46448255]], dtype=float32)>
```

`tf.nn.embedding_lookup()`函数根据给定的索引在嵌入矩阵中查找行。例如，查找表说`"INLAND"`类型位于索引1，`tf.nn.embedding_lookup()`就返回嵌入矩阵的行1：`[0.3528825, 0.46448255]`。

Keras提供了`keras.layers.Embedding`层来处理嵌入矩阵（默认可训练）；当这个层初始化时，会随机初始化嵌入矩阵，当被调用时，就返回索引所在的嵌入矩阵的那行：

```py
>>> embedding = keras.layers.Embedding(input_dim=len(vocab) + num_oov_buckets,
...                                    output_dim=embedding_dim)
...
>>> embedding(cat_indices)
<tf.Tensor: id=814, shape=(4, 2), dtype=float32, numpy=
array([[ 0.02401174,  0.03724445],
       [-0.01896119,  0.02223358],
       [-0.01471175, -0.00355174],
       [-0.01471175, -0.00355174]], dtype=float32)>
```

将这些内容放到一起，创建一个Keras模型，可以处理类型特征（和数值特征），学习每个类型（和未登录词）的嵌入：

```py
regular_inputs = keras.layers.Input(shape=[8])
categories = keras.layers.Input(shape=[], dtype=tf.string)
cat_indices = keras.layers.Lambda(lambda cats: table.lookup(cats))(categories)
cat_embed = keras.layers.Embedding(input_dim=6, output_dim=2)(cat_indices)
encoded_inputs = keras.layers.concatenate([regular_inputs, cat_embed])
outputs = keras.layers.Dense(1)(encoded_inputs)
model = keras.models.Model(inputs=[regular_inputs, categories],
                           outputs=[outputs])
```

这个模型有两个输入：一个常规输入，每个实例包括8个数值特征，机上一个类型特征。使用`Lambda`层查找每个类型的索引，然后用索引查找嵌入。接着，将嵌入和常规输入连起来，作为编码输入进神经网络。此时可以加入任意种类的神经网络，但只是添加了一个紧密输出层。

当`keras.layers.TextVectorization`准备好之后，可以调用它的`adapt()`方法，从数据样本提取词表（会自动创建查找表）。然后加入到模型中，就可以执行索引查找了（替换前面代码的`Lambda`层）。

> 笔记：独热编码加紧密层（没有激活函数和偏差项），等价于嵌入层。但是，嵌入层用的计算更少（嵌入矩阵越大，性能差距越明显）。紧密层的权重矩阵扮演的是嵌入矩阵的角色。例如，大小为20的独热矢量和10个单元的紧密层加起来，等价于`input_dim=20`、`output_dim=10`的嵌入层。作为结果，嵌入的维度超过后面的层的神经元数是浪费的。

再进一步看看Keras的预处理层。



### Keras预处理层**

Keras团队打算提供一套标准的Keras预处理层，现在已经可用了，[链接]( https://github.com/keras-team/governance/blob/master/rfcs/20190502-preprocessing-layers.md)。新的API可能会覆盖旧的Feature Columns API。

我们已经讨论了其中的两个：`keras.layers.Normalization`用来做特征标准化，`TextVectorization`层用于将文本中的词编码为词典的索引。对于这两个层，都是用数据样本调用它的`adapt()`方法，然后如常使用。其它的预处理层也是这么使用的。

API中还提供了`keras.layers.Discretization`层，它能将连续数据切成不同的组，将每个组编码为独热矢量。例如，可以用它将价格分成是三类，低、中、高，编码为[1, 0, 0]、[0, 1, 0]、[0, 0, 1]。当然，这么做会损失很多信息，但有时，相对于连续数据，这么做可以发现不那么明显的规律。

> 警告：`Discretization`层是不可微的，只能在模型一开始使用。事实上，模型的预处理层会在训练时冻结，因此预处理层的参数不会被梯度下降影响，所以可以是不可微的。这还意味着，如果想让预处理层可训练的话，不能在自定义预处理层上直接使用嵌入层，而是应该像前面的例子那样分开来做。

还可以用类`PreprocessingStage`将多个预处理层链接起来。例如，下面的代码创建了一个预处理管道，先将输入归一化，然后离散（有点类似Scikit-Learn的管道）。当将这个管道应用到数据样本时，可以作为常规层使用（还得是在模型的前部，因为包含不可微分的预处理层）：

```py
normalization = keras.layers.Normalization()
discretization = keras.layers.Discretization([...])
pipeline = keras.layers.PreprocessingStage([normalization, discretization])
pipeline.adapt(data_sample)
```

`TextVectorization`层也有一个选项用于输出词频向量，而不是词索引。例如，如果词典包括三个词，比如`["and", "basketball", "more"]`，则`"more and more"`会映射为`[1, 0, 2]`：`"and"`出现了一次，`"basketball"`没有出现，`"more"`出现了两次。这种词表征称为词袋，因为它完全失去了词的顺序。常见词，比如`"and"`，会在文本中有更高的值，尽管没什么实际意义。因此，词频向量中应该降低常见词的影响。一个常见的方法是将词频除以出现该词的文档数的对数。这种方法称为词频-逆文档频率（TF-IDF）。例如，假设`"and"`、`"basketball"`、`"more"`分别出现在了200、10、100个文档中：最终的矢量应该是`[1/log(200), 0/log(10), 2/log(100)]`，大约是`[0.19, 0., 0.43]`。`TextVectorization`层会有TF-IDF的选项。

> 笔记：如果标准预处理层不能满足你的任务，你还可以选择创建自定义预处理层，就像前面的`Standardization`。创建一个`keras.layers.PreprocessingLayer`子类，`adapt()`方法用于接收一个`data_sample`参数，或者再有一个`reset_state`参数：如果是`True`，则`adapt()`方法在计算新状态之前重置现有的状态；如果是`False`，会更新现有的状态。

可以看到，这些Keras预处理层可以使预处理更容易！现在，无论是自定义预处理层，还是使用Keras的，预处理都可以实时进行了。但在训练中，最好再提前进行预处理。下面来看看为什么，以及怎么做。



## TF Transform

预处理非常消耗算力，训练前做预处理相对于实时处理，可以极大的提高速度：数据在训练前，每个实例就处理一次，而不是在训练中每个实例在每个周期就处理一次。前面提到过，如果数据集小到可以存入内存，可以使用`cache()`方法。但如果太大，可以使用Apache Beam或Spark。它们可以在大数据上做高效的数据预处理，还可以分布进行，使用它们就能在训练前处理所有训练数据了。

虽然训练加速了，但带来一个问题：一旦模型训练好了，假如想部署到移动app上，还是需要写一些预处理数据的代码。假如想部署到TensorFlow.js，还是需要预处理代码。这是一个维护难题：无论何时想改变预处理逻辑，都需要更新Apache Beam的代码、移动端代码、JavaScript代码。不仅耗时，也容易出错：不同端的可能有细微的差别。训练/实际产品表现之间的偏差会导致bug或使效果大打折扣。

一种解决办法是在部署到app或浏览器之前，给训练好的模型加上额外的预处理层，来做实时的预处理。这样好多了，只有两套代码Apache Beam 或 Spark 代码，和预处理层代码。

如果只需定义一次预处理操作呢？这就是TF Transform要做的。TF Transform是[TensorFlow Extended (TFX)](简书)的一部分，这是一个端到端的TensorFlow模型生产化平台。首先，需要安装（TensorFlow没有捆绑）。然后通过TF Transform函数来做缩放、分桶等操作，一次性定义预处理函数。你还可以使用任意需要的TensorFlow运算。如果只有两个特征，预处理函数可能如下：

```py
import tensorflow_transform as tft

def preprocess(inputs):  # inputs = 输入特征批次
    median_age = inputs["housing_median_age"]
    ocean_proximity = inputs["ocean_proximity"]
    standardized_age = tft.scale_to_z_score(median_age)
    ocean_proximity_id = tft.compute_and_apply_vocabulary(ocean_proximity)
    return {
        "standardized_median_age": standardized_age,
        "ocean_proximity_id": ocean_proximity_id
    }
```

然后，TF Transform可以使用Apache Beam（可以使用其`AnalyzeAndTransformDataset`类）在整个训练集上应用这个`preprocess()`函数。在使用过程中，还会计算整个训练集上的必要统计数据：这个例子中，是`housing_median_age`和`the ocean_proximity`的平均值和标准差。计算这些数据的组件称为分析器。

更重要的，TF Transform还会生成一个等价的TensorFlow函数，可以放入部署的模型中。这个TF函数包括一些常量，对应于Apache Beam的统计值（平均值、标准差和词典）。

有了Data API、TFRecord，Keras预处理层和TF Transform，可以为训练搭建高度伸缩的输入管道，可以是生产又快，迁移性又好。

但是，如果只想使用标准数据集呢？只要使用TFDS就成了。



## TensorFlow Datasets（TFDS）项目

从[TensorFlow Datasets](简书)项目，可以非常方便的下载一些常见的数据集，从小数据集，比如MNIST或Fashion MNIST，到大数据集，比如ImageNet（需要大硬盘）。包括了图片数据集、文本数据集（包括翻译数据集）、和音频视频数据集。可以访问[https://www.tensorflow.org/datasets/datasets](简书)，查看完整列表，每个数据集都有介绍。

TensorFlow没有捆绑TFDS，所以需要使用pip安装库`tensorflow-datasets`。然后调用函数`tfds.load()`，就能下载数据集了（除非之前下载过），返回的数据是数据集的字典（通常是一个是训练集，一个是测试集）。例如，下载MNIST：

```dart
import tensorflow_datasets as tfds

dataset = tfds.load(name="mnist")
mnist_train, mnist_test = dataset["train"], dataset["test"]
```

然后可以对其应用任意转换（打散、批次、预提取），然后就可以训练模型了。下面是一个简单的例子：

```csharp
mnist_train = mnist_train.shuffle(10000).batch(32).prefetch(1)
for item in mnist_train:
    images = item["image"]
    labels = item["label"]
    [...]
```

> 提示：`load()`函数打散了每个下载的数据分片（只是对于训练集）。但还不够，最好再自己做打散。

注意，数据集中的每一项都是一个字典，包含特征和标签。但Keras期望每项都是一个包含两个元素（特征和标签）的元组。可以使用`map()`对数据集做转换，如下：

```cpp
mnist_train = mnist_train.shuffle(10000).batch(32)
mnist_train = mnist_train.map(lambda items: (items["image"], items["label"]))
mnist_train = mnist_train.prefetch(1)
```

更简单的方式是让`load()`函数来做这个工作，只要设定`as_supervised=True`（显然这只适用于有标签的数据集）。你还可以将数据集直接传给tf.keras模型：

```php
dataset = tfds.load(name="mnist", batch_size=32, as_supervised=True)
mnist_train = dataset["train"].prefetch(1)
model = keras.models.Sequential([...])
model.compile(loss="sparse_categorical_crossentropy", optimizer="sgd")
model.fit(mnist_train, epochs=5)
```

这一章很技术，你可能觉得没有神经网络的抽象美，但事实是深度学习经常要涉及大数据集，知道如何高效加载、解析和预处理，是一个非常重要的技能。下一章会学习卷积神经网络，它是一种用于图像处理和其它应用的、非常成功的神经网络。

