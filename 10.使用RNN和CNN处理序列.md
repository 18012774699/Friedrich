在本章中，我们将讨论**循环神经网络**，一类可以预测未来的网络（当然，是到某一点为止）。它们可以分析时间序列数据，比如股票价格，并告诉你什么时候买入和卖出。在自动驾驶系统中，他们可以预测行车轨迹，避免发生事故。更一般地说，它们可在任意长度的序列上工作，而不是截止目前我们讨论的只能在固定长度的输入上工作的网络。举个例子，它们可以将语句，文件，以及语音范本作为输入，应用在在自动翻译，语音到文本的自然语言处理应用中。

在本章中，我们将学习循环神经网络的基本概念，如何使用时间反向传播训练网络，然后用来预测时间序列。然后，会讨论RNN面对的两大难点：

- 不稳定梯度（换句话说，在第11章中讨论的梯度消失/爆炸），可以使用多种方法缓解，包括循环dropout和循环层归一化。
- 有限的短期记忆，可以通过LSTM 和 GRU 单元延长。

RNN不是唯一能处理序列数据的神经网络：对于小序列，常规紧密网络也可以；对于长序列，比如音频或文本，卷积神经网络也可以。我们会讨论这两种方法，本章最后会实现一个WaveNet：这是一种CNN架构，可以处理上万个时间步的序列。



# RNN

## 循环神经元和层

到目前为止，我们主要关注的是前馈神经网络，激活仅从输入层到输出层的一个方向流动（附录 E 中的几个网络除外）。 循环神经网络看起来非常像一个前馈神经网络，除了它也有连接指向后方。 让我们看一下最简单的 RNN，由一个神经元接收输入，产生一个输出，并将输出发送回自己，如图 15-1（左）所示。 在每个**时间步**`t`（也称为一个帧），这个循环神经元接收输入x(t)以及它自己的前一时间步长 y(t-1) 的输出。 因为第一个时间步骤没有上一次的输出，所以是0。可以用时间轴来表示这个微小的网络，如图 15-1（右）所示。 这被称为随时间展开网络。

![img](https:////upload-images.jianshu.io/upload_images/7178691-e945414fcaeeed67.png?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp)

​											图15-1 循环神经网络（左），随时间展开网络（右）

你可以轻松创建一个循环神经元层。 在每个时间步t，每个神经元都接收输入矢量x(t) 和前一个时间步 y(t-1) 的输出矢量，如图 15-2 所示。 **注意，输入和输出都是矢量（当只有一个神经元时，输出是一个标量）。**

![img](https:////upload-images.jianshu.io/upload_images/7178691-79c6bd0d2b869c73.png?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp)

​												图15-2 一层循环神经元（左），及其随时间展开（右）

每个循环神经元有两组权重：一组用于输入x(t)，另一组用于前一时间步长 y(t-1) 的输出。 我们称这些权重向量为 wx 和 wy。如果考虑的是整个循环神经元层，可以将所有权重矢量放到两个权重矩阵中，Wx 和 Wy。整个循环神经元层的输出可以用公式 15-1 表示（`b`是偏差项，`φ(·)`是激活函数，例如 ReLU）。
$$
y(t)=φ(W_x^Tx_{(t)}+W_y^Ty_{(t-1)}+b)
$$
​														公式15-1 单个实例的循环神经元层的输出

就像前馈神经网络一样，可以将所有输入和时间步`t`放到输入矩阵X(t)中，一次计算出整个小批次的输出：（见公式 15-2）。
$$
Y(t)=φ(X_{(t)}W_x+Y_{(t-1)}W_y+b)\\=φ([X_{(t)}\ Y_{(t-1)}W+b])\\with\ W=
\left[
\begin{matrix}
W_x\\
W_y
\end{matrix} 
\right]
$$
​															公式15-2 小批次实例的循环层输出

在这个公式中：

- Y(t) 是 m × nneurons **矩阵**，包含在小批次中每个实例在时间步`t`的层输出（`m`是小批次中的实例数，nneurons 是神经元数）。
- X(t) 是 m × ninputs 矩阵，包含所有实例的输入 （ninputs 是输入特征的数量）。
- Wx 是 ninputs × nneurons 矩阵，包含当前时间步的输入的连接权重。
- Wy 是 nneurons × nneurons 矩阵，包含上一个时间步的输出的连接权重。
- `b`是大小为 nneurons 的矢量，包含每个神经元的偏置项。
- 权重矩阵 Wx 和 Wy 通常纵向连接成一个权重矩阵`W`，形状为(ninputs + nneurons) × nneurons（见公式 15-2 的第二行）

> 注意，Y(t) 是 X(t) 和 Y(t-1) 的函数，Y(t-1)是 X(t-1)和  Y(t-2) 的函数，以此类推。这使得 Y(t) 是从时间`t = 0`开始的所有输入（即 X(0)，X(1)，...，X(t)）的函数。 在第一个时间步，`t = 0`，没有以前的输出，所以它们通常被假定为全零。



### 记忆单元

由于时间`t`的循环神经元的输出，是由所有先前时间步骤计算出来的的函数，你可以说它有一种记忆形式。神经网络的一部分，保留一些跨越时间步长的状态，称为**存储单元**（或简称为单元）。单个循环神经元或循环神经元层是非常基本的单元，只能学习短期规律（取决于具体任务，通常是10个时间步）。本章后面我们将介绍一些更为复杂和强大的单元，可以学习更长时间步的规律（也取决于具体任务，大概是100个时间步）。

一般情况下，时间步`t`的单元状态，记为 h(t)（`h`代表“隐藏”），是该时间步的某些输入和前一时间步状态的函数：h(t) = f(h(t–1), x(t))。 其在时间步`t`的输出，表示为 y(t)，也和前一状态和当前输入的函数有关。 我们已经讨论过的基本单元，输出等于单元状态，但是在更复杂的单元中并不总是如此，如图 15-3 所示。

![img](https:////upload-images.jianshu.io/upload_images/7178691-860eda4472257180.png?imageMogr2/auto-orient/strip|imageView2/2/w/1095/format/webp)

​															图15-3 单元的隐藏状态和输出可能不同

## 输入和输出序列**

RNN 可以同时输入序列并输出序列（见图 15-4，左上角的网络）。**这种序列到序列的网络可以有效预测时间序列**（如股票价格）：输入过去`N`天价格，则输出向未来移动一天的价格（即，从`N - 1`天前到明天）。

或者，你可以向网络输入一个序列，忽略除最后一项之外的所有输出（图15-4右上角的网络）。 换句话说，**这是一个序列到矢量的网络。** 例如，你可以向网络输入与电影评论相对应的单词序列，网络输出情感评分（例如，从`-1 [讨厌]`到`+1 [喜欢]`）。

相反，可以向网络一遍又一遍输入相同的矢量（见图15-4的左下角），输出一个序列。**这是一个矢量到序列的网络。** 例如，输入可以是图像（或是CNN的结果），输出是该图像的标题。

最后，可以**有一个序列到矢量的网络，称为编码器**，**后面跟着一个称为解码器的矢量到序列的网络**（见图15-4右下角）。 例如，这可以用于将句子从一种语言翻译成另一种语言。 给网络输入一种语言的一句话，编码器会把这个句子转换成单一的矢量表征，然后解码器将这个矢量解码成另一种语言的句子。 这种称为编码器 - 解码器的两步模型，比用单个序列到序列的 RNN实时地进行翻译要好得多，因为句子的最后一个单词可以影响翻译的第一句话，所以你需要等到听完整个句子才能翻译。第16章还会介绍如何实现编码器-解码器（会比图15-4中复杂）

![img](https:////upload-images.jianshu.io/upload_images/7178691-22b08969517e84c2.png?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp)

​		图15-4 序列到序列（左上），序列到矢量（右上），矢量到序列（左下），延迟序列到序列（右下）

## 训练RNN

训练RNN诀窍是在时间上展开（就像我们刚刚做的那样），然后只要使用常规反向传播（见图 15-5）。 这个策略被称为**时间上的反向传播**（BPTT）。

![img](https:////upload-images.jianshu.io/upload_images/7178691-0e4c0f46e526b037.png?imageMogr2/auto-orient/strip|imageView2/2/w/1091/format/webp)

​																			图15-5 随时间反向传播

就像在正常的反向传播中一样，展开的网络（用虚线箭头表示）中先有一个正向传播（虚线）。然后使用损失函数 C(Y(0), Y(1), …Y(T)) 评估输出序列（其中T是最大时间步）。这个损失函数会忽略一些输出，见图15-5（例如，在序列到矢量的RNN中，除了最后一项，其它的都被忽略了）。损失函数的梯度通过展开的网络反向传播（实线箭头）。最后使用在 BPTT 期间计算的梯度来更新模型参数。注意，梯度在损失函数所使用的所有输出中反向流动，而不仅仅通过最终输出（例如，在图 15-5 中，损失函数使用网络的最后三个输出 Y(2)，Y(3) 和 Y(4)，所以梯度流经这三个输出，但不通过 Y(0) 和 Y(1)。而且，由于在每个时间步骤使用相同的参数`W`和`b`，所以反向传播将做正确的事情并对所有时间步求和。



## 预测时间序列

假设你在研究网站每小时的活跃用户数，或是所在城市的每日气温，或公司的财务状况，用多种指标做季度衡量。在这些任务中，数据都是一个序列，每步有一个或多个值。这被称为时间序列。在前两个任务中，每个时间步只有一个值，它们是单变量时间序列。在财务状况的任务中，每个时间步有多个值（利润、欠账，等等），所以是多变量时间序列。典型的任务是预测未来值，称为“预测”。另一个任务是填空：预测（或“后测”）过去的缺失值，这被称为“填充”。例如，图15-6展示了3个单变量时间序列，每个都有50个时间步，目标是预测下一个时间步的值（用X表示）。

![img](https:////upload-images.jianshu.io/upload_images/7178691-50640c5c82d0ece6.png?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp)

​																		图15-6 时间序列预测

简单起见，使用函数`generate_time_series()`生成的时间序列，如下：

```py
def generate_time_series(batch_size, n_steps):
    freq1, freq2, offsets1, offsets2 = np.random.rand(4, batch_size, 1)
    time = np.linspace(0, 1, n_steps)
    series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10))  #   wave 1
    series += 0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20)) # + wave 2
    series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5)   # + noise
    return series[..., np.newaxis].astype(np.float32)
```

这个函数可以根据要求创建出时间序列（通过`batch_size`参数），长度为`n_steps`，每个时间步只有1个值。函数返回NumPy数组，形状是[批次大小, 时间步数, 1]，每个序列是两个正弦波之和（固定强度+随机频率和相位），加一点噪音。

> 笔记：当处理时间序列时（和其它类型的时间序列），输入特征通常用3D数组来表示，其形状是 [批次大小, 时间步数, 维度]，对于单变量时间序列，其维度是1，多变量时间序列的维度是其维度数。

用这个函数来创建训练集、验证集和测试集：

```py
n_steps = 50
series = generate_time_series(10000, n_steps + 1)
X_train, y_train = series[:7000, :n_steps], series[:7000, -1]
X_valid, y_valid = series[7000:9000, :n_steps], series[7000:9000, -1]
X_test, y_test = series[9000:, :n_steps], series[9000:, -1]
```

`X_train`包含7000个时间序列（即，形状是[7000, 50, 1]），`X_valid`有2000个，`X_test`有1000个。因为预测的是单一值，目标值是列矢量（`y_train`的形状是[7000, 1]）。



### 基线模型

使用RNN之前，最好有**基线指标**，否则做出来的模型可能比基线模型还糟。例如，最简单的方法，是预测每个序列的最后一个值。这个方法被称为**朴素预测**，有时很难被超越。在这个例子中，它的均方误差为0.020：

```py
>>> y_pred = X_valid[:, -1]
>>> np.mean(keras.losses.mean_squared_error(y_valid, y_pred))
0.020211367
```

另一个简单的方法是使用全连接网络。因为结果要是打平的特征列表，需要加一个`Flatten`层。使用简单线性回归模型，使预测值是时间序列中每个值的线性组合：

```py
model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[50, 1]),
    keras.layers.Dense(1)
])
```

使用MSE损失、Adam优化器编译模型，在训练集上训练20个周期，用验证集评估，最终得到的MSE值为0.004。比朴素预测强多了！



### 实现一个简单RNN

搭建一个简单RNN模型：

```py
model = keras.models.Sequential([
  keras.layers.SimpleRNN(1, input_shape=[None, 1])
])
```

这是能实现的最简单的RNN。只有1个层，1个神经元，如图15-1。**不用指定输入序列的长度**（和之前的模型不同），因为循环神经网络可以处理任意的时间步（这就是为什么将第一个输入维度设为`None`）。默认时，`SimpleRNN`使用双曲正切激活函数。和之前看到的一样：初始状态h(init)设为0，和时间序列的第一个值x(0)一起传递给神经元。神经元计算这两个值的加权和，对结果使用双曲正切激活函数，得到第一个输出y(0)。在简单RNN中，这个输出也是新状态h(0)。这个新状态和下一个输入值x(1)，按照这个流程，直到输出最后一个值，y49。所有这些都是同时对每个时间序列进行的。

> 笔记：默认时，Keras的循环层只返回最后一个输出。要让其返回每个时间步的输出，必须设置`return_sequences=True`。

用这个模型编译、训练、评估（和之前一样，用Adam训练20个周期），你会发现它的MSE只有0.014。击败了朴素预测，但不如简单线性模型。对于每个神经元，线性简单模型中每个时间步骤每个输入就有一个参数（前面用过的简单线性模型一共有51个参数）。**相反，对于简单RNN中每个循环神经元，每个输入、每个隐藏状态只有一个参数（在简单RNN中，就是每层循环神经元的数量），加上一个偏置项。**在这个简单RNN中，只有三个参数。

> 趋势和季节性
>
> 还有其它预测时间序列的模型，比如权重移动平均模型或自动回归集成移动平均（ARIMA）模型。某些模型需要先移出趋势和季节性。例如，如果要研究网站的活跃用户数，它每月会增长10%，就需要去掉这个趋势。训练好模型之后，在做预测时，你可以将趋势加回来做最终的预测。相似的，如果要预测防晒霜的每月销量，会观察到明显的季节性：每年夏天卖的多。需要将季节性从时间序列去除，比如计算每个时间步和前一年的差值（这个方法被称为差分）。然后，当训练好模型，做预测时，可以将季节性加回来，来得到最终结果。
>
> 使用RNN时，一般不需要做这些，但在有些任务中可以提高性能，因为模型不是非要学习这些趋势或季节性。

很显然，这个简单RNN过于简单了，性能不成。下面就来添加更多的循环层！



### 深度RNN

将多个神经元的层堆起来，见图15-7。就形成了深度RNN。

![img](https:////upload-images.jianshu.io/upload_images/7178691-0a727314de2f6f56.png?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp)

​												图15-7 深度RNN（左）和随时间展开的深度RNN（右）

用tf.keras实现深度RNN相当容易：将循环层堆起来就成。在这个例子中，我们使用三个`SimpleRNN`层（也可以添加其它类型的循环层，比如LSTM或GRU）：

```py
model = keras.models.Sequential([
    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),
    keras.layers.SimpleRNN(20, return_sequences=True),
    keras.layers.SimpleRNN(1)
])
```

> 警告：所有循环层一定要设置`return_sequences=True`（除了最后一层，因为最后一层只关心输出）。如果没有设置，输出的是2D数组（只有最终时间步的输出），而不是3D数组（包含所有时间步的输出），下一个循环层就接收不到3D格式的序列数据。

如果对这个模型做编译，训练和评估，其MSE值可以达到0.003。总算打败了线性模型！

最后一层不够理想：因为要预测单一值，每个时间步只能有一个输出值，最终层只能有一个神经元。但是一个神经元意味着隐藏态只有一个值。RNN大部分使用其他循环层的隐藏态的所有信息，最后一层的隐藏态不怎么用到。另外，因为`SimpleRNN`层默认使用tanh激活函数，预测值位于-1和1之间。想使用另一个激活函数该怎么办呢？出于这些原因，最好使用紧密层：运行更快，准确率差不多，可以选择任何激活函数。如果做了替换，要将第二个循环层的`return_sequences=True`删掉：

```py
model = keras.models.Sequential([
    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),
    keras.layers.SimpleRNN(20),
    keras.layers.Dense(1)
])
```

如果训练这个模型，会发现它收敛更快，效果也不错。



