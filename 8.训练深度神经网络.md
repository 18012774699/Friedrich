## 三大问题

- 你将面临棘手的梯度消失问题（或相关的梯度爆炸问题）：在反向传播过程中，梯度变得越来越小或越来越大。二者都会使较浅层难以训练；
- 要训练一个庞大的神经网络，但是数据量不足，或者标注成本很高；
- 训练可能非常慢；
- 具有数百万参数的模型将会有严重的过拟合训练集的风险，特别是在训练实例不多或存在噪音时。



## 梯度消失/爆炸问题

看一下logistic 激活函数（参见图 11-1），可以看到当输入变大（负或正）时，函数饱和在 0 或 1，导数非常接近 0。因此，当反向传播开始时， 它几乎没有梯度通过网络传播回来，而且由于反向传播通过顶层向下传递，所以存在的小梯度不断地被稀释，因此较低层确实没有任何东西可用。

### 权重初始化策略（Xavier和He）

Glorot 和 Bengio 在他们的论文中提出了一种显著缓解这个问题的方法。 我们需要信号在两个方向上正确地流动：在进行预测时是正向的，在反向传播梯度时是反向的。 我们不希望信号消失，也不希望它爆炸并饱和。 为了使信号正确流动，作者认为，我们需要每层输出的方差等于其输入的方差。

> 这里有一个比喻：如果将麦克风放大器的旋钮设置得太接近于零，人们听不到声音，但是如果将麦克风放大器设置得太大，声音就会饱和，人们就会听不懂你在说什么。 现在想象一下这样一个放大器的链条：它们都需要正确设置，以便在链条的末端响亮而清晰地发出声音。 你的声音必须以每个放大器的振幅相同的幅度出来。

实际上不可能保证两者都是一样的，除非这个层具有相同数量的输入和输出连接，但是他们提出了一个很好的折衷办法，在实践中证明这个折中办法非常好。

**Xavier 权重初始化**策略可以大大加快训练速度，这是导致深度学习目前取得成功的技巧之一。随机初始化时使用平均值为 0，标准差为 σ 的正态分布，或者-r~r的均匀分布，其中：
$$
σ=\sqrt {\frac{2}{n_{inputs}+n_{output}}}
$$

$$
r=\sqrt {\frac{6}{n_{inputs}+n_{output}}}
$$

当输入连接的数量大致等于输出连接的数量时，可以得到更简单的等式：
$$
σ=\frac{1}{\sqrt{n_{inputs}}}
$$

$$
r=\frac{\sqrt3}{\sqrt{n_{inputs}}}
$$

ReLU 激活函数（及其变体，包括简称 ELU 激活）的初始化策略有时称为 **He 初始化**。

默认情况下，`fully_connected()`函数（在第 10 章中介绍）使用 Xavier 初始化（具有均匀分布）。你可以通过使用如下所示的`variance_scaling_initializer()`函数来将其更改为 He 初始化：

```python
he_init = tf.contrib.layers.variance_scaling_initializer()
hidden1 = tf.layers.dense(X, n_hidden1, weights_initializer=he_init, scope="h1")
```

He 初始化只考虑了扇入，而不是像 Xavier 初始化那样扇入和扇出之间的平均值。 这也是`variance_scaling_initializer()`函数的默认值，但您可以通过设置参数`mode ="FAN_AVG"`来更改它。



### 非饱和激活函数

Glorot 和 Bengio 在 2010 年的论文中的一个见解是，消失/爆炸的梯度问题部分是由于激活函数的选择不好造成的。特别是 ReLU 激活函数，主要是因为它**对正值不会饱和，也因为它的计算速度很快**。

不幸的是，ReLU激活功能并不完美。 它有一个被称为 “ReLU 死区” 的问题。为了解决这个问题，你可能需要使用 ReLU 函数的一个变体，比如 leaky ReLU。

- LeakyReLUα(z)= max(αz，z)，事实上，设定`α= 0.2`（巨大 leak）似乎导致比`α= 0.01`（小 leak）更好的性能。
- 随机化 leaky ReLU（RReLU），其中`α`在训练期间在给定范围内随机挑选，并在测试期间固定为平均值。它表现相当好，似乎是一个正则项（减少训练集的过拟合风险）。
- 参数化 leaky ReLU（PReLU），其中`α`被授权在训练期间被学习（而不是超参数，它变成可以像任何其他参数一样被反向传播修改的参数）。据报道这在大型图像数据集上的表现强于 ReLU，但是对于较小的数据集，其具有过度拟合训练集的风险。
- 指数线性单元（exponential linear unit，ELU）激活函数，在他们的实验中表现优于所有的 ReLU 变体：训练时间减少，神经网络在测试集上表现的更好。

$$
ELU_α(z)=
\begin{cases}
α(exp(z)-1)& \text{z<0}\\
z& \text{z>=0}
\end{cases}
$$

#### ELU

它看起来很像 ReLU 函数，但有一些区别，主要区别在于：

- 首先它在`z < 0`时取负值，这使得该单元的平均输出接近于 0。这有助于减轻梯度消失问题，如前所述。 超参数`α`定义为当`z`是一个大的负数时，ELU 函数接近的值。它通常设置为 1，但是如果你愿意，你可以像调整其他超参数一样调整它。
- 其次，它对`z < 0`有一个非零的梯度，避免了神经元死亡的问题。
- 第三，函数在任何地方都是平滑的，包括`z = 0`左右，这有助于加速梯度下降，因为它不会弹回`z = 0`的左侧和右侧。
- ELU 激活函数的主要缺点是计算速度慢于 ReLU 及其变体（由于使用指数函数），但是在训练过程中，这是通过更快的收敛速度来补偿的。 然而，在测试时间，ELU 网络将比 ReLU 网络慢。

> 提示：
>  那么你应该使用哪个激活函数来处理深层神经网络的隐藏层？ 虽然你的里程会有所不同，一般 ELU > leaky ReLU（及其变体）> ReLU > tanh > sigmoid。 如果您关心运行时性能，那么您可能喜欢 leaky ReLU超过ELU。 如果你不想调整另一个超参数，你可以使用前面提到的默认的`α`值（leaky ReLU 为 0.01，ELU 为 1）。 如果您有充足的时间和计算能力，您可以使用交叉验证来评估其他激活函数，特别是如果您的神经网络过拟合，则为RReLU; 如果您拥有庞大的训练数据集，则为 PReLU。

TensorFlow 提供了一个可以用来建立神经网络的`elu()`函数。 调用`fully_connected()`函数时，只需设置`activation_fn`参数即可：

```python
hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.elu, name="hidden1")
```

TensorFlow 没有针对 leaky ReLU 的预定义函数，但是很容易定义：

```python
def leaky_relu(z, name=None):
    return tf.maximum(0.01 * z, z, name=name)

hidden1 = tf.layers.dense(X, n_hidden1, activation=leaky_relu, name="hidden1")
```



### 批量标准化

尽管使用 He初始化和 ELU（或任何 ReLU 变体）可以显著减少训练开始阶段的梯度消失/爆炸问题，但不保证在训练期间问题不会回来。

该技术包括在每层的激活函数之前在模型中添加操作，简单地对输入进行**zero-centering和规范化**，然后每层使用两个新参数（一个用于尺度变换，另一个用于偏移）对结果进行尺度变换和偏移。 换句话说，这个操作可以让模型学习到每层输入值的最佳尺度和平均值。

为了对输入进行归零和归一化，算法需要估计输入的均值和标准差。 它通过评估当前小批量输入的均值和标准差（因此命名为“批量标准化”）来实现。 整个操作在公式 11-3 中。
$$
1.\ \ \ \ μ_B=\frac{1}{m_B}\sum^{m_B}_{i=1}{x^{(i)}}
$$

$$
2.\ \ \ \ σ^2_B=\frac{1}{m_B}\sum^{m_B}_{i=1}{(x^{(i)}-μ_B)^2}
$$

$$
3.\ \ \ \ x^{(i)}_{hat}=\frac{x^{(i)}-μ_B}{\sqrt{σ_B^2+ϵ}}
$$

$$
4.\ \ \ \ z^{(i)}=γx^{(i)}_{hat}+β
$$

> μB是整个小批量B的经验均值
>
> σB是经验标准差，也是来评估整个小批量的。
>
> mB是小批量中的实例数量。
>
> Xi是以为零中心和标准化的输入。
>
> γ是层的缩放参数。
>
> β是层的便宜参数（偏移量）
>
> ϵ是一个很小的数字，以避免被零除（通常为`10 ^ -3`）。 这被称为平滑项（拉布拉斯平滑，Laplace Smoothing）。
>
> z(i) 是BN操作的输出：它是输入的**缩放和偏移量**。

> 这项技术大大改善了他们试验的所有深度神经网络。梯度消失问题大大减少了，他们可以使用饱和激活函数，如 tanh 甚至逻辑激活函数。网络对权重初始化也不那么敏感。他们能够使用更大的学习率，显著加快了学习过程。具体地，他们指出，“应用于最先进的图像分类模型，批标准减少了 14 倍的训练步骤实现了相同的精度，以显著的优势击败了原始模型。[...] 使用批量标准化的网络集合，我们改进了 ImageNet 分类上的最佳公布结果：达到4.9% 的前5个验证错误（和 4.8% 的测试错误），超出了人类评估者的准确性。批量标准化也像一个正则化项一样，减少了对其他正则化技术的需求（如本章稍后描述的 dropout）.

然而，批量标准化的确会增加模型的复杂性（尽管它不需要对输入数据进行标准化，因为第一个隐藏层会照顾到这一点，只要它是批量标准化的）。 此外，还存在运行时间的损失：由于每层所需的额外计算，神经网络的预测速度较慢。 所以，如果你需要预测闪电般快速，你可能想要检查普通ELU + He初始化执行之前如何执行批量标准化。

> 您可能会发现，训练起初相当缓慢，而梯度下降正在寻找每层的最佳缩放和偏移量，一旦找到恰当值，就会加速。

请注意，您还可以训练操作取决于更新操作：

```python
with tf.name_scope("train"):
    optimizer = tf.train.GradientDescentOptimizer(learning_rate)
    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
    with tf.control_dependencies(extra_update_ops):
        training_op = optimizer.minimize(loss)
```

这样，你只需要在训练过程中评估training_op，TensorFlow也会自动运行更新操作：

```python
sess.run(training_op, feed_dict={training: True, X: X_batch, y: y_batch})
```

### 梯度裁剪

减少梯度爆炸问题的一种常用技术是在反向传播过程中简单地剪切梯度，使它们不超过某个阈值（这对于递归神经网络是非常有用的；参见第 14 章）。 这就是所谓的梯度裁剪。一般来说，人们更喜欢批量标准化，但了解梯度裁剪以及如何实现它仍然是有用的。

在 TensorFlow 中，优化器的`minimize()`函数负责计算梯度并应用它们，所以您必须首先调用优化器的`compute_gradients()`方法，然后使用`clip_by_value()`函数创建一个裁剪梯度的操作，最后 创建一个操作来使用优化器的`apply_gradients()`方法应用裁剪梯度：

```
threshold = 1.0

optimizer = tf.train.GradientDescentOptimizer(learning_rate)
grads_and_vars = optimizer.compute_gradients(loss)
capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var)
              for grad, var in grads_and_vars]
training_op = optimizer.apply_gradients(capped_gvs)
```

像往常一样，您将在每个训练阶段运行这个`training_op`。 它将计算梯度，将它们裁剪到 -1.0 和 1.0 之间，并应用它们。 `threhold`是您可以调整的超参数。



## 有标签成本高

### 复用预训练层

从零开始训练一个非常大的 DNN 通常不是一个好主意，相反，您应该总是尝试找到一个现有的神经网络来完成与您正在尝试解决的任务类似的任务，然后复用这个网络的较低层：这就是所谓的**迁移学习**。**这不仅会大大加快训练速度，还将需要更少的训练数据。**

例如，假设您可以访问经过训练的 DNN，将图片分为 100 个不同的类别，包括动物，植物，车辆和日常物品。 您现在想要训练一个 DNN 来对特定类型的车辆进行分类。 这些任务非常相似，因此您应该尝试重新使用第一个网络的一部分（请参见图 11-4）。

如果新任务的输入图像与原始任务中使用的输入图像的大小不一致，则必须添加预处理步骤以将其大小调整为原始模型的预期大小。 更一般地说，如果输入具有类似的低级层次的特征，则迁移学习将很好地工作。 

#### 1、复用 TensorFlow 模型

#### 2、复用来自其它框架的模型

#### 3、冻结较低层

#### 4、缓存冻结层

#### 5、调整，删除或替换较高层

#### 6、Model Zoos



### 无监督的预训练

 **无监督学习--->参数初始值；监督学习--->fine-tuning，即训练有标注样本**。经过预训练最终能得到比较好的局部最优解。 

假设你想要解决一个复杂的任务，你没有太多的**有标签**的训练数据，但不幸的是，你不能找到一个类似的任务训练模型。 不要失去希望！ 首先，你当然应该尝试收集更多的有标签的训练数据，但是如果这太难或太昂贵，你仍然可以进行**无监督的训练**（见图 11-5）。

> 也就是说，如果你有很多**无标签**的训练数据，你可以尝试逐层训练层，从最低层开始，然后上升，使用无监督的特征检测算法，如限制玻尔兹曼机（RBM；见附录 E）或**自动编码器**（见第 15 章）。 每个层都被训练成先前训练过的层的输出（除了被训练的层之外的所有层都被冻结）。 一旦所有层都以这种方式进行了训练，就可以使用监督式学习（即反向传播）对网络进行微调。

这是一个相当漫长而乏味的过程，但通常运作良好。 实际上，这是 Geoffrey Hinton 和他的团队在 2006 年使用的技术，导致了神经网络的复兴和深度学习的成功。 直到 2010 年，无监督预训练（通常使用 RBM）是深度网络的标准，只有在梯度消失问题得到缓解之后，纯训练 DNN 才更为普遍。 然而，当您有一个复杂的任务需要解决时，无监督训练（现在通常使用自动编码器而不是 RBM）仍然是一个很好的选择。**如果没有类似的模型可以重复使用，而且有标签的训练数据很少，只有大量的无标签的训练数据。**（另一个选择是提出一个监督的任务，您可以轻松地收集大量标记的训练数据，然后使用迁移学习，如前所述。 例如，如果要训练一个模型来识别图片中的朋友，你可以在互联网上下载数百万张脸并训练一个分类器来检测两张脸是否相同，然后使用此分类器将新图片与你朋友的每张照片做比较。）



### 在辅助任务上预训练

最后一种选择是在辅助任务上训练第一个神经网络，您可以**使用容易获取的有标签的训练数据先训练一个神经网络，然后重新使用该网络的较低层来完成您的实际任务。**第一个神经网络的较低层将训练可能被第二个神经网络重复使用的特征检测器。

例如，如果你想建立一个识别面孔的系统，你可能只有几个人的照片 - 显然不足以训练一个好的分类器。 收集每个人的数百张照片将是不实际的。 但是，您可以在互联网上收集大量随机人员的照片，并训练第一个神经网络来检测两张不同的照片是否属于同一个人。 这样的网络将学习面部优秀的特征检测器，所以重复使用它的较低层将允许你使用很少的训练数据来训练一个好的面部分类器。

收集没有标签的训练样本通常是相当便宜的，但标注它们却相当昂贵。 在这种情况下，**一种常见的技术是将所有训练样例标记为“好”，然后通过破坏好的训练样例产生许多新的训练样例，并将这些样例标记为“坏”。**然后，您可以训练第一个神经网络将实例分类为好或不好。 例如，您可以下载数百万个句子，将其标记为“好”，然后在每个句子中随机更改一个单词，并将结果语句标记为“不好”。如果神经网络可以告诉“The dog sleeps”是好的句子，但“The dog they”是坏的，它可能知道相当多的语言。重用其较低层可能有助于许多语言处理任务。

**另一种方法是训练第一个网络为每个训练实例输出一个分数，并使用一个损失函数确保一个好的实例的分数大于一个坏实例的分数至少一定的边际。** 这被称为最大**边际学习**。

