## 梯度消失/爆炸问题

看一下logistic 激活函数（参见图 11-1），可以看到当输入变大（负或正）时，函数饱和在 0 或 1，导数非常接近 0。因此，当反向传播开始时， 它几乎没有梯度通过网络传播回来，而且由于反向传播通过顶层向下传递，所以存在的小梯度不断地被稀释，因此较低层确实没有任何东西可用。

Glorot 和 Bengio 在他们的论文中提出了一种显著缓解这个问题的方法。 我们需要信号在两个方向上正确地流动：在进行预测时是正向的，在反向传播梯度时是反向的。 我们不希望信号消失，也不希望它爆炸并饱和。 为了使信号正确流动，作者认为，我们需要每层输出的方差等于其输入的方差。

> 这里有一个比喻：如果将麦克风放大器的旋钮设置得太接近于零，人们听不到声音，但是如果将麦克风放大器设置得太大，声音就会饱和，人们就会听不懂你在说什么。 现在想象一下这样一个放大器的链条：它们都需要正确设置，以便在链条的末端响亮而清晰地发出声音。 你的声音必须以每个放大器的振幅相同的幅度出来。

实际上不可能保证两者都是一样的，除非这个层具有相同数量的输入和输出连接，但是他们提出了一个很好的折衷办法，在实践中证明这个折中办法非常好。

**Xavier 权重初始化**策略可以大大加快训练速度，这是导致深度学习目前取得成功的技巧之一。随机初始化时使用平均值为 0，标准差为 σ 的正态分布，或者-r~r的均匀分布，其中：
$$
σ=\sqrt {\frac{2}{n_{inputs}+n_{output}}}
$$

$$
r=\sqrt {\frac{6}{n_{inputs}+n_{output}}}
$$

当输入连接的数量大致等于输出连接的数量时，可以得到更简单的等式：
$$
σ=\frac{1}{\sqrt{n_{inputs}}}
$$

$$
r=\frac{\sqrt3}{\sqrt{n_{inputs}}}
$$

ReLU 激活函数（及其变体，包括简称 ELU 激活）的初始化策略有时称为 **He 初始化**。

默认情况下，`fully_connected()`函数（在第 10 章中介绍）使用 Xavier 初始化（具有均匀分布）。你可以通过使用如下所示的`variance_scaling_initializer()`函数来将其更改为 He 初始化：

```python
he_init = tf.contrib.layers.variance_scaling_initializer()
hidden1 = tf.layers.dense(X, n_hidden1, weights_initializer=he_init, scope="h1")
```

He 初始化只考虑了扇入，而不是像 Xavier 初始化那样扇入和扇出之间的平均值。 这也是`variance_scaling_initializer()`函数的默认值，但您可以通过设置参数`mode ="FAN_AVG"`来更改它。











