# 本章概述

自然语言处理：

- character RNN（预测句子中出现的下一个字符）
- 情感分析（例如，读取影评，提取评价者对电影的感情），这次是将句子当做词的序列来处理。
- 搭建编码器-解码器架构，来做神经网络机器翻译（NMT）。

注意力机制：

正如其名字，这是一种可以选择输入指定部分，模型在每个时间步都得聚焦的神经网络组件。

首先，会介绍如何使用注意力机制提升基于RNN的编码器-解码器架构的性能。

然后会完全摒弃RNN，介绍只使用注意力的架构，被称为Transformer（转换器）。

最后，会介绍2018、2019两年NLP领域的进展，包括强大的语言模型，比如GPT-2和Bert，两者都是基于Transformer的。



## 使用字符层面 RNN生成莎士比亚风格的文本

在2015年一篇著名的、名为《The Unreasonable Effectiveness of Recurrent Neural Networks》博客中，Andrej Karpathy展示了如何训练RNN，来预测句子中的下一个字符。这个 Char-RNN 可以用来生成小说，每次一个字符。下面是一段简短的、由Char-RNN模型（在莎士比亚全部著作上训练而成）生成的文本：

```shell
PANDARUS:
Alas, I think he shall be come approached and the day
When little srain would be attain’d into being never fed,
And who is but a chain and subjects of his death,
I should not sleep.
```

虽然文笔一般，但只是通过学习来预测一句话中的下一个字符，模型在单词、语法、断句等等方面做的很好。接下来一步一步搭建Char-RNN，从创建数据集开始。



### 创建训练数据集

首先，使用Keras的`get_file()`函数，从 Andrej Karpathy 的 [Char-RNN 项目](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2Fkarpathy%2Fchar-rnn)，下载所有莎士比亚的作品：

```python
shakespeare_url = "https://homl.info/shakespeare" # shortcut URL
filepath = keras.utils.get_file("shakespeare.txt", shakespeare_url)
with open(filepath) as f:
    shakespeare_text = f.read()
```

然后，将每个字符编码为一个整数。方法之一是创建一个自定义预处理层，就像之前在第13章做的那样。但在这里，使用Keras的`Tokenizer`会更加简单。首先，将一个将tokenizer拟合到文本：tokenizer能从文本中发现所有的字符，并将所有字符映射到不同的字符ID，映射从1开始（注意不是从0开始，0是用来做遮挡的，后面会看到）：

```php
tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)
tokenizer.fit_on_texts([shakespeare_text])
```

设置`char_level=True`，以得到字符级别的编码，而不是默认的单词级别的编码。这个tokenizer默认将所有文本转换成了小写（如果不想这样，可以设置`lower=False`）。现在tokenizer可以将一整句（或句子列表）编码为字符ID列表，这可以告诉我们文本中有多少个独立的字符，以及总字符数：

```py
>>> tokenizer.texts_to_sequences(["First"])
[[20, 6, 9, 8, 3]]
>>> tokenizer.sequences_to_texts([[20, 6, 9, 8, 3]])
['f i r s t']
>>> max_id = len(tokenizer.word_index) # number of distinct characters
>>> dataset_size = tokenizer.document_count # total number of characters
```

现在对完整文本做编码，将每个字符都用ID来表示（减1使ID从0到38，而不是1到39）：

```python
[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1
```

继续之前，需要将数据集分成训练集、验证集和测试集。不能大论字符，该怎么处理这种序列式的数据集呢？



### 如何切分序列数据集

什么是静态的(规律)？例如：季节趋势就是非静态的、包含博弈的非静态的。

避免训练集、验证集、测试集发生重合非常重要。例如，可以取90%的文本作为训练集，5%作为验证集，5%作为测试集。在这三个数据之间留出空隙，以避免段落重叠也是非常好的主意。

**当处理时间序列时，通常按照时间切分**：例如，可以将从2000到2012的数据作为训练集，2013年到2015年作为验证集，2016年到2018年作为测试集。但是，**在另一些任务中，可以按照其它维度来切分**，可以得到更长的时间周期进行训练。例如，10000家公司从2000年到2018年的金融健康数据，可以按照不同公司来切分。但是，很可能其中一些公司是高度关联的（比如，经济领域的公司涨落相同），**如果训练集和测试集中有关联的公司，则测试集的意义就不大，泛化误差会存在偏移**。

因此，在时间维度上切分更加安全 —— 但这实际是**默认RNN可以（在训练集）从过去学到的规律也适用于将来**。换句话说，我们假设时间序列是静态的（至少是在一个较宽的区间内）。对于时间序列，这个假设是合理的（比如，化学反应就是这样，化学定理不会每天发生改变），但其它的就不是（例如，金融市场就不是静态的，一旦交易员发现规律并从中牟利，规律就会改变）。要保证时间序列确实是**静态的**，**可以在验证集上画出模型随时间的误差：如果模型在验证集的前端表现优于后段，则时间序列可能就不够静态，最好是在一个更短的时间区间内训练**。

总而言之，将时间序列切分成训练集、验证集和测试集不是简单的工作，怎么做要取决于具体的任务。

回到莎士比亚！这里将前90%的文本作为训练集（剩下的作为验证集和测试集），创建一个`tf.data.Dataset`，可以从这个集合一个个返回每个字符：

```python
train_size = dataset_size * 90 // 100
dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])
```



### 将序列数据集切分成多个窗口

现在训练集包含一个单独的长序列，超过100万的任务，所以不能直接在这个训练集上训练神经网络：现在的RNN等同于一个有100万层的深度网络，只有一个超长的单实例来训练。所以，得**使用数据集的`window()`方法，将这个长序列转化为许多小窗口文本**。每个实例都是完整文本的相对短的子字符串，RNN只在这些子字符串上展开。这被称为**截断沿时间反向传播**。调用`window()`方法创建一个短文本窗口的数据集：

```bash
n_steps = 100
window_length = n_steps + 1 # target = input 向前移动1个字符
dataset = dataset.window(window_length, shift=1, drop_remainder=True)
```

> 提示：可以调节`n_steps`：用短输入序列训练RNN更为简单，但肯定的是RNN学不到任何长度超过`n_steps`的规律，所以`n_steps`不要太短。

默认情况下，`window()`方法创建的窗口是不重叠的，但为了获得可能的最大训练集，我们设定`shift=1`，好让第一个窗口包含字符0到100，第二个窗口包含字符1到101，等等。为了确保所有窗口是准确的101个字符长度（为了不做填充而创建批次），设置`drop_remainder=True`（否则，最后的100个窗口会包含100个字符、99个字符，一直到1个字符）。

`window()`方法创建了一个包含窗口的数据集，每个窗口也是数据集。这是一个嵌套的数据集，类似于列表的列表。当调用数据集方法处理（比如、打散或做批次）每个窗口时，这样会很方便。但是，不能直接使用嵌套数据集来训练，因为模型要的输入是张量，不是数据集。因此，必须调用`flat_map()`方法：它能将嵌套数据集转换成打平的数据集。例如，假设 {1, 2, 3} 表示包含张量1、2、3的序列。如果将嵌套数据集 {{1, 2}, {3, 4, 5, 6}} 打平，就会得到 {1, 2, 3, 4, 5, 6} 。另外，`flat_map()`方法可以接收函数作为参数，可以处理嵌套数据集的每个数据集。例如，如果将函数 `lambda ds: ds.batch(2)` 传递给 `flat_map()` ，它能将 {{1, 2}, {3, 4, 5, 6}} 转变为 {[1, 2], [3, 4], [5, 6]} ：这是一个张量大小为2的数据集。

有了这些知识，就可以打平数据集了：

```python
dataset = dataset.flat_map(lambda window: window.batch(window_length))
```

我们在每个窗口上调用了`batch(window_length)`：因为所有窗口都是这个长度，对于每个窗口，都能得到一个独立的张量。现在的数据集包含连续的窗口，每个有101个字符。因为梯度下降在训练集中的实例独立同分布时的效果最好，需要打散这些窗口。然后我们可以对窗口做批次，分割输入（前100个字符）和目标（最后一个字符）：

```python
batch_size = 32
dataset = dataset.shuffle(10000).batch(batch_size)
dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))
```

图16-1总结了数据集准备步骤（窗口长度是11，不是101，批次大小是3，不是32）。

![img](https:////upload-images.jianshu.io/upload_images/7178691-9b186df45accbdd7.png?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp)

​																图16-1 准备打散窗口的数据集

第13章讨论过，类型输入特征通常都要编码，一般是独热编码或嵌入。这里，使用独热编码，因为独立字符不多（只有39）：

```python
dataset = dataset.map(
    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))
```

最后，加上预提取：

```python
dataset = dataset.prefetch(1)
```

就是这样！准备数据集是最麻烦的部分。下面开始搭建模型。



### 搭建并训练Char-RNN模型

根据前面的100个字符预测下一个字符，可以使用一个RNN，含有两个GRU层，每个128个单元，每个单元对输入（`dropout`）和隐藏态（`recurrent_dropout`）的丢失率是20%。如果需要的话，后面可以微调这些超参数。输出层是一个时间分布的紧密层，有39个单元（`max_id`），因为文本中有39个不同的字符，需要输出每个可能字符（在每个时间步）的概率。输出概率之后应为1，所以使用softmax激活函数。然后可以使用`"sparse_categorical_crossentropy"`损失和Adam优化器，编译模型。最后，就可以训练模型几个周期了（训练过程可能要几个小时，取决于硬件）：

```python
model = keras.models.Sequential([
    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id],
                     dropout=0.2, recurrent_dropout=0.2),
    keras.layers.GRU(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),
    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation="softmax"))
])
model.compile(loss="sparse_categorical_crossentropy", optimizer="Adam")
history = model.fit(dataset, epochs=20)
```



### 使用Char-RNN模型

现在就有了可以预测莎士比亚要写的下一个字符的模型了。输入数据之前，先要像之前那样做预处理，因此写个小函数来做预处理：

```python
def preprocess(texts):
    X = np.array(tokenizer.texts_to_sequences(texts)) - 1
    return tf.one_hot(X, max_id)
```

现在，用这个模型预测文本中的下一个字母：

```python
>>> X_new = preprocess(["How are yo"])
>>> Y_pred = model.predict_classes(X_new)
>>> tokenizer.sequences_to_texts(Y_pred + 1)[0][-1] # 1st sentence, last char
'u'
```

预测成功！接下来用这个模型生成文本。



### 生成假莎士比亚文本

要使用Char-RNN生成新文本，我们可以给模型输入一些文本，让模型预测出下一个字母，将字母添加到文本的尾部，再将延长后的文本输入给模型，预测下一个字母，以此类推。但在实际中，这会导致相同的单词不断重复。相反的，可以使用`tf.random.categorical()`函数，随机挑选下一个字符，概率等同于估计概率。这样就能生成一些多样且有趣的文本。根据类的对数概率（logits），`categorical()`函数随机从类索引采样。为了对生成文本的多样性更可控，我们可以用一个称为“温度“的可调节的数来除以对数概率：温度接近0，会利于高概率字符，而高温度会使所有字符概率相近。下面的`next_char()`函数使用这个方法，来挑选添加进文本中的字符：

```python
def next_char(text, temperature=1):
    X_new = preprocess([text])
    y_proba = model.predict(X_new)[0, -1:, :]
    rescaled_logits = tf.math.log(y_proba) / temperature
    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1
    return tokenizer.sequences_to_texts(char_id.numpy())[0]
```

然后，可以写一个小函数，重复调用`next_char()`：

```python
def complete_text(text, n_chars=50, temperature=1):
    for _ in range(n_chars):
        text += next_char(text, temperature)
    return text
```

现在就可以生成一些文本了！先尝试下不同的温度数：

```python
>>> print(complete_text("t", temperature=0.2))
the belly the great and who shall be the belly the
>>> print(complete_text("w", temperature=1))
thing? or why you gremio.
who make which the first
>>> print(complete_text("w", temperature=2))
th no cce:
yeolg-hormer firi. a play asks.
fol rusb
```

显然，当温度数接近1时，我们的莎士比亚模型效果最好。**为了生成更有信服力的文字，可以尝试用更多`GRU`层、每层更多的神经元、更长的训练时间，添加正则（例如，可以在`GRU`层中设置`recurrent_dropout=0.3`）。**另外，模型不能学习长度超过`n_steps`（只有100个字符）的规律。你可以使用更大的窗口，但也会让训练更为困难，甚至LSTM和GRU单元也不能处理长序列。另外，还可以使用有状态RNN。



## 有状态RNN

到目前为止，我们只使用了**无状态RNN**：**在每个训练迭代中，模型从全是0的隐藏状态开始训练，然后在每个时间步更新其状态，在最后一个时间步，隐藏态就被丢掉，以后再也不用了。**如果让RNN保留这个状态，供下一个训练批次使用如何呢？**这么做的话，尽管反向传播只在短序列传播，模型也可以学到长时规律。**这被称为**有状态RNN**。

首先，有状态RNN只在前一批次的序列离开，后一批次中的对应输入序列开始的情况下才有意义。所以第一件要做的事情是**使用没有重叠的输入序列**（而不是用来训练无状态RNN时的打散和重叠的序列）。当创建`Dataset`时，调用`window()`必须使用`shift=n_steps`（而不是`shift=1`）。另外，不能使用`shuffle()`方法。因此，准备有状态RNN数据集的批次会麻烦些。**事实上，如果调用`batch(32)`，32个连续的窗口会放到一个相同的批次中，后面的批次不会接着这些窗口。第一个批次含有窗口1到32，第二个批次批次含有窗口33到64，因此每个批次中的第一个窗口（窗口1和33），它们是不连续的。最简单办法是使用只包含一个窗口的“批次”**：

```python
dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])
dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)
dataset = dataset.flat_map(lambda window: window.batch(window_length))
dataset = dataset.batch(1)
dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))
dataset = dataset.map(lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))
dataset = dataset.prefetch(1)
```

图16-2展示了处理的第一步。

![img](https:////upload-images.jianshu.io/upload_images/7178691-ce6ba5d54bf30f2c.png?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp)

​												图16-2 为有状态RNN准备连续序列片段的数据集

做批次虽然麻烦，但可以实现。例如，我们可以将莎士比亚作品切分成32段等长的文本，每个做成一个连续序列的数据集，最后使用`tf.train.Dataset.zip(datasets).map(lambda *windows: tf.stack(windows))`来创建合适的连续批次，批次中的nth输入序列紧跟着nth结束的地方（notebook中有完整代码）。

现在创建有状态RNN。首先，创建每个循环层时需要设置`stateful=True`。第二，有状态RNN需要知道批次大小（因为要为批次中的输入序列保存状态），所以要在第一层中设置`batch_input_shape`参数。不用指定第二个维度，因为不限制序列的长度：

```python
model = keras.models.Sequential([
    keras.layers.GRU(128, return_sequences=True, stateful=True,
                     dropout=0.2, recurrent_dropout=0.2,
                     batch_input_shape=[batch_size, None, max_id]),
    keras.layers.GRU(128, return_sequences=True, stateful=True,
                     dropout=0.2, recurrent_dropout=0.2),
    keras.layers.TimeDistributed(keras.layers.Dense(max_id, ctivation="softmax"))
])
```

在每个周期之后，回到文本开头之前，需要**重设状态**。要这么做，可以使用一个小调回：

```python
class ResetStatesCallback(keras.callbacks.Callback):
    def on_epoch_begin(self, epoch, logs):
        self.model.reset_states()
```

现在可以编译、训练模型了（周期数更多，是因为每个周期比之前变短了，每个批次只有一个实例）：

```python
model.compile(loss="sparse_categorical_crossentropy", optimizer="Adam")
model.fit(dataset, epochs=50, callbacks=[ResetStatesCallback()])
```

> 提示：训练好模型之后，只能预测训练时相同大小的批次。为了避免这个限制，可以创建一个相同的无状态模型，将有状态模型的参数复制到里面。

创建了一个**字符层面**的模型，接下来看看**词层面**的模型，并做一个常见的自然语言处理任务：**情感分析**。我们会学习使用**遮掩**来处理变化长度的序列。



