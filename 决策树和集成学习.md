## **决策树**

 决策树对特征缩放不敏感 

### **决策树的训练和可视化**

 你可以通过使用export_graphviz()方法，通过生成一个叫做iris_tree.dot的图形定义文件将一个训练好的决策树模型可视化。 

```
dot -Tpng iris_tree.dot -o iris_tree.png
```

### **开始预测**

 决策树的众多特性之一就是， 它不需要太多的数据预处理， 尤其是不需要进行特征的缩放或者归一化。 

### **CART 训练算法**

### **Gini不纯度或信息熵**

### **正则化超参数**

### **不稳定性**

## **集成学习**

 特别著名的集成方法，包括 *bagging, boosting, stacking*，和其他一些算法。我们也会讨论随机森林。 



### 大数定



### 硬投票和软投票



### 集成方法

-  使用不同的训练算法去得到一些不同的分类器。
-   每一个分类器都使用相同的训练算法，但是在不同的训练集上去训练它们。 (Bagging 和 Pasting)



### Out-of-Bag 评价

对于 Bagging 来说，一些实例可能被一些分类器重复采样，但其他的有可能不会被采样。BaggingClassifier默认采样。BaggingClassifier默认是有放回的采样m个实例 （bootstrap=True），其中m是训练集的大小，这意味着平均下来只有63%的训练实例被每个分类器采样，剩下的37%个没有被采样的训练实例就叫做 *Out-of-Bag* 实例。注意对于每一个的分类器它们的 37% 不是相同的。

因为在训练中分类器从来没有看到过 oob 实例，所以它可以在这些实例上进行评估，而不需要单独的验证集或交叉验证。你可以拿出每一个分类器的 oob 来评估集成本身。



### 特征采样

```
BaggingClassifier也支持采样特征。它被两个超参数max_features和bootstrap_features控制。他们的工作方式和max_samples和bootstrap一样，但这是对于特征采样而不是实例采样。因此，每一个分类器都会被在随机的输入特征内进行训练。

当你在处理高维度输入下（例如图片）此方法尤其有效。对训练实例和特征的采样被叫做随机贴片。保留了所有的训练实例（例如bootstrap=False和max_samples=1.0），但是对特征采样（bootstrap_features=True并且/或者max_features小于 1.0）叫做随机子空间。

采样特征导致更多的预测多样性，用高偏差换低方差。
```

## **随机森林**

随机森林算法在树生长时引入了额外的随机；与在节点分裂时需要找到最好分裂特征相反（详见第六章），它在一个随机的特征集中找最好的特征。它导致了树的差异性，并且再一次用高偏差换低方差，总的来说是一个更好的模型。



### 极端随机树

 ExtraTreesClassifier/*ExtraTreesRegressor* 



### 特征重要度

如果你观察一个单一决策树，重要的特征会出现在更靠近根部的位置，而不重要的特征会经常出现在靠近叶子的位置。因此我们可以通过计算一个特征在森林的全部树中出现的平均深度来预测特征的重要性。sklearn 在训练后会自动计算每个特征的重要度。你可以通过feature_importances_变量来查看结果。

随机森林可以非常方便快速得了解哪些特征实际上是重要的，特别是你需要进行特征选择的时候。 



## 提升

### **Adaboost**

 适应性提升，是 *Adaptive Boosting* 的简称 