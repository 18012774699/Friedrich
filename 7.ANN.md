## ANN

### 感知器（Perceptron）

感知器是最简单的人工神经网络结构之一，由 Frank Rosenblatt 发明于 1957。它是基于一种稍微不同的人工神经元（见图 10-4），称为线性阈值单元（LTU）。

#### 线性阈值单元（LTU）

- 输入
- 权重加和
- 阶跃函数
- 输出

最常见的在感知器中使用的阶跃函数是 Heaviside 阶跃函数（见方程 10-1）。
$$
heaviside(z)=
\begin{cases}
0& \text{z<0}\\
1& \text{z>=0}
\end{cases}
$$
有时使用符号函数代替。
$$
sgn(z)=
\begin{cases}
-1& \text{z<0}\\
0& \text{z=0}\\
+1& \text{z>=0}
\end{cases}
$$
感知器简单地由一层 LTU 组成，每个神经元连接到所有输入。这些连接通常用特殊的被称为输入神经元的传递神经元来表示：它们只输出它们所输入的任何输入。此外，通常添加额外偏置特征（X0=1）。这种偏置特性通常用一种称为偏置神经元的特殊类型的神经元来表示，它总是输出 1。

####  感知器的训练方式 

感知器学习算法类似于随机梯度下降。
$$
w_{i,j}^{next step}=w_{i,j}+η(y_{j}^{hat}-y_j)x_i
$$

- 其中`Wi,j`是第`i`输入神经元与第`j`个输出神经元之间的连接权重。
- `xi`是当前训练实例的第`i`个输入值。
- `hat yj`是当前训练实例的第`j`个输出神经元的输出。
- `yj`是当前训练实例的第`j`个输出神经元的目标输出。
- `η`是学习率。

事实上，sklearn 的感知器类相当于使用具有以下超参数的 SGD 分类器：`loss="perceptron"`，`learning_rate="constant"`，`eta0=1`（学习率），`penalty=None`（无正则化）。

 注意，与逻辑回归分类器相反，感知器不输出类概率，而是基于硬阈值进行预测。 

  感知器不能解决一些琐碎的问题（例如，异或（XOR）分类问题）.



####  多层感知器（MLP） 

 感知器的一些局限性可以通过堆叠多个感知器来消除。由此产生的人工神经网络被称为多层感知器（MLP）。特别地，MLP 可以解决 XOR 问题 。



### 深度神经网络（DNN）

MLP 由一个（通过）输入层、一个或多个称为隐藏层的 LTU 组成，一个最终层 LTU 称为输出层（见图 10-7）。除了输出层之外的每一层包括偏置神经元，并且全连接到下一层。当人工神经网络有两个或多个隐含层时，称为深度神经网络（DNN）。



#### 反向传播算法

第 9 章我们将其描述为使用反向自动微分的梯度下降。

让我们更简短一些：对于每个训练实例，反向传播算法首先进行预测（前向），测量误差，然后反向遍历每个层来测量每个连接（反向传递）的误差贡献，最后稍微调整连接器权值以减少误差（梯度下降步长）。

为了使算法能够正常工作，作者对 MLP 的体系结构进行了一个关键性的改变：用 **Logistic 函数**代替了阶跃函数，`σ(z) = 1 / (1 + exp(–z))`。这是必要的，因为阶跃函数只包含平坦的段，因此没有梯度来工作（梯度下降不能在平面上移动），而 Logistic 函数到处都有一个定义良好的非零导数，允许梯度下降在每步上取得一些进展。反向传播算法可以与**其他激活函数**一起使用，而不是 Logistic 函数。另外两个流行的激活函数是：

- 双曲正切函数 `tanh (z) = 2σ(2z) – 1`
- ReLU 函数

 事实证明，另外两种方案更好。

![activation-function](D:\AI\Friedrich\pictrue\activation-function.png)

#### 微调神经网络超参数

神经网络的灵活性也是其主要缺点之一：有很多超参数要进行调整。 不仅可以使用任何可想象的网络拓扑（如何神经元互连），而且即使在简单的 MLP 中，您可以更改层数，每层神经元数，每层使用的激活函数类型，权重初始化逻辑等等。 你怎么知道什么组合的超参数是最适合你的任务？

当然，您可以使用具有交叉验证的**网格搜索**来查找正确的超参数，就像您在前几章中所做的那样，但是由于要调整许多超参数，并且由于在大型数据集上训练神经网络需要很多时间， 您只能在合理的时间内探索超参数空间的一小部分。 正如我们在第2章中讨论的那样，使用**随机搜索**要好得多。另一个选择是使用诸如 **Oscar** 之类的工具，它可以实现更复杂的算法，以帮助您快速找到一组好的超参数.

它有助于了解每个超级参数的值是合理的，因此您可以限制搜索空间。 我们从隐藏层数开始。



#### 隐藏层数量

对于许多问题，您只需从单个隐藏层开始，就能获得理想的结果。 实际上已经表明，只有一个隐藏层的 MLP 可以建模甚至最复杂的功能，只要它具有足够的神经元。 长期以来，这些事实说服了研究人员，没有必要调查任何更深层次的神经网络。 但是他们忽略了这样一个事实：深层网络具有比浅层网络更高的参数效率：他们可以使用比浅网格更少的神经元来建模复杂的函数，使得训练更快。

**要了解为什么**，假设您被要求使用一些绘图软件绘制一个森林，但是您被禁止使用复制/粘贴。 你必须单独绘制每棵树，每枝分枝，每叶叶。 如果你可以画一个叶，复制/粘贴它来绘制一个分支，然后复制/粘贴该分支来创建一个树，最后复制/粘贴这个树来制作一个林，你将很快完成。 现实世界的数据通常以这样一种分层的方式进行结构化，DNN 自动利用这一事实：较低的隐藏层模拟低级结构（例如，各种形状和方向的线段），中间隐藏层将这些低级结构组合到 模型中级结构（例如，正方形，圆形）和最高隐藏层和输出层将这些中间结构组合在一起，以模拟高级结构（如面）。

这种分层架构不仅可以帮助 DNN 更快地融合到一个很好的解决方案，而且还可以提高其将其推广到新数据集的能力。 例如，如果您已经训练了模型以识别图片中的脸部，并且您现在想要训练一个新的神经网络来识别发型，那么您可以通过重新使用第一个网络的较低层次来启动训练。 而不是随机初始化新神经网络的前几层的权重和偏置，您可以将其初始化为第一个网络的较低层的权重和偏置的值。这样，网络将不必从大多数图片中低结构中从头学习；它只需要学习更高层次的结构（例如发型）。

总而言之，对于许多问题，您可以从一个或两个隐藏层开始，它可以正常工作（例如，您可以使用只有一个隐藏层和几百个神经元，在 MNIST 数据集上容易达到 97% 以上的准确度使用两个具有相同总神经元数量的隐藏层，在大致相同的训练时间量中精确度为 98%）。**对于更复杂的问题**，您可以逐渐增加隐藏层的数量，直到您开始覆盖训练集。**非常复杂的任务**，例如大型图像分类或语音识别，通常需要具有数十个层（或甚至数百个**但不完全相连的网络**）的网络，正如我们将在第 13 章中看到的那样），并且需要大量的训练数据。但是，您将很少从头开始训练这样的网络：重用预先训练的最先进的网络执行类似任务的部分更为常见。训练将会更快，需要更少的数据（我们将在第 11 章中进行讨论）



#### 每层隐藏层的神经元数量

显然，输入和输出层中神经元的数量由您的任务需要的输入和输出类型决定。例如，MNIST 任务需要`28×28 = 784`个输入神经元和 10 个输出神经元。**对于隐藏的层次来说，通常的做法是将其设置为形成一个漏斗，每个层面上的神经元越来越少，原因在于许多低级别功能可以合并成更少的高级功能。**例如，MNIST 的典型神经网络可能具有两个隐藏层，第一个具有 300 个神经元，第二个具有 100 个。但是，这种做法现在并不常见，您可以为所有隐藏层使用相同的大小 - 例如，所有隐藏的层与 150 个神经元：这样只用调整一次超参数而不是每层都需要调整（因为如果每层一样，比如 150，之后调就每层都调成 160）。就像层数一样，您可以尝试逐渐增加神经元的数量，直到网络开始过度拟合。一般来说，通过增加每层的神经元数量，可以增加层数，从而获得更多的消耗。不幸的是，正如你所看到的，找到完美的神经元数量仍然是黑色的艺术.

**一个更简单的方法是选择一个具有比实际需要的更多层次和神经元的模型，然后使用早期停止来防止它过度拟合**（以及其他正则化技术，特别是 drop out，我们将在第 11 章中看到）。 这被称为“拉伸裤”的方法：而不是浪费时间寻找完美匹配您的大小的裤子，只需使用大型伸缩裤，缩小到合适的尺寸。



#### 激活函数

在大多数情况下，您可以在隐藏层中使用 **ReLU** 激活函数（或其中一个变体，我们将在第 11 章中看到）。 **与其他激活函数相比，计算速度要快一些，而梯度下降在局部最高点上并不会被卡住，因为它不会对大的输入值饱和**（与逻辑函数或双曲正切函数相反, 他们容易在 1 饱和)

对于输出层，softmax 激活函数通常是分类任务的良好选择（当这些类是互斥的时）。 对于回归任务，您完全可以不使用激活函数。

