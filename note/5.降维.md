## 关于降维

```
警告：降维肯定会丢失一些信息（这就好比将一个图片压缩成 JPEG 的格式会降低图像的质量），因此即使这种方法可以加快训练的速度，同时也会让你的系统表现的稍微差一点。
降维会让你的工作流水线更复杂因而更难维护。
所以你应该先尝试使用原始的数据来训练，如果训练速度太慢的话再考虑使用降维。
在某些情况下，降低训练集数据的维度可能会筛选掉一些噪音和不必要的细节，这可能会让你的结果比降维之前更好（这种情况通常不会发生；它只会加快你训练的速度）。
```

降维除了可以加快训练速度外，在数据可视化方面（或者 DataViz）也十分有用。降低特征维度到 2（或者 3）维从而可以在图中画出一个高维度的训练集，让我们可以通过视觉直观的发现一些非常重要的信息，比如聚类。

两种主要的降维方法：投影（projection）和流形学习（Manifold Learning）

三种流行的降维技术：主成分分析（PCA），核主成分分析（Kernel PCA）和局部线性嵌入（LLE）



## 降维的方法和思想

### 投影（Projection）

投影并不总是降维的最佳方法。



### 流形学习

```
曲线是二维空间中扭曲的直线，可以在2D空间中找到两点更短的距离，3D空间同理
学习空间的扭曲性：例如，将立方体展开，寻找最短路径
```



许多降维算法通过对训练实例所在的流形进行建模从而达到降维目的；这叫做流形学习。它依赖于流形猜想（manifold assumption），也被称为流形假设（manifold hypothesis），它认为大多数现实世界的高维数据集大都靠近一个更低维的流形。这种假设经常在实践中被证实。



## 降维的技术和算法

### 主成分分析（PCA）

 选择保持最大方差的轴看起来是合理的，因为它很可能比其他投影损失更少的信息。证明这种选择的另一种方法是，选择这个轴使得将原始数据集投影到该轴上的均方距离最小。这是就 PCA 背后的思想，相当简单。 

 主成分分析（Principal Component Analysis）是目前为止最流行的降维算法。首先它找到接近数据集分布的超平面，然后将所有的数据都投影到这个超平面上。 

下面的 Python 代码使用了 Numpy 提供的`svd()`函数获得训练集的所有主成分，然后提取前两个PC:

```python
X_centered=X-X.mean(axis=0)
U,s,V=np.linalg.svd(X_centered)
c1=V.T[:,0]
c2=V.T[:,1]
# 投影到d维空间
W2=V.T[:,:2]
X2D=X_centered.dot(W2)
```

```python
from sklearn.decomposition import PCA

pca=PCA(n_components=2)
X2D=pca.fit_transform(X)
```



#### 方差解释率（Explained Variance Ratio）



#### 选择正确的维度

通常我们倾向于选择加起来到方差解释率能够达到足够占比（例如 95%）的维度的数量，而不是任意选择要降低到的维度数量。当然，除非您正在为数据可视化而降低维度 -- 在这种情况下，您通常希望将维度降低到 2 或 3。

下面的代码在不降维的情况下进行 PCA，然后计算出保留训练集方差 95% 所需的最小维数：

```python
pca=PCA()
pac.fit(X)
cumsum=np.cumsum(pca.explained_variance_ratio_)
d=np.argmax(cumsum>=0.95)+1

pca=PCA(n_components=0.95)
X_reduced=pca.fit_transform(X)
```

 另一种选择是画出方差解释率关于维数的函数（简单地绘制`cumsum`；参见图 8-8）。曲线中通常会有一个肘部，方差解释率停止快速增长。 



#### PCA 压缩

```python
pca=PCA(n_components=154)
X_mnist_reduced=pca.fit_transform(X_mnist)
X_mnist_recovered=pca.inverse_transform(X_mnist_reduced)
```



#### 增量 PCA（Incremental PCA）

```python
from sklearn.decomposition import IncrementalPCA

n_batches=100
inc_pca=IncrementalPCA(n_components=154)
for X_batch in np.array_spplit(X_mnist,n_batches):
    inc_pca.partial_fit(X_batch)
X_mnist_reduced=inc_pca.transform(X_mnist)
```



#### 随机 PCA（Randomized PCA）



### 核 PCA（Kernel PCA）

### LLE

