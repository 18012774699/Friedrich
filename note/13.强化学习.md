# 强化学习

Reinforcement Learning。

在本章中，我们将首先解释强化学习是什么，以及它擅长于什么；

然后我们将介绍两个在深度强化学习领域最重要的技术：策略梯度和深度 Q 网络（DQN），包括讨论马尔可夫决策过程（MDP）。我们将使用这些技术来训练一个模型来平衡移动车上的杆子。

然后，我会介绍TF-Agents库，这个库利用先进的算法，可以大大简化创建RL系统。

然后我们会用这个系统来玩Breakout，一个著名的 Atari 游戏。

本章最后，会介绍强化学习领域的最新进展。



## 学习优化奖励

在强化学习中，智能体在环境（environment）中观察（observation）并且做出决策（action），随后它会得到奖励（reward）。它的目标是去学习如何行动能最大化**期望奖励**。如果你不在意拟人化的话，可以认为正奖励是愉快，负奖励是痛苦（这样的话奖励一词就有点误导了）。总之，智能体在环境中行动，并且在实验和错误中去学习最大化它的愉快，最小化它的痛苦。

这是一个相当广泛的设置，可以适用于各种各样的任务。以下是几个例子（详见图 16-1）：

1. 智能体可以是控制一个机器人的程序。在此例中，环境就是真实的世界，智能体通过许多的传感器例如摄像机或者触觉传感器来观察，它可以通过给电机发送信号来行动。它可以被编程设置为如果到达了目的地就得到正奖励，如果浪费时间，或者走错方向，或摔倒了就得到负奖励。
2. 智能体可以是控制 Ms.Pac-Man 的程序。在此例中，环境是 Atari 游戏的模拟器，行为是 9 个操纵杆位（上下左右中间等等），观察是屏幕，回报就是游戏点数。
3. 相似地，智能体也可以是棋盘游戏的程序，例如围棋。
4. 智能体也可以不用去控制一个实体（或虚拟的）去移动。例如它可以是一个智能恒温器，当它调整到目标温度以节能时会得到正奖励，当人们需要自己去调节温度时它会得到负奖励，所以智能体必须学会预见人们的需要。
5. 智能体也可以去观测股票市场价格以实时决定买卖。奖励的依据为挣钱或者赔钱。

![img](https:////upload-images.jianshu.io/upload_images/7178691-b19812d128bd7e19.png?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp)

图18-1 强化学习案例：（a）行走机器人，（b）Ms.Pac-Man游戏，（c）围棋玩家，（d）恒温器，（e）自动交易员

其实没有正奖励也是可以的，例如智能体在迷宫内移动，它每分每秒都得到一个负奖励，所以它要尽可能快的找到出口！还有很多适合强化学习的领域，例如自动驾驶汽车，推荐系统，在网页上放广告，或者控制一个图像分类系统让它明白它应该关注于什么。



## 策略搜索**

智能体用于改变行为的算法称为策略。例如，策略可以是一个把观测当输入，行为当做输出的神经网络（见图16-2）。

![img](https:////upload-images.jianshu.io/upload_images/7178691-adfa22f81749a60a.png?imageMogr2/auto-orient/strip|imageView2/2/w/1168/format/webp)

图18-2 用神经网络策略做加强学习

这个策略可以是你能想到的任何算法，它甚至可以是非确定性的。事实上，在某些任务中，策略根本不必观察环境！举个例子，例如，考虑一个真空吸尘器，它的奖励是在 30 分钟内捡起的灰尘数量。它的策略可以是每秒以概率`p`向前移动，或者以概率`1-p`随机地向左或向右旋转。旋转角度将是`-r`和`+r`之间的随机角度，因为该策略涉及一些随机性，所以称为随机策略。机器人将有一个不确定的轨迹，它保证它最终会到达任何可以到达的地方，并捡起所有的灰尘。问题是：30分钟后它会捡起多少灰尘？

### 随机组合

怎么训练这样的机器人？你能调整的策略参数只有两个：概率`p`和角度范围`r`。**一个想法是这些参数尝试许多不同的值，并选择执行最佳的组合**（见图 18-3）。这是一个策略搜索的例子，在这种情况下使用暴力方法。然而，当策略空间太大（通常情况下），以这样的方式找到一组好的参数就像是大海捞针。

![img](https:////upload-images.jianshu.io/upload_images/7178691-7602f060c2e11566.png?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp)

​												图18-3 策略空间中的四个点以及机器人的对应行为

### 遗传算法

另一种搜寻策略空间的方法是**遗传算法**（Genetic Algorithm）。例如你可以随机创造一个包含 100 个策略的第一代基因，随后杀死 80 个糟糕的策略，随后让 20 个幸存策略繁衍 4 代。一个后代只是它父辈基因的复制品加上一些随机变异。幸存的策略加上他们的后代共同构成了第二代。你可以继续以这种方式迭代代，直到找到一个好的策略。

### 策略梯度

另一种方法是使用优化技术，通过评估奖励关于策略参数的梯度，然后通过跟随梯度向更高的奖励（梯度上升）调整这些参数。这种方法被称为**策略梯度**（policy gradient, PG），我们将在本章后面详细讨论。例如，回到真空吸尘器机器人，你可以稍微增加概率P并评估这是否增加了机器人在 30 分钟内拾起的灰尘的量；如果确实增加了，就相对应增加`p`，否则减少`p`。我们将使用 Tensorflow 来实现 PG 算法，但是在这之前我们需要为智能体创造一个生存的环境，所以现在是介绍 OpenAI Gym的时候了。



## OpenAI Gym 介绍

强化学习的一个挑战是，为了训练对象，首先需要有一个工作环境。如果你想设计一个可以学习 Atari 游戏的程序，你需要一个 Atari 游戏模拟器。如果你想设计一个步行机器人，那么环境就是真实的世界，你可以直接在这个环境中训练你的机器人，但是这有其局限性：如果机器人从悬崖上掉下来，你不能仅仅点击“撤消”。你也不能加快时间；增加更多的计算能力不会让机器人移动得更快。一般来说，同时训练 1000 个机器人是非常昂贵的。简而言之，训练在现实世界中是困难和缓慢的，所以你通常需要一个模拟环境，至少需要引导训练。例如，你可以使用PyBullet或MuJoCo来做3D物理模拟。

OpenAI Gym 是一个工具包，它提供各种各样的模拟环境（Atari 游戏，棋盘游戏，2D 和 3D 物理模拟等等），所以你可以训练，比较，或开发新的 RL 算法。

安装之前，如果你是用虚拟环境创建的独立的环境，需要先激活：

```ruby
$ cd $ML_PATH                # 工作目录 (e.g., $HOME/ml)
$ source my_env/bin/activate # Linux or MacOS
$ .\my_env\Scripts\activate  # Windows
```

接下来安装 OpenAI gym。可通过`pip`安装：

```bash
$ python3 -m pip install --upgrade gym
```

取决于系统，你可能还要安装Mesa OpenGL Utility（GLU）库（比如，在Ubuntu 18.04上，你需要运行`apt install libglu1-mesa`）。这个库用来渲染第一个环境。接着，打开一个Python终端或Jupyter notebook，用`make()`创建一个环境：

```bash
>>> import gym
>>> env = gym.make("CartPole-v1")
>>> obs = env.reset()
>>> obs
array([-0.01258566, -0.00156614,  0.04207708, -0.00180545])
```

这里创建了一个 CartPole 环境。这是一个 2D 模拟，其中推车可以被左右加速，以平衡放置在它上面的平衡杆（见图 18-4）。你可以用`gym.envs.registry.all()`获得所有可用的环境。在创建环境之后，需要使用`reset()`初始化。这会返回第一个观察结果。观察取决于环境的类型。对于 CartPole 环境，每个观测是包含四个浮点数的 1D Numpy 向量：这些浮点数代表推车的水平位置（0.0 为中心）、速度（正是右）、杆的角度（0.0 为垂直）及角速度（正为顺时针）。

用`render()`方法展示环境（见图18-4）。在Windows上，这需要安装X Server，比如VcXsrv或Xming：

```bash
>>> env.render()
True
```

![img](https:////upload-images.jianshu.io/upload_images/7178691-cd9359142f22dfff.png?imageMogr2/auto-orient/strip|imageView2/2/w/896/format/webp)

图18-4 CartPole环境

> 提示：如果你在使用无头服务器（即，没有显示器），比如云上的虚拟机，渲染就会失败。解决的唯一方法是使用假X server，比如Xvfb 或 Xdummy。例如，装好Xvfb之后（Ubuntu或Debian上运行`apt install xvfb`），用这条命令启动Python：`xvfb-run -s "-screen 0 1400x900x24" python3`。或者，安装Xvfb和[`pyvirtualdisplay` 库](https://links.jianshu.com/go?to=https%3A%2F%2Fhoml.info%2Fpyvd)（这个库包装了Xvfb），在程序启动处运行`pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()`。

如果你想让`render()`让图像以一个 Numpy 数组格式返回，可以将`mode`参数设置为`rgb_array`（注意，这个环境会渲染环境到屏幕上）：

```python
>>> img = env.render(mode="rgb_array") 
>>> img.shape  # height, width, channels (3=RGB) 
(800, 1200, 3)
```

询问环境，可以采取的可能行动：

```bash
>>> env.action_space
Discrete(2)
```

`Discrete(2)`的意思是可能的行动是整数0和1，表示向左（0）或向右（1）加速。其它的环境可能有其它离散的行动，或其它种类的行动（例如，连续性行动）。因为棍子是向右偏的（`obs[2] > 0`），让车子向右加速：

```python
>>> action = 1  # accelerate right
>>> obs, reward, done, info = env.step(action)
>>> obs
array([-0.01261699,  0.19292789,  0.04204097, -0.28092127])
>>> reward
1.0
>>> done
False
>>> info
{}
```

`step()`方法执行给定的动作并返回四个值：

`obs`:

这是新的观测，小车现在正在向右走（`obs[1]>0`，注：当前速度为正，向右为正）。平衡杆仍然向右倾斜（`obs[2]>0`），但是他的角速度现在为负（`obs[3]<0`），所以它在下一步后可能会向左倾斜。

`reward`：

在这个环境中，无论你做什么，每一步都会得到 1.0 奖励，所以游戏的目标就是尽可能长的运行。

`done`：

当游戏结束时这个值会为`True`。当平衡杆倾斜太多、或越过屏幕、或超过200步时会发生这种情况。之后，必须重新设置环境才能重新使用。

`info`：

该字典可以在其他环境中提供额外信息用于调试或训练。例如，在一些游戏中，可以指示agent还剩多少条命。

> 提示：使用完环境后，应当调用它的`close()`方法释放资源。

让我们硬编码一个简单的策略，当杆向左倾斜时向左边加速，当杆向右倾斜时加速向右边加速。我们使用这个策略来获得超过 500 步的平均回报：

```python
def basic_policy(obs):
    angle = obs[2]
    return 0 if angle < 0 else 1

totals = []
for episode in range(500):
    episode_rewards = 0
    obs = env.reset()
    for step in range(200):
        action = basic_policy(obs)
        obs, reward, done, info = env.step(action)
        episode_rewards += reward
        if done:
            break
    totals.append(episode_rewards)
```

这段代码不难。让我们看看结果：

```python
>>> import numpy as np
>>> np.mean(totals), np.std(totals), np.min(totals), np.max(totals)
(41.718, 8.858356280936096, 24.0, 68.0)
```

**即使有 500 次尝试，这一策略从未使平衡杆在超过 68 个连续的步骤里保持直立。结果太好。**如果你看一下 Juyter Notebook 中的模拟，你会发现，推车越来越强烈地左右摆动，直到平衡杆倾斜过度。让我们看看神经网络是否能提出更好的策略。



## 神经网络策略

让我们创建一个神经网络策略。就像之前我们编码的策略一样，这个神经网络将把观察作为输入，输出要执行的动作。更确切地说，它将估计每个动作的概率，然后我们将**根据估计的概率随机地选择一个动作**（见图 18-5）。在 CartPole 环境中，只有两种可能的动作（左或右），所以我们只需要一个输出神经元。它将输出动作 0（左）的概率`p`，动作 1（右）的概率显然将是`1 - p`。例如，如果它输出 0.7，那么我们将以 70% 的概率选择动作 0，以 30% 的概率选择动作 1。

![img](https:////upload-images.jianshu.io/upload_images/7178691-0d218e745652d303.png?imageMogr2/auto-orient/strip|imageView2/2/w/1097/format/webp)

​																		图18-5 神经网络策略

你可能奇怪为什么我们根据神经网络给出的概率来选择随机的动作，而不是选择最高分数的动作。这种方法使智能体在**探索新的行为**和**利用那些已知可行的行动**之间找到正确的平衡。举个类比：假设你第一次去餐馆，所有的菜看起来同样吸引人，所以你随机挑选一个。如果菜好吃，你可以增加下一次点它的概率，但是你不应该把这个概率提高到 100%，否则你将永远不会尝试其他菜肴，其中一些甚至比你尝试的更好。

还要注意，在这个特定的环境中，过去的动作和观察可以被放心地忽略，因为每个观察都包含环境的完整状态。如果有一些隐藏状态，那么你也需要考虑过去的行为和观察。例如，如果环境仅仅揭示了推车的位置，而不是它的速度，那么你不仅要考虑当前的观测，还要考虑先前的观测，以便估计当前的速度。另一个例子是当观测是有噪声的时候，在这种情况下，通常你想用过去的观察来估计最可能的当前状态。因此，CartPole 问题是简单的；观测是无噪声的，而且它们包含环境的全状态。

下面是用tf.keras创建这个神经网络策略的代码：

```python
import tensorflow as tf
from tensorflow import keras

n_inputs = 4 # == env.observation_space.shape[0]

model = keras.models.Sequential([
    keras.layers.Dense(5, activation="elu", input_shape=[n_inputs]),
    keras.layers.Dense(1, activation="sigmoid"),
])
```

在导入之后，我们使用`Sequential`模型定义策略网络。输入的数量是观测空间的大小（在 CartPole 的情况下是 4 个），我们只有 5 个隐藏单元，并且我们只有 1 个输出概率（向左的概率），所以输出层只需一个使用sigmoid的神经元就成。如果超过两个动作，每个动作就要有一个神经元，然后使用softmax激活函数。

好了，现在我们有一个可以观察和输出动作的神经网络了，那我们怎么训练它呢？



## 评价行为：信用分配问题

如果我们知道每一步的最佳动作，我们可以像通常一样训练神经网络，通过最小化估计概率和目标概率之间的交叉熵。这只是通常的监督学习。然而，在强化学习中，智能体获得的指导的唯一途径是通过奖励，奖励通常是稀疏的和延迟的。例如，如果智能体在 100 个步骤内设法平衡杆，它怎么知道它采取的 100 个行动中的哪一个是好的，哪些是坏的？它所知道的是，在最后一次行动之后，杆子坠落了，但最后一次行动肯定不是负全责的。这被称为信用分配问题：当智能体得到奖励时，很难知道哪些行为应该被信任（或责备）。如果一只狗在表现优秀几小时后才得到奖励，它会明白它做对了什么吗？

为了解决这个问题，一个通常的策略是基于这个动作后得分的总和来评估这个个动作，通常在每个步骤中应用衰减因子`r`。例如（见图 18-6），如果一个智能体决定连续三次向右，在第一步之后得到 +10 奖励，第二步后得到 0，最后在第三步之后得到 -50，然后假设我们使用衰减率`r=0.8`，那么第一个动作将得到`10 +r×0 + r2×(-50)=-22`的分数。如果衰减率接近 0，那么与即时奖励相比，未来的奖励不会有多大意义。相反，如果衰减率接近 1，那么对未来的奖励几乎等于即时回报。典型的衰减率通常从 0.9 到 0.99之间。如果衰减率为 0.95，那么未来 13 步的奖励大约是即时奖励的一半（`0.9513×0.5`），而当衰减率为 0.99，未来 69 步的奖励是即时奖励的一半。在 CartPole 环境下，行为具有相当短期的影响，因此选择 0.95 的折扣率是合理的。
$$
reward_{future}=reward_0+r*reward_1+r^2*reward_2
$$
![img](https:////upload-images.jianshu.io/upload_images/7178691-81c0920496022314.png?imageMogr2/auto-orient/strip|imageView2/2/w/865/format/webp)

​														图18-6 计算行动的回报：未来衰减求和

当然，一个好的动作可能会紧跟着一串坏动作，这些动作会导致平衡杆迅速下降，从而导致一个好的动作得到一个低分数（类似的，一个好演员有时会在一部烂片中扮演主角）。然而，如果我们花足够多的时间来训练游戏，平均下来好的行为会得到比坏的更好的分数。因此，为了获得相当可靠的动作分数，我们必须运行很多次并将所有动作分数归一化（通过减去平均值并除以标准偏差）。之后，我们可以合理地假设消极得分的行为是坏的，而积极得分的行为是好的。现在我们有一个方法来评估每一个动作，我们已经准备好使用策略梯度来训练我们的第一个智能体。让我们看看如何做。



## 策略梯度

正如前面所讨论的，PG 算法通过遵循更高回报的梯度来优化策略参数。一种流行的 PG 算法，称为**增强算法**，在 1929 由 Ronald Williams 提出。这是一个常见的变体：

1. 首先，让神经网络策略玩几次游戏，并在每一步计算梯度，这使得智能体更可能选择行为，但不应用这些梯度。
2. 运行几次后，计算每个动作的得分（使用前面段落中描述的方法）。
3. 如果一个动作的分数是正的，这意味着动作是好的，可应用较早计算的梯度，以便将来有更大的的概率选择这个动作。但是，如果分数是负的，这意味着动作是坏的，要应用相反梯度来使得这个动作在将来采取的可能性更低。我们的方法就是简单地将每个梯度向量乘以相应的动作得分。
4. 最后，计算所有得到的梯度向量的平均值，并使用它来执行梯度下降步骤。

让我们使用 tf.keras 实现这个算法。我们将训练我们早先建立的神经网络策略，让它学会平衡车上的平衡杆。首先，需要一个能执行一步的函数。假定做出的动作都是对的，激素亲戚损失和梯度（梯度会保存一会，根据动作的结果再对其修改）：

```bash
def play_one_step(env, obs, model, loss_fn):
    with tf.GradientTape() as tape:
        left_proba = model(obs[np.newaxis])
        action = (tf.random.uniform([1, 1]) > left_proba)
        y_target = tf.constant([[1.]]) - tf.cast(action, tf.float32)
        loss = tf.reduce_mean(loss_fn(y_target, left_proba))
    grads = tape.gradient(loss, model.trainable_variables)
    obs, reward, done, info = env.step(int(action[0, 0].numpy()))
    return obs, reward, done, grads
```

逐行看代码：

- 在`GradientTape`代码块内，先调用模型，传入一个观察（将观察变形为包含单个实例的批次）。输出是向左的概率。
- 然后，选取一个0到1之间的浮点数，检查是否大于`left_proba`。概率为`left_proba`时，`action`是`False`；概率为`1-left_proba`时，`action`是`True`。当将这个布尔值转变为数字时，动作是0（左）或1（右）及对应的概率。
- 接着，定义向左的目标概率：1减去动作（浮点值）。如果动作是0（左），则向左的目标概率等于1。如果动作是1（右），则目标概率等于0。
- 然后使用损失函数计算损失，使用记录器计算模型**可训练变量的损失梯度**。这些梯度会在后面应用前，根据动作的结果做微调。
- 最后，执行选择的动作，无论是否结束，返回新的观察、奖励，和刚刚计算的梯度。

现在，创建另一个函数基于`play_one_step()`的多次执行函数，返回所有奖励和每个周期和步骤的梯度：

```python
def play_multiple_episodes(env, n_episodes, n_max_steps, model, loss_fn):
    all_rewards = []
    all_grads = []
    for episode in range(n_episodes):
        current_rewards = []
        current_grads = []
        obs = env.reset()
        for step in range(n_max_steps):
            obs, reward, done, grads = play_one_step(env, obs, model, loss_fn)
            current_rewards.append(reward)
            current_grads.append(grads)
            if done:
                break
        all_rewards.append(current_rewards)
        all_grads.append(current_grads)
    return all_rewards, all_grads
```

这段代码返回了奖励列表（每个周期一个奖励列表，每个步骤一个奖励），还有一个梯度列表（每个周期一个梯度列表，每个步骤一个梯度元组，每个元组每个变脸有一个梯度张量）。

算法会使用`play_multiple_episodes()`函数，多次执行游戏（比如，10次），然后会检查所有奖励，做衰减，然后归一化。要这么做，需要多个函数：第一个计算每个步骤的**未来衰减奖励的和**，第二个**归一化所有这些衰减奖励**（减去平均值，除以标准差）：

```bash
def discount_rewards(rewards, discount_factor):
    discounted = np.array(rewards)
    for step in range(len(rewards) - 2, -1, -1):
        discounted[step] += discounted[step + 1] * discount_factor
    return discounted

def discount_and_normalize_rewards(all_rewards, discount_factor):
    all_discounted_rewards = [discount_rewards(rewards, discount_factor)
                              for rewards in all_rewards]
    flat_rewards = np.concatenate(all_discounted_rewards)
    reward_mean = flat_rewards.mean()
    reward_std = flat_rewards.std()
    return [(discounted_rewards - reward_mean) / reward_std
            for discounted_rewards in all_discounted_rewards]
```

检测其是否有效：

```bash
>>> discount_rewards([10, 0, -50], discount_factor=0.8)
array([-22, -40, -50])
>>> discount_and_normalize_rewards([[10, 0, -50], [10, 20]],
...                                discount_factor=0.8)
...
[array([-0.28435071, -0.86597718, -1.18910299]),
 array([1.26665318, 1.0727777 ])]
```

调用`discount_rewards()`，返回了我们想要的结果（见图18-6）。可以确认函数`discount_and_normalize_rewards()`返回了每个周期每个步骤的归一化的行动的结果。可以看到，第一个周期的表现比第二个周期的表现糟糕，所以归一化的结果都是负的；第一个周期中的动作都是不好的，而第二个周期中的动作被认为是好的。

可以准备运行算法了！现在定义超参数。运行150个训练迭代，每次迭代完成10次周期，每个周期最多200个步骤。衰减因子是0.95：

```bash
n_iterations = 150
n_episodes_per_update = 10
n_max_steps = 200
discount_factor = 0.95
```

还需要一个优化器和损失函数。优化器用普通的Adam就成，学习率用0.01，因为是二元分类器，使用二元交叉熵损失函数：

```bash
optimizer = keras.optimizers.Adam(lr=0.01)
loss_fn = keras.losses.binary_crossentropy
```

接下来创建和运行训练循环。

```python
for iteration in range(n_iterations):
    all_rewards, all_grads = play_multiple_episodes(
        env, n_episodes_per_update, n_max_steps, model, loss_fn)
    all_final_rewards = discount_and_normalize_rewards(all_rewards,
                                                       discount_factor)
    all_mean_grads = []
    for var_index in range(len(model.trainable_variables)):
        mean_grads = tf.reduce_mean(
            [final_reward * all_grads[episode_index][step][var_index]
             for episode_index, final_rewards in enumerate(all_final_rewards)
                 for step, final_reward in enumerate(final_rewards)], axis=0)
        all_mean_grads.append(mean_grads)
    optimizer.apply_gradients(zip(all_mean_grads, model.trainable_variables))
```

逐行看下代码：

- 在每次训练迭代，循环调用`play_multiple_episodes()`，这个函数玩10次游戏，返回每个周期和步骤的奖励和梯度。
- 然后调用`discount_and_normalize_rewards()`计算每个动作的归一化结果（代码中是`final_reward`）。这样可以测量每个动作的好坏结果。
- 接着，循环每个可训练变量，计算每个变量的梯度加权平均，权重是`final_reward`。
- 最后，将这些平均梯度应用于优化器：微调模型的变量。

就是这样。这段代码可以训练神经网络策略，模型可以学习保持棍子的平衡（可以尝试notebook中的“策略梯度”部分）。每个周期的平均奖励会非常接近200（200是环境默认的最大值）。成功！

> 提示：研究人员试图找到一种即使当智能体最初对环境一无所知时也能很好工作的算法。然而，除非你正在写论文，否则你应该尽可能多地将先前的知识注入到智能体中，因为它会极大地加速训练。例如，因为知道棍子要尽量垂直，你可以添加与棍子角度成正比的负奖励。这可以让奖励不那么分散，是训练加速。此外，如果你已经有一个相当好的策略，你可以训练神经网络模仿它，然后使用策略梯度来改进它。

尽管它相对简单，但是该算法是非常强大的。你可以用它来解决更难的问题，而不仅仅是平衡一辆手推车上的平衡杆。事实上，因为样本不足，必须多次玩游戏，才能取得更大进展。但这个算法是更强大算法的基础，比如Actor-Critic算法（后面会介绍）。

现在我们来看看另一个流行的算法族。与 PG 算法直接尝试优化策略以增加奖励相反，我们现在看的算法不那么直接：智能体学习去估计每个状态的未来衰减奖励的期望总和，或者在每个状态中的每个行为未来衰减奖励的期望和。然后，使用这些知识来决定如何行动。为了理解这些算法，我们必须首先介绍马尔可夫决策过程（MDP）。



