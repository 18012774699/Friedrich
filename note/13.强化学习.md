# 强化学习

Reinforcement Learning。

在本章中，我们将首先解释强化学习是什么，以及它擅长于什么；

然后我们将介绍两个在深度强化学习领域最重要的技术：策略梯度和深度 Q 网络（DQN），包括讨论马尔可夫决策过程（MDP）。我们将使用这些技术来训练一个模型来平衡移动车上的杆子。

然后，我会介绍TF-Agents库，这个库利用先进的算法，可以大大简化创建RL系统。

然后我们会用这个系统来玩Breakout，一个著名的 Atari 游戏。

本章最后，会介绍强化学习领域的最新进展。



## 学习优化奖励

在强化学习中，智能体在环境（environment）中观察（observation）并且做出决策（action），随后它会得到奖励（reward）。它的目标是去学习如何行动能最大化**期望奖励**。如果你不在意拟人化的话，可以认为正奖励是愉快，负奖励是痛苦（这样的话奖励一词就有点误导了）。总之，智能体在环境中行动，并且在实验和错误中去学习最大化它的愉快，最小化它的痛苦。

这是一个相当广泛的设置，可以适用于各种各样的任务。以下是几个例子（详见图 16-1）：

1. 智能体可以是控制一个机器人的程序。在此例中，环境就是真实的世界，智能体通过许多的传感器例如摄像机或者触觉传感器来观察，它可以通过给电机发送信号来行动。它可以被编程设置为如果到达了目的地就得到正奖励，如果浪费时间，或者走错方向，或摔倒了就得到负奖励。
2. 智能体可以是控制 Ms.Pac-Man 的程序。在此例中，环境是 Atari 游戏的模拟器，行为是 9 个操纵杆位（上下左右中间等等），观察是屏幕，回报就是游戏点数。
3. 相似地，智能体也可以是棋盘游戏的程序，例如围棋。
4. 智能体也可以不用去控制一个实体（或虚拟的）去移动。例如它可以是一个智能恒温器，当它调整到目标温度以节能时会得到正奖励，当人们需要自己去调节温度时它会得到负奖励，所以智能体必须学会预见人们的需要。
5. 智能体也可以去观测股票市场价格以实时决定买卖。奖励的依据为挣钱或者赔钱。

![img](https:////upload-images.jianshu.io/upload_images/7178691-b19812d128bd7e19.png?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp)

图18-1 强化学习案例：（a）行走机器人，（b）Ms.Pac-Man游戏，（c）围棋玩家，（d）恒温器，（e）自动交易员

其实没有正奖励也是可以的，例如智能体在迷宫内移动，它每分每秒都得到一个负奖励，所以它要尽可能快的找到出口！还有很多适合强化学习的领域，例如自动驾驶汽车，推荐系统，在网页上放广告，或者控制一个图像分类系统让它明白它应该关注于什么。



## 策略搜索

智能体用于改变行为的算法称为策略。例如，策略可以是一个把观测当输入，行为当做输出的神经网络（见图16-2）。

![img](https:////upload-images.jianshu.io/upload_images/7178691-adfa22f81749a60a.png?imageMogr2/auto-orient/strip|imageView2/2/w/1168/format/webp)

图18-2 用神经网络策略做加强学习

这个策略可以是你能想到的任何算法，它甚至可以是非确定性的。事实上，在某些任务中，策略根本不必观察环境！举个例子，例如，考虑一个真空吸尘器，它的奖励是在 30 分钟内捡起的灰尘数量。它的策略可以是每秒以概率`p`向前移动，或者以概率`1-p`随机地向左或向右旋转。旋转角度将是`-r`和`+r`之间的随机角度，因为该策略涉及一些随机性，所以称为随机策略。机器人将有一个不确定的轨迹，它保证它最终会到达任何可以到达的地方，并捡起所有的灰尘。问题是：30分钟后它会捡起多少灰尘？

怎么训练这样的机器人？你能调整的策略参数只有两个：概率`p`和角度范围`r`。一个想法是这些参数尝试许多不同的值，并选择执行最佳的组合（见图 18-3）。这是一个策略搜索的例子，在这种情况下使用暴力方法。然而，当策略空间太大（通常情况下），以这样的方式找到一组好的参数就像是大海捞针。

![img](https:////upload-images.jianshu.io/upload_images/7178691-7602f060c2e11566.png?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp)

图18-3 策略空间中的四个点以及机器人的对应行为

另一种搜寻策略空间的方法是遗传算法。例如你可以随机创造一个包含 100 个策略的第一代基因，随后杀死 80 个糟糕的策略，随后让 20 个幸存策略繁衍 4 代。一个后代只是它父辈基因的复制品加上一些随机变异。幸存的策略加上他们的后代共同构成了第二代。你可以继续以这种方式迭代代，直到找到一个好的策略。

另一种方法是使用优化技术，通过评估奖励关于策略参数的梯度，然后通过跟随梯度向更高的奖励（梯度上升）调整这些参数。这种方法被称为策略梯度（policy gradient, PG），我们将在本章后面详细讨论。例如，回到真空吸尘器机器人，你可以稍微增加概率P并评估这是否增加了机器人在 30 分钟内拾起的灰尘的量；如果确实增加了，就相对应增加`p`，否则减少`p`。我们将使用 Tensorflow 来实现 PG 算法，但是在这之前我们需要为智能体创造一个生存的环境，所以现在是介绍 OpenAI Gym的时候了。



## OpenAI Gym 介绍

强化学习的一个挑战是，为了训练对象，首先需要有一个工作环境。如果你想设计一个可以学习 Atari 游戏的程序，你需要一个 Atari 游戏模拟器。如果你想设计一个步行机器人，那么环境就是真实的世界，你可以直接在这个环境中训练你的机器人，但是这有其局限性：如果机器人从悬崖上掉下来，你不能仅仅点击“撤消”。你也不能加快时间；增加更多的计算能力不会让机器人移动得更快。一般来说，同时训练 1000 个机器人是非常昂贵的。简而言之，训练在现实世界中是困难和缓慢的，所以你通常需要一个模拟环境，至少需要引导训练。例如，你可以使用PyBullet或MuJoCo来做3D物理模拟。

OpenAI Gym 是一个工具包，它提供各种各样的模拟环境（Atari 游戏，棋盘游戏，2D 和 3D 物理模拟等等），所以你可以训练，比较，或开发新的 RL 算法。

安装之前，如果你是用虚拟环境创建的独立的环境，需要先激活：

```ruby
$ cd $ML_PATH                # 工作目录 (e.g., $HOME/ml)
$ source my_env/bin/activate # Linux or MacOS
$ .\my_env\Scripts\activate  # Windows
```

接下来安装 OpenAI gym。可通过`pip`安装：

```bash
$ python3 -m pip install --upgrade gym
```

取决于系统，你可能还要安装Mesa OpenGL Utility（GLU）库（比如，在Ubuntu 18.04上，你需要运行`apt install libglu1-mesa`）。这个库用来渲染第一个环境。接着，打开一个Python终端或Jupyter notebook，用`make()`创建一个环境：

```py
>>> import gym
>>> env = gym.make("CartPole-v1")
>>> obs = env.reset()
>>> obs
array([-0.01258566, -0.00156614,  0.04207708, -0.00180545])
```

这里创建了一个 CartPole 环境。这是一个 2D 模拟，其中推车可以被左右加速，以平衡放置在它上面的平衡杆（见图 18-4）。你可以用`gym.envs.registry.all()`获得所有可用的环境。在创建环境之后，需要使用`reset()`初始化。这会返回第一个观察结果。观察取决于环境的类型。对于 CartPole 环境，每个观测是包含四个浮点数的 1D Numpy 向量：这些浮点数代表推车的水平位置（0.0 为中心）、速度（正是右）、杆的角度（0.0 为垂直）及角速度（正为顺时针）。

用`render()`方法展示环境（见图18-4）。在Windows上，这需要安装X Server，比如VcXsrv或Xming：

```py
>>> env.render()
True
```

![img](https:////upload-images.jianshu.io/upload_images/7178691-cd9359142f22dfff.png?imageMogr2/auto-orient/strip|imageView2/2/w/896/format/webp)

图18-4 CartPole环境

> 提示：如果你在使用无头服务器（即，没有显示器），比如云上的虚拟机，渲染就会失败。解决的唯一方法是使用假X server，比如Xvfb 或 Xdummy。例如，装好Xvfb之后（Ubuntu或Debian上运行`apt install xvfb`），用这条命令启动Python：`xvfb-run -s "-screen 0 1400x900x24" python3`。或者，安装Xvfb和[`pyvirtualdisplay` 库](https://links.jianshu.com/go?to=https%3A%2F%2Fhoml.info%2Fpyvd)（这个库包装了Xvfb），在程序启动处运行`pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()`。

如果你想让`render()`让图像以一个 Numpy 数组格式返回，可以将`mode`参数设置为`rgb_array`（注意，这个环境会渲染环境到屏幕上）：

```python
>>> img = env.render(mode="rgb_array") 
>>> img.shape  # height, width, channels (3=RGB) 
(800, 1200, 3)
```

询问环境，可以采取的可能行动：

```py
>>> env.action_space
Discrete(2)
```

`Discrete(2)`的意思是可能的行动是整数0和1，表示向左（0）或向右（1）加速。其它的环境可能有其它离散的行动，或其它种类的行动（例如，连续性行动）。因为棍子是向右偏的（`obs[2] > 0`），让车子向右加速：

```python
>>> action = 1  # accelerate right
>>> obs, reward, done, info = env.step(action)
>>> obs
array([-0.01261699,  0.19292789,  0.04204097, -0.28092127])
>>> reward
1.0
>>> done
False
>>> info
{}
```

`step()`方法执行给定的动作并返回四个值：

`obs`:

这是新的观测，小车现在正在向右走（`obs[1]>0`，注：当前速度为正，向右为正）。平衡杆仍然向右倾斜（`obs[2]>0`），但是他的角速度现在为负（`obs[3]<0`），所以它在下一步后可能会向左倾斜。

`reward`：

在这个环境中，无论你做什么，每一步都会得到 1.0 奖励，所以游戏的目标就是尽可能长的运行。

`done`：

当游戏结束时这个值会为`True`。当平衡杆倾斜太多、或越过屏幕、或超过200步时会发生这种情况。之后，必须重新设置环境才能重新使用。

`info`：

该字典可以在其他环境中提供额外信息用于调试或训练。例如，在一些游戏中，可以指示agent还剩多少条命。

> 提示：使用完环境后，应当调用它的`close()`方法释放资源。

让我们硬编码一个简单的策略，当杆向左倾斜时向左边加速，当杆向右倾斜时加速向右边加速。我们使用这个策略来获得超过 500 步的平均回报：

```python
def basic_policy(obs):
    angle = obs[2]
    return 0 if angle < 0 else 1

totals = []
for episode in range(500):
    episode_rewards = 0
    obs = env.reset()
    for step in range(200):
        action = basic_policy(obs)
        obs, reward, done, info = env.step(action)
        episode_rewards += reward
        if done:
            break
    totals.append(episode_rewards)
```

这段代码不难。让我们看看结果：

```python
>>> import numpy as np
>>> np.mean(totals), np.std(totals), np.min(totals), np.max(totals)
(41.718, 8.858356280936096, 24.0, 68.0)
```

即使有 500 次尝试，这一策略从未使平衡杆在超过 68 个连续的步骤里保持直立。结果太好。如果你看一下 Juyter Notebook 中的模拟，你会发现，推车越来越强烈地左右摆动，直到平衡杆倾斜过度。让我们看看神经网络是否能提出更好的策略。



## 神经网络策略

让我们创建一个神经网络策略。就像之前我们编码的策略一样，这个神经网络将把观察作为输入，输出要执行的动作。更确切地说，它将估计每个动作的概率，然后我们将根据估计的概率随机地选择一个动作（见图 18-5）。在 CartPole 环境中，只有两种可能的动作（左或右），所以我们只需要一个输出神经元。它将输出动作 0（左）的概率`p`，动作 1（右）的概率显然将是`1 - p`。例如，如果它输出 0.7，那么我们将以 70% 的概率选择动作 0，以 30% 的概率选择动作 1。

![img](https:////upload-images.jianshu.io/upload_images/7178691-0d218e745652d303.png?imageMogr2/auto-orient/strip|imageView2/2/w/1097/format/webp)

图18-5 神经网络策略

你可能奇怪为什么我们根据神经网络给出的概率来选择随机的动作，而不是选择最高分数的动作。这种方法使智能体在**探索新的行为**和**利用那些已知可行的行动**之间找到正确的平衡。举个类比：假设你第一次去餐馆，所有的菜看起来同样吸引人，所以你随机挑选一个。如果菜好吃，你可以增加下一次点它的概率，但是你不应该把这个概率提高到 100%，否则你将永远不会尝试其他菜肴，其中一些甚至比你尝试的更好。

还要注意，在这个特定的环境中，过去的动作和观察可以被放心地忽略，因为每个观察都包含环境的完整状态。如果有一些隐藏状态，那么你也需要考虑过去的行为和观察。例如，如果环境仅仅揭示了推车的位置，而不是它的速度，那么你不仅要考虑当前的观测，还要考虑先前的观测，以便估计当前的速度。另一个例子是当观测是有噪声的的，在这种情况下，通常你想用过去的观察来估计最可能的当前状态。因此，CartPole 问题是简单的；观测是无噪声的，而且它们包含环境的全状态。

下面是用tf.keras创建这个神经网络策略的代码：

```python
import tensorflow as tf
from tensorflow import keras

n_inputs = 4 # == env.observation_space.shape[0]

model = keras.models.Sequential([
    keras.layers.Dense(5, activation="elu", input_shape=[n_inputs]),
    keras.layers.Dense(1, activation="sigmoid"),
])
```

在导入之后，我们使用`Sequential`模型定义策略网络。输入的数量是观测空间的大小（在 CartPole 的情况下是 4 个），我们只有 5 个隐藏单元，并且我们只有 1 个输出概率（向左的概率），所以输出层只需一个使用sigmoid的神经元就成。如果超过两个动作，每个动作就要有一个神经元，然后使用softmax激活函数。

好了，现在我们有一个可以观察和输出动作的神经网络了，那我们怎么训练它呢？



