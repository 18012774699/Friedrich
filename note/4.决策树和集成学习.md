## 决策树

 决策树对特征缩放不敏感 



### 决策树的训练和可视化

 你可以通过使用`export_graphviz()`方法，通过生成一个叫做`iris_tree.dot`的图形定义文件将一个训练好的决策树模型可视化。 

```ruby
dot -Tpng iris_tree.dot -o iris_tree.png
```



### 开始预测

 决策树的众多特性之一就是， 它不需要太多的数据预处理， 尤其是不需要进行特征的缩放或者归一化。 



### CART 训练算法

### Gini不纯度或信息熵

### 正则化超参数

### 不稳定性



## 集成学习

 特别著名的集成方法，包括 *bagging, boosting, stacking*，和其他一些算法。我们也会讨论随机森林。 



### 大数定律

### 硬投票和软投票

### 集成方法

- 使用不同的训练算法去得到一些不同的分类器。
- 每一个分类器都使用相同的训练算法，但是在不同的训练集上去训练它们。 (Bagging 和 Pasting)
- Boosting, *Adaboost* 适应性提升和 *Gradient Boosting*（梯度提升） 
- Stacking（ *stacked generalization* ），思想类似神经网络

### Out-of-Bag 评价

对于 Bagging 来说，一些实例可能被一些分类器重复采样，但其他的有可能不会被采样。`BaggingClassifier`默认采样。`BaggingClassifier`默认是有放回的采样`m`个实例 （`bootstrap=True`），其中`m`是训练集的大小，这意味着平均下来只有63%的训练实例被每个分类器采样，剩下的37%个没有被采样的训练实例就叫做 *Out-of-Bag* 实例。注意对于每一个的分类器它们的 37% 不是相同的。

因为在训练中分类器从来没有看到过 oob 实例，所以它可以在这些实例上进行评估，而不需要单独的验证集或交叉验证。你可以拿出每一个分类器的 oob 来评估集成本身。



### 特征采样

```
BaggingClassifier也支持采样特征。它被两个超参数max_features和bootstrap_features控制。他们的工作方式和max_samples和bootstrap一样，但这是对于特征采样而不是实例采样。因此，每一个分类器都会被在随机的输入特征内进行训练。

当你在处理高维度输入下（例如图片）此方法尤其有效。对训练实例和特征的采样被叫做随机贴片。保留了所有的训练实例（例如bootstrap=False和max_samples=1.0），但是对特征采样（bootstrap_features=True并且/或者max_features小于 1.0）叫做随机子空间。

采样特征导致更多的预测多样性，用高偏差换低方差。
```



## 随机森林

随机森林算法在树生长时引入了额外的随机；与在节点分裂时需要找到最好分裂特征相反（详见第六章），它在一个随机的特征集中找最好的特征。它导致了树的差异性，并且再一次用高偏差换低方差，总的来说是一个更好的模型。



### 极端随机树

 ExtraTreesClassifier/*ExtraTreesRegressor*  



### 特征重要度

如果你观察一个单一决策树，重要的特征会出现在更靠近根部的位置，而不重要的特征会经常出现在靠近叶子的位置。因此我们可以通过计算一个特征在森林的全部树中出现的平均深度来预测特征的重要性。sklearn 在训练后会自动计算每个特征的重要度。你可以通过`feature_importances_`变量来查看结果。

随机森林可以非常方便快速得了解哪些特征实际上是重要的，特别是你需要进行特征选择的时候。 



## 提升

### Adaboost

适应性提升，是 *Adaptive Boosting* 的简称 （ AdaBoostClassifier / AdaBoostRegressor ）

-  使一个新的分类器去修正之前分类结果的方法就是对之前分类结果不对的训练**实例**多加关注。这导致新的预测因子越来越多地聚焦于这种情况。这是 *Adaboost* 使用的技术。 


-  序列学习技术的一个重要的缺点就是：它不能被并行化（只能按步骤），因为每个分类器只能在之前的分类器已经被训练和评价后再进行训练。因此，它不像Bagging和Pasting一样。 

- **分类器的权重 α_{j}** 随后用公式 7-2 计算出来。其中`η`是超参数学习率（默认为 1）。分类器准确率越高，它的权重就越高。如果它只是瞎猜，那么它的权重会趋近于 0。然而，如果它总是出错（比瞎猜的几率都低），它的权重会变为负数。
  $$
  α_j=ηlog\frac{1-r_j}{r_j}
  $$
- 如果你的 Adaboost 集成**过拟合**了训练集，你可以尝试减少基分类器的数量或者对基分类器使用更强的正则化。 



### 梯度提升

- 另一个非常著名的提升算法是梯度提升。与 Adaboost 一样，梯度提升也是通过向集成中逐步增加分类器运行的，每一个分类器都修正之前的分类结果。
- 然而，它并不像 Adaboost 那样每一次迭代都更改实例的权重，这个方法是去使用新的分类器去拟合前面分类器预测的*残差* 。
- 超参数`learning_rate` 确立了每个树的贡献。如果你把它设置为很小，例如 0.1，在集成中就需要更多的树去拟合训练集，但预测通常会更好。这个正则化技术叫做 *shrinkage*。 

![img](file:///C:/Users/d84138318/AppData/Roaming/eSpace_Desktop/UserData/d84138318/imagefiles/E005B442-24DB-4444-9F63-6DBE2B17E471.png)

- 为了找到树的最优数量，你可以使用**早停技术**（第四章讨论过）。最简单使用这个技术的方法就是使用`staged_predict()`：它在训练的每个阶段（用一棵树，两棵树等）返回一个迭代器。加下来的代码用 120 个树训练了一个 GBRT 集成，然后在训练的每个阶段验证错误以找到树的最佳数量，最后使用 GBRT 树的最优数量训练另一个集成：

```python
import numpy as np 
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

X_train, X_val, y_train, y_val = train_test_split(X, y)
gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120) 
gbrt.fit(X_train, y_train)
errors = [mean_squared_error(y_val, y_pred) for y_pred in brt.staged_predict(X_val)] 
bst_n_estimators = np.argmin(errors)
gbrt_best = GradientBoostingRegressor(max_depth=2,n_estimators=bst_n_estimators) 
gbrt_best.fit(X_train, y_train) 
```

- 你也可以早早的停止训练来实现早停（而不是先在一大堆树中训练，然后再回头去找最佳数量）。你可以通过设置`warm_start=True`来实现 ，这使得当`fit()`方法被调用时 sklearn 保留现有树，并允许增量训练。接下来的代码在当一行中的五次迭代验证错误没有改善时会停止训练：

```python
gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True)
min_val_error = float("inf") 
error_going_up = 0 
for n_estimators in range(1, 120):    
    gbrt.n_estimators = n_estimators    
gbrt.fit(X_train, y_train)    
    y_pred = gbrt.predict(X_val)    
    val_error = mean_squared_error(y_val, y_pred)    
    if val_error < min_val_error:        
        min_val_error = val_error        
        error_going_up = 0    
    else:        
        error_going_up += 1        
        if error_going_up == 5:            
            break  # early stopping 
```

- `GradientBoostingRegressor`也支持指定用于训练每棵树的训练实例比例的超参数`subsample`。例如如果`subsample=0.25`，那么每个树都会在 25% 随机选择的训练实例上训练。你现在也能猜出来，这也是个**高偏差换低方差**的作用。它同样也加速了训练。这个技术叫做***随机梯度提升***。

也可能对其他损失函数使用梯度提升。这是由损失超参数控制（见 sklearn 文档）。

