## 人工神经元

### 感知器（Perceptron）

感知器是最简单的人工神经网络结构之一，由 Frank Rosenblatt 发明于 1957。它是基于一种稍微不同的人工神经元（见图 10-4），称为线性阈值单元（LTU）。

#### 线性阈值单元（LTU）

- 输入
- 权重加和
- 阶跃函数
- 输出

最常见的在感知器中使用的阶跃函数是 Heaviside 阶跃函数（见方程 10-1）。
$$
heaviside(z)=
\begin{cases}
0& \text{z<0}\\
1& \text{z>=0}
\end{cases}
$$
有时使用符号函数代替。
$$
sgn(z)=
\begin{cases}
-1& \text{z<0}\\
0& \text{z=0}\\
+1& \text{z>=0}
\end{cases}
$$
感知器简单地由一层 LTU 组成，每个神经元连接到所有输入。这些连接通常用特殊的被称为输入神经元的传递神经元来表示：它们只输出它们所输入的任何输入。此外，通常添加额外偏置特征（X0=1）。这种偏置特性通常用一种称为偏置神经元的特殊类型的神经元来表示，它总是输出 1。

#### 单层LTU输出

借助线性代数，利用公式10-2可以方便地同时算出几个输入的一层神经网络的输出。
$$
h_{W,b}(X)=φ(XW+b)
$$
在这个公式中：

- `X`表示输入特征矩阵，每行是一个实例，每列是一个特征；
- 权重矩阵`W`包含所有的连接权重，除了偏置神经元。每有一个输入神经元权重矩阵就有一行，神经层每有一个神经元权重矩阵就有一列；
- 偏置矢量`b`含有所有偏置神经元和人工神经元的连接权重。每有一个人工神经元就对应一个偏置项；
- 函数φ被称为**激活函数**，当人工神经网络是TLU时，激活函数是阶跃函数（后面会讨论更多的激活函数）。

####  感知器的训练方式 

> 那么感知器是如何训练的呢？Frank Rosenblatt 提出的感知器训练算法在很大程度上受到 Hebb 规则的启发。在 1949 出版的《行为组织》一书中，Donald Hebb 提出，当一个生物神经元经常触发另一个神经元时，这两个神经元之间的联系就会变得更强。这个想法后来被 Siegrid Löwel 总结为一经典短语：“一起燃烧的细胞，汇合在一起。”这个规则后来被称为 Hebb 规则（或 Hebbian learning）。使用这个规则的变体来训练感知器，该规则考虑了网络所犯的误差。更具体地，感知器一次被馈送一个训练实例，对于每个实例，它进行预测。对于每一个产生错误预测的输出神经元，修正输入的连接权重，以获得正确的预测。公式 10-3 展示了Hebb 规则。

感知器学习算法类似于随机梯度下降。
$$
w_{i,j}^{next step}=w_{i,j}+η(y_{j}^{hat}-y_j)x_i
$$

- 其中`Wi,j`是第`i`输入神经元与第`j`个输出神经元之间的连接权重。
- `xi`是当前训练实例的第`i`个输入值。
- `hat yj`是当前训练实例的第`j`个输出神经元的输出。
- `yj`是当前训练实例的第`j`个输出神经元的目标输出。
- `η`是学习率。

事实上，sklearn 的感知器类相当于使用具有以下超参数的 SGD 分类器：`loss="perceptron"`，`learning_rate="constant"`，`eta0=1`（学习率），`penalty=None`（无正则化）。

 注意，与逻辑回归分类器相反，感知器不输出类概率，而是基于硬阈值进行预测。 

  感知器不能解决一些琐碎的问题（例如，异或（XOR）分类问题）.



####  多层感知器（MLP） 

 感知器的一些局限性可以通过堆叠多个感知器来消除。由此产生的人工神经网络被称为多层感知器（MLP）。特别地，MLP 可以解决 XOR 问题 。



#### 深度神经网络（DNN）

MLP 由一个（通过）输入层、一个或多个称为隐藏层的 LTU 组成，一个最终层 LTU 称为输出层（见图 10-7）。除了输出层之外的每一层包括偏置神经元，并且全连接到下一层。当人工神经网络有两个或多个隐含层时，称为深度神经网络（DNN）。



#### 反向传播算法

第 9 章我们将其描述为使用反向自动微分的梯度下降。

> 笔记：自动计算梯度被称为自动微分。有多种自动微分的方法，各有优缺点。反向传播使用的是反向模式自微分。这种方法快而准，当函数有多个变量（连接权重）和多个输出（损失函数）要微分时也能应对。附录D介绍了自微分。

对BP做详细分解：

- 每次处理一个微批次（假如每个批次包含32个实例），用训练集多次训练BP，每次被称为一个周期（epoch）；
- 每个微批次先进入输入层，输入层再将其发到第一个隐藏层。计算得到该层所有神经元的（微批次的每个实例的）输出。输出接着传到下一层，直到得到输出层的输出。这个过程就是前向传播：就像做预测一样，只是保存了每个中间结果，中间结果要用于反向传播；
- 然后计算输出误差（使用损失函数比较目标值和实际输出值，然后返回误差）；
- 接着，计算每个输出连接对误差的贡献量。这是通过**链式法则**（就是对多个变量做微分的方法）实现的；
- 然后还是使用链式法则，计算最后一个隐藏层的每个连接对误差的贡献，这个过程不断向后传播，直到到达输入层。
- 最后，BP算法做一次梯度下降步骤，用刚刚计算的误差梯度调整所有连接权重。

BP算法十分重要，再归纳一下：对每个训练实例，BP算法先做一次预测（前向传播），然后计算误差，然后反向通过每一层以测量误差贡献量（反向传播），最后调整所有连接权重以降低误差（梯度下降）。（译者注：我也总结下吧，每次训练都先是要设置周期epoch数，每次epoch其实做的就是三件事，向前传一次，向后传一次，然后调整参数，接着再进行下一次epoch。）

> 警告：随机初始化隐藏层的连接权重是很重要的。假如所有的权重和偏置都初始化为0，则在给定一层的所有神经元都是一样的，BP算法对这些神经元的调整也会是一样的。换句话，就算每层有几百个神经元，模型的整体表现就像每层只有一个神经元一样，模型会显得笨笨的。如果权重是随机初始化的，就可以打破对称性，训练出不同的神经元。

为了使算法能够正常工作，作者对 MLP 的体系结构进行了一个关键性的改变：用 **Logistic 函数**代替了阶跃函数，`σ(z) = 1 / (1 + exp(–z))`。这是必要的，因为阶跃函数只包含平坦的段，因此没有梯度来工作（梯度下降不能在平面上移动），而 Logistic 函数到处都有一个定义良好的非零导数，允许梯度下降在每步上取得一些进展。反向传播算法可以与**其他激活函数**一起使用，而不是 Logistic 函数。另外两个流行的激活函数是：

- 双曲正切函数 `tanh (z) = 2σ(2z) – 1`
- ReLU 函数

 事实证明，另外两种方案更好。

![723.png](D:\AI\Friedrich\pictrue\activation-function.png)

> 这些流行的激活函数及其变体如图 10-8 所示。但是，究竟为什么需要激活函数呢？如果将几个线性变化链式组合起来，得到的还是线性变换。比如，对于 `f(x) = 2x + 3` 和 `g(x) = 5x – 1` ，两者组合起来仍是线性变换：`f(g(x)) = 2(5x – 1) + 3 = 10x + 1`。如果层之间不具有非线性，则深层网络和单层网络其实是等同的，这样就不能解决复杂问题。相反的，足够深且有非线性激活函数的DNN，在理论上可以近似于任意连续函数。

#### 回归MLP

首先，MLP可以用来回归任务。如果想要预测一个单值（例如根据许多特征预测房价），就只需要一个输出神经元，它的输出值就是预测值。对于多变量回归（即一次预测多个值），则每一维度都要有一个神经元。例如，想要定位一张图片的中心，就要预测2D坐标，因此需要两个输出神经元。如果再给对象加个边框，还需要两个值：对象的宽度和高度。

通常，当用MLP做回归时，输出神经元不需要任何激活函数。如果要让输出是正值，则可在输出值使用ReLU激活函数。另外，还可以使用softplus激活函数，这是ReLu的一个平滑化变体：`softplus(z) = log(1 + exp(z))`。z是负值时，softplus接近0，z是正值时，softplus接近z。最后，如果想让输出落入一定范围内，则可以使用调整过的Logistic或双曲正切函数：Logistic函数用于0到1，双曲正切函数用于-1到1。

训练中的损失函数一般是均方误差，但如果训练集有许多异常值，则可以使用平均绝对误差。另外，也可以使用Huber损失函数，它是前两者的组合。

> 提示：当误差小于阈值δ时（一般为1），Huber损失函数是二次的；误差大于阈值时，Huber损失函数是线性的。相比均方误差，线性部分可以让Huber对异常值不那么敏感，二次部分可以让收敛更快，也比均绝对误差更精确。

#### 分类MLP

MLP也可用于分类，对于二元分类问题，只需要一个使用Logistic激活的输出神经元：输出是一个0和1之间的值，作为正类的估计概率。

MLP也可以处理多标签二元分类（见第3章）。例如，邮件分类系统可以预测一封邮件是垃圾邮件，还是正常邮件，同时预测是紧急，还是非紧急邮件。这时，就需要两个输出神经元，两个都是用Logistic函数：第一个输出垃圾邮件的概率，第二个输出紧急的概率。更为一般的讲，需要为每个正类配一个输出神经元。多个输出概率的和不一定非要等于1。这样模型就可以输出各种标签的组合：非紧急非垃圾邮件、紧急非垃圾邮件、非紧急垃圾邮件、紧急垃圾邮件。

如果每个实例只能属于一个类，但可能是三个或多个类中的一个（比如对于数字图片分类，可以使class 0到class 9），则每一类都要有一个输出神经元，整个输出层（见图10-9）要使用softmax激活函数。softmax函数可以保证，每个估计概率位于0和1之间，并且各个值相加等于1。这被称为多类分类。

根据损失函数，因为要预测概率分布，交叉商损失函数（也称为log损失，见第4章）是不错的选择。 



### 微调神经网络超参数

神经网络的灵活性同时也是它的缺点：要微调的超参数太多了。不仅架构可能不同，就算对于一个简单的MLP，就可以调节层数、每层的神经元数、每层使用什么激活函数、初始化的权重，等等。怎么才能知道哪个超参数的组合才是最佳的呢？

一种方法是直接试验超参数的组合，看哪一个在验证集（或使用K折交叉验证）的表现最好。例如，可以使用`GridSearchCV`或`RandomizedSearchCV`探索超参数空间，就像第2章中那样。要这么做的话，必须将Keras模型包装进模仿Scikit-Learn回归器的对象中。第一步是给定一组超参数，创建一个搭建和编译Keras模型的函数：

```csharp
def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):
    model = keras.models.Sequential()
    model.add(keras.layers.InputLayer(input_shape=input_shape))
    for layer in range(n_hidden):
        model.add(keras.layers.Dense(n_neurons, activation="relu"))
    model.add(keras.layers.Dense(1))
    optimizer = keras.optimizers.SGD(lr=learning_rate)
    model.compile(loss="mse", optimizer=optimizer)
    return model
```

这个函数创建了一个单回归（只有一个输出神经元）Sequential模型，数据形状、隐藏层的层数和神经元数是给定的，使用指定学习率的`SGD`优化器编译。最好尽量给大多数超参数都设置合理的默认值，就像Scikit-Learn那样。

然后使用函数`build_model()`创建一个`KerasRegressor`：

```python
keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)
```

`KerasRegressor`是通过`build_model()`将Keras模型包装起来的。因为在创建时没有指定任何超参数，使用的是`build_model()`的默认参数。现在就可以像常规的Scikit-Learn回归器一样来使用它了：使用`fit()`方法训练，使用`score()`方法评估，使用`predict()`方法预测，见下面代码：

```python
keras_reg.fit(X_train, y_train, epochs=100,
              validation_data=(X_valid, y_valid),
              callbacks=[keras.callbacks.EarlyStopping(patience=10)])
mse_test = keras_reg.score(X_test, y_test)
y_pred = keras_reg.predict(X_new)
```

任何传给`fit()`的参数都会传给底层的Keras模型。另外，score分数的意义和MSE是相反的（即，分数越高越好）。因为超参数太多，最好使用随机搜索而不是网格搜索（见第2章的解释）。下面来探索下隐藏层的层数、神经元数和学习率：

```python
from scipy.stats import reciprocal
from sklearn.model_selection import RandomizedSearchCV

param_distribs = {
    "n_hidden": [0, 1, 2, 3],
    "n_neurons": np.arange(1, 100),
    "learning_rate": reciprocal(3e-4, 3e-2),
}

rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3)
rnd_search_cv.fit(X_train, y_train, epochs=100,
                  validation_data=(X_valid, y_valid),
                  callbacks=[keras.callbacks.EarlyStopping(patience=10)])
```

所做的和第2章差不多，除了这里试讲参数传给`fit()`，`fit()`再传给底层的Keras。注意，`RandomizedSearchCV`使用的是K折交叉验证，没有用`X_valid`和`y_valid`（只有早停时才使用）。

取决于硬件、数据集大小、模型复杂度、`n_iter`和`cv`，求解过程可能会持续几个小时。计算完毕后，就能得到最佳参数、最佳得分和训练好的Keras模型，如下所示：

```python
>>> rnd_search_cv.best_params_
{'learning_rate': 0.0033625641252688094, 'n_hidden': 2, 'n_neurons': 42}
>>> rnd_search_cv.best_score_
-0.3189529188278931
>>> model = rnd_search_cv.best_estimator_.model
```

现在就可以保存模型、在测试集上评估，如果对效果满意，就可以部署了。使用随机搜索并不难，适用于许多相对简单的问题。但是当训练较慢时（大数据集的复杂问题），这个方法就只能探索超参数空间的一小部分而已。通过手动调节可以缓解一下：首先使用大范围的超参数值先做一次随机搜索，然后根据第一次的结果再做一次小范围的计算，以此类推。这样就能缩放到最优超参数的范围了。但是，这么做很耗时。

幸好，有比随机搜索更好的探索超参数空间的方法。核心思想很简单：**当某块空间的区域表现好时，就多探索这块区域。这些方法可以代替用户做“放大”工作，可以在更短的时间得到更好的结果。**下面是一些可以用来优化超参数的Python库：

[Hyperopt](https://github.com/hyperopt/hyperopt)
 一个可以优化各种复杂搜索空间（包括真实值，比如学习率和离散值，比如层数）的库。

[Hyperas](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2Fmaxpumperla%2Fhyperas)，[kopt](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2FAvsecz%2Fkopt) 或 [Talos](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2Fautonomio%2Ftalos)
 用来优化Keras模型超参数的库（前两个是基于Hyperopt的）。

[Keras Tuner](https://links.jianshu.com/go?to=https%3A%2F%2Fhoml.info%2Fkerastuner)
 Google开发的简单易用的Keras超参数优化库，还有可视化和分析功能。

[Scikit-Optimize (`skopt`)](https://links.jianshu.com/go?to=https%3A%2F%2Fscikit-optimize.github.io%2F)
 一个通用的优化库。类`BayesSearchCV`使用类似于`GridSearchCV`的接口做贝叶斯优化。

[Spearmint](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2FJasperSnoek%2Fspearmint)
 一个贝叶斯优化库。

[Hyperband](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2Fzygmuntz%2Fhyperband)
 一个快速超参数调节库，基于Lisha Li的论文 《Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization》，[https://arxiv.org/abs/1603.06560](https://links.jianshu.com/go?to=https%3A%2F%2Farxiv.org%2Fabs%2F1603.06560)。

[Sklearn-Deap](https://links.jianshu.com/go?to=https%3A%2F%2Fgithub.com%2Frsteca%2Fsklearn-deap)
 一个基于进化算法的超参数优化库，接口类似`GridSearchCV`。

另外，许多公司也提供超参数优化服务。第19章会讨论Google Cloud AI平台的超参数调节服务（[https://cloud.google.com/ml-engine/docs/tensorflow/using-hyperparameter-tuning](https://links.jianshu.com/go?to=https%3A%2F%2Fcloud.google.com%2Fml-engine%2Fdocs%2Ftensorflow%2Fusing-hyperparameter-tuning)）。其它公司有[Arimo](https://links.jianshu.com/go?to=https%3A%2F%2Farimo.com%2F) 、 [SigOpt](https://links.jianshu.com/go?to=https%3A%2F%2Fsigopt.com%2F)，和CallDesk的 [Oscar](https://links.jianshu.com/go?to=http%3A%2F%2Foscar.calldesk.ai%2F).

超参数调节仍然是活跃的研究领域，其中进化算法表现很突出。例如，在2017年的论文《Population Based Training of Neural Networks》（[https://arxiv.org/abs/1711.09846](https://links.jianshu.com/go?to=https%3A%2F%2Farxiv.org%2Fabs%2F1711.09846)）中，Deepmind的作者用统一优化了一组模型及其超参数。Google也使用了一种进化算法，不仅用来搜索查参数，还可以搜索最佳的神经网络架构；Google的AutoML套间已经可以在云服务上使用了（[https://cloud.google.com/automl/](https://links.jianshu.com/go?to=https%3A%2F%2Fcloud.google.com%2Fautoml%2F)）。也许手动搭建神经网络的日子就要结束了？看看Google的这篇文章：[https://ai.googleblog.com/2018/03/using-evolutionary-automl-to-discover.html](https://links.jianshu.com/go?to=https%3A%2F%2Fai.googleblog.com%2F2018%2F03%2Fusing-evolutionary-automl-to-discover.html)。事实上，用进化算法训练独立的神经网络很成功，已经取代梯度下降了。例如，Uber在2017年介绍了名为Deep Neuroevolution的技术，见[https://eng.uber.com/deep-neuroevolution/](https://links.jianshu.com/go?to=https%3A%2F%2Feng.uber.com%2Fdeep-neuroevolution%2F)。

尽管有这些工具和服务，知道每个超参数该取什么值仍然是帮助的，可以快速创建原型和收缩搜索范围。后面的文字介绍了选择MLP隐藏层数和神经元数的原则，以及如何选择主要的超参数值。



#### 隐藏层数量

对于许多问题，您只需从单个隐藏层开始，就能获得理想的结果。 实际上已经表明，只有一个隐藏层的 MLP 可以建模甚至最复杂的功能，只要它具有足够的神经元。 长期以来，这些事实说服了研究人员，没有必要调查任何更深层次的神经网络。 但是他们忽略了这样一个事实：深层网络具有比浅层网络更高的参数效率：他们可以使用比浅网格更少的神经元来建模复杂的函数，使得训练更快。

**要了解为什么**，假设您被要求使用一些绘图软件绘制一个森林，但是您被禁止使用复制/粘贴。 你必须单独绘制每棵树，每枝分枝，每叶叶。 如果你可以画一个叶，复制/粘贴它来绘制一个分支，然后复制/粘贴该分支来创建一个树，最后复制/粘贴这个树来制作一个林，你将很快完成。 现实世界的数据通常以这样一种分层的方式进行结构化，DNN 自动利用这一事实：较低的隐藏层模拟低级结构（例如，各种形状和方向的线段），中间隐藏层将这些低级结构组合到 模型中级结构（例如，正方形，圆形）和最高隐藏层和输出层将这些中间结构组合在一起，以模拟高级结构（如面）。

**层级化的结构不仅帮助深度神经网络收敛更快，也提高了对新数据集的泛化能力。** 例如，如果您已经训练了模型以识别图片中的脸部，并且您现在想要训练一个新的神经网络来识别发型，那么您可以通过重新使用第一个网络的较低层次来启动训练。 而不是随机初始化新神经网络的前几层的权重和偏置，您可以将其初始化为第一个网络的较低层的权重和偏置的值。这样，网络将不必从大多数图片中低结构中从头学习；它只需要学习更高层次的结构（例如发型）。

总而言之，对于许多问题，您可以从一个或两个隐藏层开始，它可以正常工作（例如，您可以使用只有一个隐藏层和几百个神经元，在 MNIST 数据集上容易达到 97% 以上的准确度使用两个具有相同总神经元数量的隐藏层，在大致相同的训练时间量中精确度为 98%）。**对于更复杂的问题**，您可以逐渐增加隐藏层的数量，直到您开始覆盖训练集。**非常复杂的任务**，例如大型图像分类或语音识别，通常需要具有数十个层（或甚至数百个**但不完全相连的网络**）的网络，正如我们将在第 13 章中看到的那样），并且需要大量的训练数据。但是，您将很少从头开始训练这样的网络：重用预先训练的最先进的网络执行类似任务的部分更为常见。训练将会更快，需要更少的数据（我们将在第 11 章中进行讨论）



#### 每层隐藏层的神经元数量

显然，输入和输出层中神经元的数量由您的任务需要的输入和输出类型决定。例如，MNIST 任务需要`28×28 = 784`个输入神经元和 10 个输出神经元。**对于隐藏的层次来说，通常的做法是将其设置为形成一个漏斗，每个层面上的神经元越来越少，原因在于许多低级别功能可以合并成更少的高级功能。**例如，MNIST 的典型神经网络可能具有两个隐藏层，第一个具有 300 个神经元，第二个具有 100 个。但是，这种做法现在并不常见，您可以为所有隐藏层使用相同的大小 - 例如，所有隐藏的层与 150 个神经元：这样只用调整一次超参数而不是每层都需要调整（因为如果每层一样，比如 150，之后调就每层都调成 160）。就像层数一样，您可以尝试逐渐增加神经元的数量，直到网络开始过度拟合。一般来说，通过增加每层的神经元数量，可以增加层数，从而获得更多的消耗。不幸的是，正如你所看到的，找到完美的神经元数量仍然是黑色的艺术.

**一个更简单的方法是选择一个具有比实际需要的更多层次和神经元的模型，然后使用早期停止来防止它过度拟合**（以及其他正则化技术，特别是 drop out，我们将在第 11 章中看到）。 这被称为“拉伸裤”的方法：而不是浪费时间寻找完美匹配您的大小的裤子，只需使用大型伸缩裤，缩小到合适的尺寸。

> 提示：通常，增加层数比增加每层的神经元的收益更高。提升维度。
>



隐藏层的层数和神经元数不是MLP唯二要调节的参数。下面是一些其它的超参数和调节策略：

#### **学习率

**学习率可能是最重要的超参数。**通常，最佳学习率是最大学习率（最大学习率是超过一定值，训练算法发生分叉的学习率，见第4章）的大概一半。找到最佳学习率的方式之一是从一个极小值开始（比如10-5）训练模型几百次，直到学习率达到一个比较大的值（比如10）。这是通过在每次迭代，将学习率乘以一个常数实现的（例如 exp(log(106)/500，通过500次迭代，从10-5到10 ）。如果将损失作为学习率的函数画出来（学习率使用log），能看到损失一开始是下降的。过了一段时间，学习率会变得非常高，损失就会升高：最佳学习率要比损失开始升高的点低一点（通常比拐点低10倍）。然后就可以重新初始化模型，用这个学习率开始训练了。第11章会介绍更多的学习率优化方法。

> 提示：最佳学习率还取决于其它超参数，特别是批次大小，所以如果调节了任意超参数，最好也更新学习率。

#### 优化器

选择一个更好的优化器（并调节超参数）而不是传统的小批量梯度下降优化器同样重要。第11章会介绍更先进的优化器。

#### **批次大小

**批次大小对模型的表现和训练时间非常重要。**使用大批次的好处是硬件（比如GPU）可以快速处理（见第19章），每秒可以处理更多实例。因此，许多人建议批次大小开到GPU内存的最大值。但也有缺点：在实际中，大批次，会导致训练不稳定，特别是在训练开始时，并且不如小批次模型的泛化能力好。2018年四月，Yann LeCun甚至发了一条推特：“朋友之间不会让对方的批次大小超过32”，引用的是Dominic Masters和Carlo Luschi的论文[《Revisiting Small Batch Training for Deep Neural Networks》](https://arxiv.org/abs/1804.07612)，在这篇论文中，作者的结论是小批次（2到32）更可取，因为小批次可以在更短的训练时间得到更好的模型。但是，有的论文的结论截然相反：2017年，两篇论文[《Train longer, generalize better: closing the generalization gap in large batch training of neural networks》](https://arxiv.org/abs/1705.08741)和[《Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour》](https://arxiv.org/abs/1706.02677)建议，通过多种方法，比如给学习率热身（即学习率一开始很小，然后逐渐提高，见第11章），就能使用大批次（最大8192）。这样，训练时间就能非常短，也没有泛化鸿沟。因此，一种策略是通过学习率热身使用大批次，如果训练不稳定或效果不好，就换成小批次。

#### 激活函数

在大多数情况下，您可以在隐藏层中使用 **ReLU** 激活函数（或其中一个变体，我们将在第 11 章中看到）。 **与其他激活函数相比，计算速度要快一些，而梯度下降在局部最高点上并不会被卡住，因为它不会对大的输入值饱和**（与逻辑函数或双曲正切函数相反, 他们容易在 1 饱和)

对于输出层，softmax 激活函数通常是分类任务的良好选择（当这些类是互斥的时）。 对于回归任务，您完全可以不使用激活函数。

#### 迭代次数

 对于大多数情况，用不着调节训练的迭代次数：使用早停就成了。

#### 更多

想看更多关于调节超参数的实践，可以参考Leslie Smith的论文[《A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, momentum, and weight decay》](https://arxiv.org/abs/1803.09820)。



### **TensorFlow Playground

[TensorFlow Playground](https://playground.tensorflow.org/)是TensorFlow团队推出的一个便利的神经网络模拟器。只需点击几下，就能训练出二元分类器，通过调整架构和超参数，可以从直观上理解神经网络是如何工作的，以及超参数的作用。

